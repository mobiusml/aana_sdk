{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools\n",
      "  Using cached setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Using cached setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: setuptools\n",
      "Successfully installed setuptools-75.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install  ipywidgets\n",
    "# %pip install ipywidgets vllm transformers\n",
    "# %pip install  hf_transfer huggingface_hub\n",
    "# %pip install torch\n",
    "# %pip install git+https://github.com/mobiusml/gemlite.git -qU\n",
    "# %pip install git+https://github.com/mobiusml/hqq.git -q\n",
    "# !apt-get update && apt-get install -y nvidia-cuda-toolkit\n",
    "# Uninstall existing PyTorch\n",
    "# !pip uninstall -y torch torchvision torchaudio\n",
    "# Install PyTorch with CUDA support\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install setuptools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 22:01:14 config.py:542] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 02-06 22:01:15 config.py:621] hqq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 02-06 22:01:15 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='mobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf', speculative_config=None, tokenizer='mobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=hqq, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-06 22:01:16 cuda.py:230] Using Flash Attention backend.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/aana_sdk/notebooks/hqq_gemlite_test.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba40/root/aana_sdk/notebooks/hqq_gemlite_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mvllm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msampling_params\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m SamplingParams\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba40/root/aana_sdk/notebooks/hqq_gemlite_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Ba40/root/aana_sdk/notebooks/hqq_gemlite_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m llm \u001b[39m=\u001b[39m LLM(model\u001b[39m=\u001b[39;49mmodel_id, max_model_len\u001b[39m=\u001b[39;49m\u001b[39m4096\u001b[39;49m, enable_chunked_prefill\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba40/root/aana_sdk/notebooks/hqq_gemlite_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m sampling_params \u001b[39m=\u001b[39m SamplingParams(temperature\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, top_p\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m, max_tokens\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba40/root/aana_sdk/notebooks/hqq_gemlite_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39mgenerate([\u001b[39m\"\u001b[39m\u001b[39mWhat is the capital of Germany?\u001b[39m\u001b[39m\"\u001b[39m], sampling_params)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/vllm/utils.py:1051\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00madditional_message\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1046\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1047\u001b[0m             \u001b[39mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1048\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,  \u001b[39m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m         )\n\u001b[0;32m-> 1051\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine_class\u001b[39m.\u001b[39;49mfrom_engine_args(\n\u001b[1;32m    243\u001b[0m     engine_args, usage_context\u001b[39m=\u001b[39;49mUsageContext\u001b[39m.\u001b[39;49mLLM_CLASS)\n\u001b[1;32m    245\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_counter \u001b[39m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/vllm/engine/llm_engine.py:484\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    482\u001b[0m executor_class \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    483\u001b[0m \u001b[39m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    485\u001b[0m     vllm_config\u001b[39m=\u001b[39;49mengine_config,\n\u001b[1;32m    486\u001b[0m     executor_class\u001b[39m=\u001b[39;49mexecutor_class,\n\u001b[1;32m    487\u001b[0m     log_stats\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m engine_args\u001b[39m.\u001b[39;49mdisable_log_stats,\n\u001b[1;32m    488\u001b[0m     usage_context\u001b[39m=\u001b[39;49musage_context,\n\u001b[1;32m    489\u001b[0m     stat_loggers\u001b[39m=\u001b[39;49mstat_loggers,\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/vllm/engine/llm_engine.py:273\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_registry \u001b[39m=\u001b[39m input_registry\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_processor \u001b[39m=\u001b[39m input_registry\u001b[39m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config)\n\u001b[0;32m--> 273\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_executor \u001b[39m=\u001b[39m executor_class(vllm_config\u001b[39m=\u001b[39;49mvllm_config, )\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mrunner_type \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpooling\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    276\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/vllm/executor/executor_base.py:51\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_adapter_config \u001b[39m=\u001b[39m vllm_config\u001b[39m.\u001b[39mprompt_adapter_config\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservability_config \u001b[39m=\u001b[39m vllm_config\u001b[39m.\u001b[39mobservability_config\n\u001b[0;32m---> 51\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_executor()\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_sleeping \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py:41\u001b[0m, in \u001b[0;36mUniProcExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m     33\u001b[0m     vllm_config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvllm_config,\n\u001b[1;32m     34\u001b[0m     local_rank\u001b[39m=\u001b[39mlocal_rank,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39mor\u001b[39;00m (rank \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_config\u001b[39m.\u001b[39mtensor_parallel_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m),\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollective_rpc(\u001b[39m\"\u001b[39m\u001b[39minit_worker\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39m([kwargs], ))\n\u001b[0;32m---> 41\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollective_rpc(\u001b[39m\"\u001b[39;49m\u001b[39minit_device\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollective_rpc(\u001b[39m\"\u001b[39m\u001b[39mload_model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py:51\u001b[0m, in \u001b[0;36mUniProcExecutor.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 51\u001b[0m answer \u001b[39m=\u001b[39m run_method(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdriver_worker, method, args, kwargs)\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m [answer]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/vllm/utils.py:2220\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2218\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2219\u001b[0m     func \u001b[39m=\u001b[39m partial(method, obj)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2220\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/vllm/worker/worker.py:155\u001b[0m, in \u001b[0;36mWorker.init_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mNCCL_ASYNC_ERROR_HANDLING\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_rank\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mset_device(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    157\u001b[0m _check_if_gpu_supports_dtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    158\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/torch/cuda/__init__.py:478\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    476\u001b[0m device \u001b[39m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 478\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_setDevice(device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    320\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "model_id = \"mobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf\"\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096, enable_chunked_prefill=False)\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=1024)\n",
    "outputs = llm.generate([\"What is the capital of Germany?\"], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
