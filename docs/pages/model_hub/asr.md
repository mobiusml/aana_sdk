# Automatic Speech Recognition (ASR) Models

[WhisperDeployment](./../../reference/deployments.md#aana.deployments.WhisperDeployment) allows you to transcribe or translate audio with Whisper models. The deployment is based on the [faster-whisper](https://github.com/SYSTRAN/faster-whisper) library.

[WhisperConfig](./../../reference/deployments.md#aana.deployments.WhisperConfig) is used to configure the Whisper deployment.

::: aana.deployments.WhisperConfig
    options:
        show_bases: false
        heading_level: 4
        show_docstring_description: false
        docstring_section_style: list

### Example Configurations

As an example, let's see how to configure the Whisper deployment for the [Whisper Medium model](https://huggingface.co/Systran/faster-whisper-medium).


!!! example "Whisper Medium"
    
    ```python
    from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType

    WhisperDeployment.options(
        num_replicas=1,
        max_ongoing_requests=1000,
        ray_actor_options={"num_gpus": 0.25},
        user_config=WhisperConfig(
            model_size=WhisperModelSize.MEDIUM,
            compute_type=WhisperComputeType.FLOAT16,
        ).model_dump(mode="json"),
    )
    ```

Model size is the one of the Whisper model sizes available in the `faster-whisper` library. `compute_type` is the data type to be used for the model.

Here are some other possible configurations for the Whisper deployment:

??? example "Whisper Tiny on CPU"
    
    ```python
    from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType

    # for CPU do not specify num_gpus and use FLOAT32 compute type
    WhisperDeployment.options(
        num_replicas=1,
        user_config=WhisperConfig(
            model_size=WhisperModelSize.TINY,
            compute_type=WhisperComputeType.FLOAT32,
        ).model_dump(mode="json"),
    )
    ```

### Diarized ASR

Diarized transcription can be generated by using [WhisperDeployment](./../../reference/deployments.md#aana.deployments.WhisperDeployment) and [PyannoteSpeakerDiarizationDeployment](./../../reference/deployments.md#aana.deployments.PyannoteSpeakerDiarizationDeployment) and combining the timelines using post processing with [ASRPostProcessingForDiarization](./../../reference/processors.md).

Example configuration for the PyannoteSpeakerDiarization model is available at [Speaker Diarization](./speaker_recognition.md) model hub.

You can simply define the model deployments and the endpoint to transcribe the video with diarization. Below code snippet shows the custom endpoint class `TranscribeVideoWithDiarEndpoint` to combine the outputs from ASR and diarization deployments:

    ```python
    from aana.api.api_generation import Endpoint
    from aana.core.models.speaker import PyannoteSpeakerDiarizationParams
    from aana.core.models.video import VideoInput
    from aana.core.models.whisper import WhisperParams
    from aana.deployments.whisper_deployment import WhisperOutput

    from aana.deployments.aana_deployment_handle import AanaDeploymentHandle

    from aana.integrations.external.yt_dlp import download_video
    from aana.processors.remote import run_remote
    from aana.processors.speaker import ASRPostProcessingForDiarization
    from aana.processors.video import extract_audio

    class TranscribeVideoWithDiarEndpoint(Endpoint):
    """Transcribe video with diarization endpoint."""

    async def initialize(self):
        """Initialize the endpoint."""
        self.asr_handle = await AanaDeploymentHandle.create("asr_deployment")
        self.diar_handle = await AanaDeploymentHandle.create("diarization_deployment")
        await super().initialize()

    async def run(
        self,
        video: VideoInput,
        whisper_params: WhisperParams,
        diar_params: PyannoteSpeakerDiarizationParams,
    ) -> WhisperOutput:
        """Transcribe video with diarization."""
        video_obj = await run_remote(download_video)(video_input=video)
        audio = extract_audio(video=video_obj)

        # diarized transcript requires word_timestamps from ASR
        whisper_params.word_timestamps = True
        transcription = await self.asr_handle.transcribe(
            audio=audio, params=whisper_params
        )
        diarized_output = await self.diar_handle.diarize(
            audio=audio, params=diar_params
        )
        post_processor = ASRPostProcessingForDiarization(
            diarized_segments=diarized_output["segments"],
            transcription_segments=transcription["segments"],
        )
        updated_segments = post_processor.process()
        output_segments = [
            s.model_dump(include=["text", "time_interval", "speaker"])
            for s in updated_segments
        ]

        return {"segments": output_segments}
    ```
An example notebook on diarized transcription is available at `notebooks/diarized_transcription_example.ipynb`.

