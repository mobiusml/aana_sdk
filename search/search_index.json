{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Aana SDK is a powerful framework for building multimodal applications. It facilitates the large-scale deployment of machine learning models, including those for vision, audio, and language, and supports Retrieval-Augmented Generation (RAG) systems. This enables the development of advanced applications such as search engines, recommendation systems, and data insights platforms.</p> <p>The SDK is designed according to the following principles:</p> <ul> <li>Reliability: Aana is designed to be reliable and robust. It is built to be fault-tolerant and to handle failures gracefully.</li> <li>Scalability: Aana is designed to be scalable. It is built on top of Ray, a distributed computing framework, and can be easily scaled to multiple servers.</li> <li>Efficiency: Aana is designed to be efficient. It is built to be fast and parallel and to use resources efficiently.</li> <li>Easy to Use: Aana is designed to be easy to use by developers. It is built to be modular, with a lot of automation and abstraction.</li> </ul> <p>The SDK is still in development, and not all features are fully implemented. We are constantly working on improving the SDK, and we welcome any feedback or suggestions.</p>"},{"location":"#why-use-aana-sdk","title":"Why use Aana SDK?","text":"<p>Nowadays, it is getting easier to experiment with machine learning models and build prototypes. However, deploying these models at scale and integrating them into real-world applications is still a challenge. </p> <p>Aana SDK simplifies this process by providing a framework that allows: - Deploy and scale machine learning models on a single machine or a cluster. - Build multimodal applications that combine multiple different machine learning models.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>Model Deployment:</p> <ul> <li>Deploy models on a single machine or scale them across a cluster.</li> </ul> <p>API Generation:</p> <ul> <li>Automatically generate an API for your application based on the endpoints you define.</li> <li>Input and output of the endpoints will be automatically validated.</li> <li>Simply annotate the types of input and output of the endpoint functions.</li> </ul> <p>Predefined Types:</p> <ul> <li>Comes with a set of predefined types for various data such as images, videos, etc.</li> </ul> <p>Documentation Generation:</p> <ul> <li>Automatically generate documentation for your application based on the defined endpoints.</li> </ul> <p>Streaming Support:</p> <ul> <li>Stream the output of the endpoint to the client as it is generated.</li> <li>Ideal for real-time applications and Large Language Models (LLMs).</li> </ul> <p>Task Queue Support:</p> <ul> <li>Run every endpoint you define as a task in the background without any changes to your code.</li> </ul> <p>Integrations:  </p> <ul> <li>Aana SDK has integrations with various machine learning models and libraries: Whisper, vLLM, Hugging Face Transformers, Deepset Haystack, and more to come (for more information see Integrations).</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#installing-via-pypi","title":"Installing via PyPI","text":"<p>To install Aana SDK via PyPI, you can use the following command:</p> <pre><code>pip install aana\n</code></pre> <p>By default <code>aana</code> installs only the core dependencies. The deployment-specific dependencies are not installed by default. You have two options: - Install the dependencies manually. You will be prompted to install the dependencies when you try to use a deployment that requires them. - Use extras to install all dependencies. Here are the available extras:   - <code>all</code>: Install dependencies for all deployments.   - <code>vllm</code>: Install dependencies for the vLLM deployment.   - <code>asr</code>: Install dependencies for the Automatic Speech Recognition (Whisper) deployment and other ASR models (diarization, voice activity detection, etc.).   - <code>transformers</code>: Install dependencies for the Hugging Face Transformers deployment. There are multiple deployments that use Transformers.   - <code>hqq</code>: Install dependencies for Half-Quadratic Quantization (HQQ) deployment.</p> <p>For example, to install all dependencies, you can use the following command:</p> <pre><code>pip install aana[all]\n</code></pre> <p>For optimal performance install PyTorch version &gt;=2.1 appropriate for your system. You can skip it, but it will install a default version that may not make optimal use of your system's resources, for example, a GPU or even some SIMD operations. Therefore we recommend choosing your PyTorch package carefully and installing it manually.</p> <p>Some models use Flash Attention. Install Flash Attention library for better performance. See flash attention installation instructions for more details and supported GPUs.</p>"},{"location":"#installing-from-github","title":"Installing from GitHub","text":"<ol> <li>Clone the repository.</li> </ol> <pre><code>git clone https://github.com/mobiusml/aana_sdk.git\n</code></pre> <ol> <li>Install additional libraries.</li> </ol> <p>For optimal performance install PyTorch version &gt;=2.1 appropriate for your system. You can continue directly to the next step, but it will install a default version that may not make optimal use of your system's resources, for example, a GPU or even some SIMD operations. Therefore we recommend choosing your PyTorch package carefully and installing it manually.</p> <p>Some models use Flash Attention. Install Flash Attention library for better performance. See flash attention installation instructions for more details and supported GPUs.</p> <ol> <li>Install the package with poetry.</li> </ol> <p>The project is managed with Poetry. See the Poetry installation instructions on how to install it on your system. Use poetry &gt;= 2.0 for the best experience.</p> <p>It will install the package in the virtual environment created by Poetry. Add <code>--extras all</code> to install all extra dependencies.</p> <pre><code>poetry install --extras all\n</code></pre> <p>For the development environment, it is recommended to install all extras and tests and dev dependencies. You can do this by running the following command:</p> <pre><code>poetry install --extras all --with dev,tests\n</code></pre>"},{"location":"#integrations","title":"Integrations","text":"<ul> <li>Integrations: Overview of the available predefined deployments like Whisper, vLLM, Hugging Face Transformers, Haystack etc.</li> <li>OpenAI API: Overview of the OpenAI-compatible Chat Completions API.</li> </ul>"},{"location":"#deployment","title":"Deployment","text":"<ul> <li>Docker: Instructions for using Docker with Aana SDK.</li> <li>Serve Config Files: Information about Serve Config Files for production deployment, how to build them, and deploy applications using them.</li> <li>Cluster Setup: Instructions for setting up a Ray cluster for deployment.</li> </ul>"},{"location":"#configuration","title":"Configuration","text":"<ul> <li>Settings: Documentation on the available settings and configuration options for the project.</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Code Overview: An overview of the structure of the project.</li> <li>Development Environment: Instructions for setting up the development environment.</li> <li>Code Standards: Learn about our coding standards and best practices for contributing to the project.</li> <li>Database: Learn how to use SQL databases in the project.</li> <li>Testing: Information on how to run tests and write new tests.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you're new to the project, we recommend starting with the Tutorial to get a hands-on introduction. From there, you can explore the other documentation files based on your specific needs or interests.</p> <p>For developers looking to contribute, make sure to review the Code Standards and Development Guide.</p> <p>If you have any questions or need further assistance, please don't hesitate to reach out to our support team or community forums.</p>"},{"location":"#creating-a-new-application","title":"Creating a New Application","text":"<p>You can quickly develop multimodal applications using Aana SDK's intuitive APIs and components.</p> <p>If you want to start building a new application, you can use the following GitHub template: Aana App Template. It will help you get started with the Aana SDK and provide you with a basic structure for your application and its dependencies.</p> <p>Let's create a simple application that transcribes a video. The application will download a video from YouTube, extract the audio, and transcribe it using an ASR model.</p> <p>Aana SDK already provides a deployment for ASR (Automatic Speech Recognition) based on the Whisper model. We will use this deployment in the example.</p> <pre><code>from aana.api.api_generation import Endpoint\nfrom aana.core.models.video import VideoInput\nfrom aana.deployments.aana_deployment_handle import AanaDeploymentHandle\nfrom aana.deployments.whisper_deployment import (\n    WhisperComputeType,\n    WhisperConfig,\n    WhisperDeployment,\n    WhisperModelSize,\n    WhisperOutput,\n)\nfrom aana.integrations.external.yt_dlp import download_video\nfrom aana.processors.remote import run_remote\nfrom aana.processors.video import extract_audio\nfrom aana.sdk import AanaSDK\n\n\n# Define the model deployments.\nasr_deployment = WhisperDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25}, # Remove this line if you want to run Whisper on a CPU.\n    user_config=WhisperConfig(\n        model_size=WhisperModelSize.MEDIUM,\n        compute_type=WhisperComputeType.FLOAT16,\n    ).model_dump(mode=\"json\"),\n)\ndeployments = [{\"name\": \"asr_deployment\", \"instance\": asr_deployment}]\n\n\n# Define the endpoint to transcribe the video.\nclass TranscribeVideoEndpoint(Endpoint):\n    \"\"\"Transcribe video endpoint.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        await super().initialize()\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n\n    async def run(self, video: VideoInput) -&gt; WhisperOutput:\n        \"\"\"Transcribe video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        return transcription\n\nendpoints = [\n    {\n        \"name\": \"transcribe_video\",\n        \"path\": \"/video/transcribe\",\n        \"summary\": \"Transcribe a video\",\n        \"endpoint_cls\": TranscribeVideoEndpoint,\n    },\n]\n\naana_app = AanaSDK(name=\"transcribe_video_app\")\n\nfor deployment in deployments:\n    aana_app.register_deployment(**deployment)\n\nfor endpoint in endpoints:\n    aana_app.register_endpoint(**endpoint)\n\nif __name__ == \"__main__\":\n    aana_app.connect(host=\"127.0.0.1\", port=8000, show_logs=False)  # Connects to the Ray cluster or starts a new one.\n    aana_app.migrate()                                              # Runs the migrations to create the database tables.\n    aana_app.deploy(blocking=True)                                  # Deploys the application.\n</code></pre> <p>You have a few options to run the application:</p> <ul> <li>Copy the code above and run it in a Jupyter notebook.</li> <li>Save the code to a Python file, for example <code>app.py</code>, and run it as a Python script: <code>python app.py</code>.</li> <li>Save the code to a Python file, for example <code>app.py</code>, and run it using the Aana CLI: <code>aana deploy app:aana_app --host 127.0.0.1 --port 8000 --hide-logs</code>.</li> </ul> <p>Once the application is running, you will see the message <code>Deployed successfully.</code> in the logs. You can now send a request to the application to transcribe a video.</p> <p>To get an overview of the Ray cluster, you can use the Ray Dashboard. The Ray Dashboard is available at <code>http://127.0.0.1:8265</code> by default. You can see the status of the Ray cluster, the resources used, running applications and deployments, logs, and more. It is a useful tool for monitoring and debugging your applications. See Ray Dashboard documentation for more information.</p> <p>Let's transcribe Gordon Ramsay's perfect scrambled eggs tutorial using the application.</p> <pre><code>curl -X POST http://127.0.0.1:8000/video/transcribe -Fbody='{\"video\":{\"url\":\"https://www.youtube.com/watch?v=VhJFyyukAzA\"}}'\n</code></pre> <p>This will return the full transcription of the video, transcription for each segment, and transcription info like identified language. You can also use the Swagger UI to send the request.</p>"},{"location":"#running-example-applications","title":"Running Example Applications","text":"<p>We provide a few example applications that demonstrate the capabilities of Aana SDK.</p> <ul> <li>Chat with Video: A multimodal chat application that allows users to upload a video and ask questions about the video content based on the visual and audio information. See Chat with Video Demo notebook to see how to use the application.</li> <li>Summarize Video: An Aana application that summarizes a video by extracting transcription from the audio and generating a summary using a Language Model (LLM). This application is a part of the tutorial on how to build multimodal applications with Aana SDK.</li> </ul> <p>See the README files of the applications for more information on how to install and run them.</p> <p>The full list of example applications is available in the Aana Examples repository. You can use these examples as a starting point for building your own applications.</p>"},{"location":"#main-components","title":"Main components","text":"<p>There are three main components in Aana SDK: deployments, endpoints, and AanaSDK.</p>"},{"location":"#deployments","title":"Deployments","text":"<p>Deployments are the building blocks of Aana SDK. They represent the machine learning models that you want to deploy. Aana SDK comes with a set of predefined deployments that you can use or you can define your own deployments. See Integrations section for more information about predefined deployments.</p> <p>Each deployment has a main class that defines it and a configuration class that allows you to specify the deployment parameters.</p> <p>For example, we have a predefined deployment for the Whisper model that allows you to transcribe audio. You can define the deployment like this:</p> <pre><code>from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType\n\nasr_deployment = WhisperDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=WhisperConfig(model_size=WhisperModelSize.MEDIUM, compute_type=WhisperComputeType.FLOAT16).model_dump(mode=\"json\"),\n)\n</code></pre> <p>See Model Hub for a collection of configurations for different models that can be used with the predefined deployments.</p>"},{"location":"#endpoints","title":"Endpoints","text":"<p>Endpoints define the functionality of your application. They allow you to connect multiple deployments (models) to each other and define the input and output of your application.</p> <p>Each endpoint is defined as a class that inherits from the <code>Endpoint</code> class. The class has two main methods: <code>initialize</code> and <code>run</code>.</p> <p>For example, you can define an endpoint that transcribes a video like this:</p> <pre><code>class TranscribeVideoEndpoint(Endpoint):\n    \"\"\"Transcribe video endpoint.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        await super().initialize()\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n\n    async def run(self, video: VideoInput) -&gt; WhisperOutput:\n        \"\"\"Transcribe video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        return transcription\n</code></pre>"},{"location":"#aanasdk","title":"AanaSDK","text":"<p>AanaSDK is the main class that you use to build your application. It allows you to deploy the deployments and endpoints you defined and start the application.</p> <p>For example, you can define an application that transcribes a video like this:</p> <pre><code>aana_app = AanaSDK(name=\"transcribe_video_app\")\n\naana_app.register_deployment(name=\"asr_deployment\", instance=asr_deployment)\naana_app.register_endpoint(\n    name=\"transcribe_video\",\n    path=\"/video/transcribe\",\n    summary=\"Transcribe a video\",\n    endpoint_cls=TranscribeVideoEndpoint,\n)\n\naana_app.connect()  # Connects to the Ray cluster or starts a new one.\naana_app.migrate()  # Runs the migrations to create the database tables.\naana_app.deploy()   # Deploys the application.\n</code></pre> <p>All you need to do is define the deployments and endpoints you want to use in your application, and Aana SDK will take care of the rest.</p>"},{"location":"#api","title":"API","text":"<p>Aana SDK uses form data for API requests, which allows sending both binary data and structured fields in a single request. The request body is sent as a JSON string in the <code>body</code> field, and any binary data is sent as files.</p>"},{"location":"#making-api-requests","title":"Making API Requests","text":"<p>You can send requests to the SDK endpoints with only structured data or a combination of structured data and binary data.</p>"},{"location":"#only-structured-data","title":"Only Structured Data","text":"<p>When your request includes only structured data, you can send it as a JSON string in the <code>body</code> field.</p> <ul> <li> <p>cURL Example: <pre><code>curl http://127.0.0.1:8000/endpoint \\\n    -F body='{\"input\": \"data\", \"param\": \"value\"}'\n</code></pre></p> </li> <li> <p>Python Example: <pre><code>import json, requests\n\nurl = \"http://127.0.0.1:8000/endpoint\"\nbody = {\n    \"input\": \"data\",\n    \"param\": \"value\"\n}\n\nresponse = requests.post(\n    url,\n    data={\"body\": json.dumps(body)}\n)\n\nprint(response.json())\n</code></pre></p> </li> </ul>"},{"location":"#with-binary-data","title":"With Binary Data","text":"<p>When your request includes binary files (images, audio, etc.), you can send them as files in the request and include the names of the files in the <code>body</code> field as a reference.</p> <p>For example, if you want to send an image, you can use <code>aana.core.models.image.ImageInput</code> as the input type that supports binary data upload. The <code>content</code> field in the input type should be set to the name of the file you are sending.</p> <p>You can send multiple files in a single request by including multiple files in the request and referencing them in the <code>body</code> field even if they are of different types.</p> <ul> <li> <p>cURL Example: <pre><code>curl http://127.0.0.1:8000/process_images \\\n    -H \"Content-Type: multipart/form-data\" \\\n    -F body='{\"image\": {\"content\": \"file1\"}}' \\\n    -F file1=\"@image.jpeg\"\n</code></pre></p> </li> <li> <p>Python Example: <pre><code>import json, requests \n\nurl = \"http://127.0.0.1:8000/process_images\"\nbody = {\n    \"image\": {\"content\": \"file1\"}\n}\nwith open(\"image.jpeg\", \"rb\") as file:\n    files = {\"file1\": file}\n\n    response = requests.post(\n        url,\n        data={\"body\": json.dumps(body)},\n        files=files\n    )\n\n    print(response.text)\n</code></pre></p> </li> </ul>"},{"location":"pages/cluster_setup/","title":"Cluster Setup","text":"<p>Based on the documentation, Ray supports the following cloud providers out of the box: AWS, Azure, GCP, Aliyun, vSphere, and KubeRay. We can also implement the node provider interface to use Ray on other cloud providers like Oracle Cloud but it requires implementing the node provider manually which is a bit more work.</p> <p>Another option is to use Ray on Vertex AI which is a managed service that allows you to run Ray on Google Cloud. It allows to setup Ray Cluster without setting up the Kubernetes cluster manually.</p>"},{"location":"pages/cluster_setup/#aana-on-kubernetes","title":"Aana on Kubernetes","text":"<p>Step 1: Create a Kubernetes cluster</p> <p>The first step is to create a Kubernetes cluster on the cloud provider of your choice. Ray has instructions on how to do this for AWS, Azure, and GCP in Managed Kubernetes services docs. </p> <p>Step 2: Deploy Ray on Kubernetes</p> <p>Once you have a Kubernetes cluster, you need to install KubeRay on it. KubeRay is a Kubernetes operator that manages Ray clusters on Kubernetes. You can install KubeRay using Helm. Here is an example of how to install KubeRay on a Kubernetes cluster:</p> <pre><code>helm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm repo update\n\n# Install both CRDs and KubeRay operator v1.1.1.\nhelm install kuberay-operator kuberay/kuberay-operator --version 1.1.1\n\n# Confirm that the operator is running in the namespace `default`.\nkubectl get pods\n# NAME                                READY   STATUS    RESTARTS   AGE\n# kuberay-operator-7fbdbf8c89-pt8bk   1/1     Running   0          27s\n</code></pre> <p>KubeRay offers multiple options for operator installations, such as Helm, Kustomize, and a single-namespaced operator. For further information, please refer to the installation instructions in the KubeRay documentation.</p> <p>Step 3: Create a YAML file for your application</p> <p>Next, you need to create a YAML file that describes your Ray application. See the example below to get an idea of what the YAML file should look like:</p> <pre><code>apiVersion: ray.io/v1\nkind: RayService\nmetadata:\n  name: &lt;service-name&gt;\nspec:\n  serviceUnhealthySecondThreshold: 900 # Config for the health check threshold for Ray Serve applications. Default value is 900.\n  deploymentUnhealthySecondThreshold: 900 # Config for the health check threshold for Ray dashboard agent. Default value is 900.\n  serveConfigV2: |\n    &lt;serve config generated by aana build&gt;\n\n  rayClusterConfig:\n    rayVersion: '2.20.0' # Should match the Ray version in the image of the containers\n    # Ray head pod template.\n    headGroupSpec:\n      # The `rayStartParams` are used to configure the `ray start` command.\n      # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.\n      # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.\n      rayStartParams:\n        dashboard-host: '0.0.0.0'\n      # Pod template\n      template:\n        spec:\n          containers:\n          - name: ray-head\n            image: &lt;base image for the application&gt;\n            ports:\n            - containerPort: 6379\n              name: gcs\n            - containerPort: 8265\n              name: dashboard\n            - containerPort: 10001\n              name: client\n            - containerPort: 8000\n              name: serve\n            resources:\n              limits:\n                cpu: \"3\" # CPU limit for the head pod\n                memory: \"28G\" # Memory limit for the head pod\n                ephemeral-storage: \"95Gi\" # Ephemeral storage limit for the head pod\n              requests:\n                cpu: \"3\" # CPU request for the head pod\n                memory: \"28G\" # Memory request for the head pod\n                ephemeral-storage: \"95Gi\" # Ephemeral storage request for the head pod\n    workerGroupSpecs:\n    # The pod replicas in this group typed worker\n    - replicas: 1 # Number of worker nodes\n      minReplicas: 1\n      maxReplicas: 10\n      groupName: gpu-group\n      rayStartParams: {}\n      # Pod template\n      template:\n        spec:\n          containers:\n          - name: ray-worker\n            image: &lt;base image for the application&gt;\n            resources:\n              limits:\n                cpu: \"3\" # CPU limit for the worker pod\n                memory: \"28G\" # Memory limit for the worker pod\n                ephemeral-storage: \"95Gi\" # Ephemeral storage limit for the worker pod\n              requests:\n                cpu: \"3\" # CPU request for the worker pod\n                memory: \"28G\" # Memory request for the worker pod\n                ephemeral-storage: \"95Gi\" # Ephemeral storage request for the worker pod\n          # Please add the following taints to the GPU node.\n          tolerations:\n            - key: \"ray.io/node-type\"\n              operator: \"Equal\"\n              value: \"worker\"\n              effect: \"NoSchedule\"\n</code></pre> <p><code>serveConfigV2</code> can be generated by the <code>aana build</code> command. It contains the configuration for the Ray Serve applications. </p> <p>The full file will look like this:</p> <pre><code>apiVersion: ray.io/v1\nkind: RayService\nmetadata:\n  name: aana-sdk\nspec:\n  serviceUnhealthySecondThreshold: 900 # Config for the health check threshold for Ray Serve applications. Default value is 900.\n  deploymentUnhealthySecondThreshold: 900 # Config for the health check threshold for Ray dashboard agent. Default value is 900.\n  serveConfigV2: |\n    applications:\n\n    - name: asr_deployment\n\n      route_prefix: /asr_deployment\n\n      import_path: test_project.app_config:asr_deployment\n\n      runtime_env:\n        working_dir: \"https://mobius-public.s3.eu-west-1.amazonaws.com/test_project.zip\"\n        env_vars: \n          DB_CONFIG: '{\"datastore_type\": \"sqlite\", \"datastore_config\": {\"path\": \"/tmp/aana_db.sqlite\"}}'\n\n      deployments:\n\n      - name: WhisperDeployment\n        num_replicas: 1\n        max_ongoing_requests: 1000\n        user_config:\n          model_size: tiny\n          compute_type: float32\n        ray_actor_options:\n          num_cpus: 1.0\n\n    - name: vad_deployment\n\n      route_prefix: /vad_deployment\n\n      import_path: test_project.app_config:vad_deployment\n\n      runtime_env:\n        working_dir: \"https://mobius-public.s3.eu-west-1.amazonaws.com/test_project.zip\"\n        env_vars: \n          DB_CONFIG: '{\"datastore_type\": \"sqlite\", \"datastore_config\": {\"path\": \"/tmp/aana_db.sqlite\"}}'\n\n      deployments:\n\n      - name: VadDeployment\n        num_replicas: 1\n        max_ongoing_requests: 1000\n        user_config:\n          model: https://whisperx.s3.eu-west-2.amazonaws.com/model_weights/segmentation/0b5b3216d60a2d32fc086b47ea8c67589aaeb26b7e07fcbe620d6d0b83e209ea/pytorch_model.bin\n          onset: 0.5\n          offset: 0.363\n          min_duration_on: 0.1\n          min_duration_off: 0.1\n          sample_rate: 16000\n        ray_actor_options:\n          num_cpus: 1.0\n\n    - name: whisper_app\n\n      route_prefix: /\n\n      import_path: test_project.app_config:whisper_app\n\n      runtime_env:\n        working_dir: \"https://mobius-public.s3.eu-west-1.amazonaws.com/test_project.zip\"\n        env_vars: \n          DB_CONFIG: '{\"datastore_type\": \"sqlite\", \"datastore_config\": {\"path\": \"/tmp/aana_db.sqlite\"}}'\n\n      deployments:\n\n      - name: RequestHandler\n        num_replicas: 2\n        ray_actor_options:\n          num_cpus: 0.1\n\n\n  rayClusterConfig:\n    rayVersion: '2.20.0' # Should match the Ray version in the image of the containers\n    ######################headGroupSpecs#################################\n    # Ray head pod template.\n    headGroupSpec:\n      # The `rayStartParams` are used to configure the `ray start` command.\n      # See https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayStartParams.md for the default settings of `rayStartParams` in KubeRay.\n      # See https://docs.ray.io/en/latest/cluster/cli.html#ray-start for all available options in `rayStartParams`.\n      rayStartParams:\n        dashboard-host: '0.0.0.0'\n      # Pod template\n      template:\n        spec:\n          containers:\n          - name: ray-head\n            image: europe-docker.pkg.dev/customised-training-app/eu.gcr.io/aana/aana:0.2-ray-2.20@sha256:8814a3c12c6249a3c2bb216c0cba6eef01267d4c91bb58700f7ffc2311d21a3d\n            ports:\n            - containerPort: 6379\n              name: gcs\n            - containerPort: 8265\n              name: dashboard\n            - containerPort: 10001\n              name: client\n            - containerPort: 8000\n              name: serve\n            resources:\n              limits:\n                cpu: \"3\"\n                memory: \"28G\"\n                ephemeral-storage: \"95Gi\"\n              requests:\n                cpu: \"3\"\n                memory: \"28G\"\n                ephemeral-storage: \"95Gi\"\n    workerGroupSpecs:\n    # The pod replicas in this group typed worker\n    - replicas: 1\n      minReplicas: 1\n      maxReplicas: 10\n      groupName: gpu-group\n      rayStartParams: {}\n      # Pod template\n      template:\n        spec:\n          containers:\n          - name: ray-worker\n            image: europe-docker.pkg.dev/customised-training-app/eu.gcr.io/aana/aana:0.2-ray-2.20@sha256:8814a3c12c6249a3c2bb216c0cba6eef01267d4c91bb58700f7ffc2311d21a3d\n            resources:\n              limits:\n                cpu: \"3\"\n                memory: \"28G\"\n                ephemeral-storage: \"95Gi\"\n              requests:\n                cpu: \"3\"\n                memory: \"28G\"\n                ephemeral-storage: \"95Gi\"\n          # Please add the following taints to the GPU node.\n          tolerations:\n            - key: \"ray.io/node-type\"\n              operator: \"Equal\"\n              value: \"worker\"\n              effect: \"NoSchedule\"\n</code></pre> <p>Let's take a look at a few critical sections of the YAML file:</p> <p>runtime_env: This section specifies the runtime environment for the application. It includes the working directory, environment variables, and potentially python packages that need to be installed.</p> <p>The working directory should be a URL pointing to a zip file containing the application code. It is possible to include the working directory directly in the docker image, but this is not recommended as it makes it harder to update the application code. See the Remote URIs docs for more information.</p> <p>The environment variables are passed to the application as a dictionary. In this example, we are passing a configuration for a SQLite database. </p> <p>You can also specify additional python dependencies using keys like <code>py_modules</code>, <code>pip</code>, <code>conda</code>. For more information, see the docs about handling dependencies.</p> <p>You can also change the deployment parameters if needed. You can specify the number of replicas for each deployment or even change the model parameters.</p> <p>Another important section is the base image for the application. Usually, you can use a pre-built image from the ray project. However, Aana requires some additional dependencies to be installed. It also makes sense to include Aana and all other Python dependencies in the image.</p> <p>Here is an example of a Dockerfile that includes Aana and ray:</p> <pre><code>FROM rayproject/ray:2.20.0.0ae93f-py310\nRUN sudo apt-get update &amp;&amp; sudo apt-get install -y libgl1 libglib2.0-0 ffmpeg\nRUN pip install https://test-files.pythonhosted.org/packages/2e/e7/822893595c45f91acec902612c458fec9ed2684567dcd57bd3ba1770f2ed/aana-0.2.0-py3-none-any.whl\nRUN pip install ray[serve]==2.20\n</code></pre> <p>Keep in mind that this image does not have GPU support. If you need GPU support, choose a different base image from the ray project.</p> <p>Ideally, we should build a few base images for Aana so they can be used directly in the YAML file without any additional build steps and pushing to the registry.</p> <p>In the example, we are using Artifact Registry from Google Cloud. You can use any other registry like Docker Hub, GitHub Container Registry, or any other registry that supports Docker images.</p> <p>Another thing that also needs adjustment is the resource limits and requests. You can adjust them based on your application requirements. But keep in mind that the ephemeral storage needs to be set to a reasonably high value otherwise the application will not deploy.</p> <p>Step 4: Deploy the application</p> <p>After creating the YAML file, you can deploy the application to the Kubernetes cluster using the following command:</p> <pre><code>kubectl apply -f &lt;your-yaml-file&gt;.yaml\n</code></pre> <p>This will create the necessary resources in the Kubernetes cluster to run your Ray application.</p> <p>You can also use the same command to update the application if you make changes to the YAML file. For example, if you want to scale the number of replicas for an ASR deployment, you can set <code>num_replicas: 2</code> in the WhisperDeployment section and then run <code>kubectl apply -f &lt;your-yaml-file&gt;.yaml</code> again and kubernetes will start another replica of the ASR deployment.</p> <p>Step 5: Monitor the application</p> <p>To access the Ray dashboard, you can use port forwarding to access it locally:</p> <pre><code>kubectl port-forward service/aana-sdk-head-svc 8265:8265 8000:8000\n</code></pre> <p>This will forward ports 8265 and 8000 from the Ray head pod to your local machine. You can then access the Ray dashboard by opening a browser and going to <code>http://localhost:8265</code>. The application will be available at <code>http://localhost:8000</code>. The documentation will be available at <code>http://localhost:8000/docs</code> and <code>http://localhost:8000/redoc</code>.</p>"},{"location":"pages/cluster_setup/#things-to-consider","title":"Things to Consider","text":""},{"location":"pages/cluster_setup/#shared-storage","title":"Shared storage","text":"<p>The application stores some files on the local disk that will not be accessible from other nodes in the cluster. This can be a problem if the application is deployed on a multi-node cluster. The solution would be to use a shared storage like NFS. This is a recommendation from the Ray documentation. GKE has Filestore that can be used as a shared storage.</p>"},{"location":"pages/cluster_setup/#database","title":"Database","text":"<p>By default Aana SDK uses SQLite as a database. For cluster deployments, it's recommended to use a more robust database like PostgreSQL. You can use a managed database service like Cloud SQL on GCP or RDS on AWS.</p>"},{"location":"pages/code_overview/","title":"Code overview","text":"<pre><code>aana/                         | top level source code directory for the project\n\u251c\u2500\u2500 alembic/                  | directory for database migrations\n\u2502   \u2514\u2500\u2500 versions/             | individual migrations\n\u251c\u2500\u2500 api/                      | API functionality\n\u2502   \u251c\u2500\u2500 api_generation.py     | API generation code, defines Endpoint class\n\u2502   \u251c\u2500\u2500 request_handler.py    | request handler routes requests to endpoints\n\u2502   \u251c\u2500\u2500 exception_handler.py  | exception handler to process exceptions and return them as JSON\n\u2502   \u251c\u2500\u2500 responses.py          | custom responses for the API\n\u2502   \u2514\u2500\u2500 app.py                | defines the FastAPI app and connects exception handlers\n\u251c\u2500\u2500 config/                   | various configuration objects, including settings, but preconfigured deployments\n\u2502   \u251c\u2500\u2500 db.py                 | config for the database\n\u2502   \u251c\u2500\u2500 deployments.py        | preconfigured for deployments\n\u2502   \u2514\u2500\u2500 settings.py           | app settings\n\u251c\u2500\u2500 core/                     | core models and functionality\n\u2502   \u251c\u2500\u2500 models/               | core data models\n\u2502   \u251c\u2500\u2500 libraries/            | base libraries for audio, images etc.\n\u2502   \u2514\u2500\u2500 chat/                 | LLM chat templates\n\u251c\u2500\u2500 deployments/              | classes for predefined deployments (e.g. Hugging Face Transformers, Whisper, vLLM)\n\u251c\u2500\u2500 exceptions/               | custom exception classes\n\u251c\u2500\u2500 integrations/             | integrations with 3rd party libraries\n\u2502   \u251c\u2500\u2500 external/             | integrations with 3rd party libraries for example image, video, audio processing, download youtube videos, etc.\n\u2502   \u2514\u2500\u2500 haystack/             | integrations with Deepset Haystack\n\u251c\u2500\u2500 processors/               | utility functions for processing data\n\u251c\u2500\u2500 storage/                  | storage functionality\n\u2502   \u251c\u2500\u2500 models/               | database models\n\u2502   \u251c\u2500\u2500 repository/           | repository classes for storage\n\u2502   \u2514\u2500\u2500 services/             | utility functions for storage\n\u251c\u2500\u2500 tests/                    | automated tests for the SDK\n\u2502   \u251c\u2500\u2500 db/                   | tests for database functions\n\u2502   \u251c\u2500\u2500 deployments/          | tests for model deployments\n\u2502   \u251c\u2500\u2500 files/                | assets for testing\n\u2502   \u251c\u2500\u2500 integrations/         | tests for integrations\n\u2502   \u251c\u2500\u2500 projects/             | test projects\n\u2502   \u2514\u2500\u2500 units/                | unit tests\n\u251c\u2500\u2500 utils/                    | various utility functionality\n\u251c\u2500\u2500 cli.py                    | command-line interface to build and deploy the SDK\n\u2514\u2500\u2500 sdk.py                    | base class to create an SDK instance\n</code></pre>"},{"location":"pages/code_standards/","title":"Code Standards","text":"<p>This project uses Ruff for linting and formatting. If you want to  manually run Ruff on the codebase, using poetry it's</p> <pre><code>poetry run ruff check aana\n</code></pre> <p>You can automatically fix some issues with the <code>--fix</code>  and <code>--unsafe-fixes</code> options. (Be sure to install the dev   dependencies: <code>poetry install --with=dev</code>. )</p> <p>To run the auto-formatter, it's</p> <pre><code>poetry run ruff format aana\n</code></pre> <p>(If you are running code in a non-poetry environment, just leave off <code>poetry run</code>.)</p> <p>For users of VS Code, the included <code>settings.json</code> should ensure that Ruff problems appear while you edit, and formatting is applied automatically on save.</p>"},{"location":"pages/database/","title":"Databases","text":"<p>Aana SDK provides a database layer that uses SQLAlchemy as an ORM layer and Alembic for migrations. </p>"},{"location":"pages/database/#configuration","title":"Configuration","text":"<p>By default, the SDK uses SQLite as the database.</p> <p>The database configuration can be set using the environment variable <code>DB_CONFIG</code> or by changing the Settings class. The default configuration:</p> <pre><code>{\n  \"datastore_type\": \"sqlite\",\n  \"datastore_config\": {\n    \"path\": \"/var/lib/aana_data\"\n  }\n}\n</code></pre> <p>Currently, Aana SDK supports SQLite and PostgreSQL databases. See the DbSettings for more information on available options.</p>"},{"location":"pages/database/#migration","title":"Migration","text":"<p>The SDK comes with a set of predefined models and migrations that will automatically set up the database.</p> <p>The migrations are run with <code>aana_app.migrate()</code> or with CLI command <code>aana migrate</code>.</p>"},{"location":"pages/database/#data-models","title":"Data Models","text":"<p>The SDK has following predefined data models:</p> <ul> <li><code>TaskEntity</code>: Represents a task for the task queue. It is used internally by the SDK and is not intended to be used directly.</li> <li><code>BaseEntity</code>: Base class for all entities in the SDK. Use it as a base class for your custom models.</li> <li><code>MediaEntity</code>: Base class for media entities (audio, video, image). You can use it as a base class for your custom media models.</li> <li><code>VideoEntity</code>: Represents a video. You can use it in your application directly or as a base class for your custom video models.</li> <li><code>TranscriptEntity</code>: Represents an ASR transcript. You can use it in your application directly or as a base class for your custom ASR models.</li> <li><code>CaptionEntity</code>: Represents a caption. You can use it in your application directly or as a base class for your custom caption models.</li> </ul>"},{"location":"pages/database/#repositories","title":"Repositories","text":"<p>Repositories are classes that provide an interface to interact with the database. The SDK provides repositories for each data model.</p> <p>The repositories are available in the <code>aana.storage.repository</code> module. Here is a list of available repositories: - <code>TaskRepository</code>: Repository for the <code>TaskEntity</code> model. It is used internally by the SDK and is not intended to be used directly. - <code>BaseRepository</code>: Base repository class for all entities in the SDK. Use it as a base class for your custom repositories. - <code>MediaRepository</code>: Repository for media entities (audio, video, image). You can use it as a base class for your custom media repositories.  - <code>VideoRepository</code>: Repository for the <code>VideoEntity</code> model. You can use it in your application directly or as a base class for your custom video repositories. - <code>TranscriptRepository</code>: Repository for the <code>TranscriptEntity</code> model. You can use it in your application directly or as a base class for your custom ASR repositories. - <code>CaptionRepository</code>: Repository for the <code>CaptionEntity</code> model. You can use it in your application directly or as a base class for your custom caption repositories.</p> <p>To learn more how to use predefined repositories and models, see the How to Use Provided Models and Repositories section in the reference documentation.</p>"},{"location":"pages/database/#custom-models-and-repositories","title":"Custom Models and Repositories","text":"<p>If predefined models and repositories do not meet your requirements, you can create your own models and repositories by extending the provided ones or creating new ones from scratch.</p> <p>If you create a completely new data model, it is advisable to inherit it from the <code>BaseEntity</code> class. This will ensure that your model is compatible with the SDK's storage and retrieval mechanisms. </p> <p>If you want to extend the existing models (e.g., add custom fields to the <code>VideoEntity</code> model), you can create a new model class that inherits from the existing one. </p> <p>In any case, you should create a corresponding repository class that provides an interface to interact with the database.</p> <p>Aana SDK uses Joined Table Inheritance to implement polymorphic inheritance. This means that you can create a new model class that inherits from an existing one and add custom fields to it. The new fields will be stored in a separate table but will be transparently joined with the base table when querying the database.</p> <p>As an example, let's say you want to extend the <code>VideoEntity</code> model to add two custom fields: <code>duration</code> and <code>status</code>.</p> <pre><code>from enum import Enum\n\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy.orm import Mapped, mapped_column\n\nfrom aana.core.models.media import MediaId\nfrom aana.storage.models.video import VideoEntity\n\nclass VideoProcessingStatus(str, Enum):\n    \"\"\"Enum for video status.\"\"\"\n\n    CREATED = \"created\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n\nclass ExtendedVideoEntity(VideoEntity):\n    \"\"\"ORM class for videos with additional metadata.\"\"\"\n\n    __tablename__ = \"extended_video\"\n\n    id: Mapped[MediaId] = mapped_column(ForeignKey(\"video.id\"), primary_key=True)\n    duration: Mapped[float | None] = mapped_column(comment=\"Video duration in seconds\")\n    status: Mapped[VideoProcessingStatus] = mapped_column(\n        nullable=False,\n        default=VideoProcessingStatus.CREATED,\n        comment=\"Processing status\",\n    )\n\n    __mapper_args__ = {  # noqa: RUF012\n        \"polymorphic_identity\": \"extended_video\",\n    }\n</code></pre> <p>In this example, we created a new model class <code>ExtendedVideoEntity</code> that inherits from the <code>VideoEntity</code> model. We added two custom fields: <code>duration</code> and <code>status</code>. The <code>__mapper_args__</code> attribute specifies the polymorphic identity of the new model. This is necessary for SQLAlchemy to correctly handle inheritance.</p> <p>Next, we need to create a repository class that provides an interface to interact with the database. Here is an example of how to create a repository for the <code>ExtendedVideoEntity</code> model:</p> <pre><code>from sqlalchemy.orm import Session\n\nfrom aana.core.models.media import MediaId\nfrom aana.core.models.video import Video, VideoMetadata\nfrom aana.storage.repository.video import VideoRepository\n\nclass ExtendedVideoRepository(VideoRepository[ExtendedVideoEntity]):\n    \"\"\"Repository for videos with additional metadata.\"\"\"\n\n    def __init__(self, session: Session):\n        \"\"\"Constructor.\"\"\"\n        super().__init__(session, ExtendedVideoEntity)\n\n    def save(self, video: Video, duration: float | None = None) -&gt; dict:\n        \"\"\"Saves a video to datastore.\n\n        Args:\n            video (Video): The video object.\n            duration (float): the duration of the video object\n\n        Returns:\n            dict: The dictionary with video and media IDs.\n        \"\"\"\n        video_entity = ExtendedVideoEntity(\n            id=video.media_id,\n            path=str(video.path),\n            url=video.url,\n            title=video.title,\n            description=video.description,\n            duration=duration,\n        )\n        self.create(video_entity)\n        return video_entity\n\n    def get_status(self, media_id: MediaId) -&gt; VideoProcessingStatus:\n        \"\"\"Get the status of a video.\n\n        Args:\n            media_id (str): The media ID.\n\n        Returns:\n            VideoProcessingStatus: The status of the video.\n        \"\"\"\n        entity: ExtendedVideoEntity = self.read(media_id)\n        return entity.status\n\n    def update_status(self, media_id: MediaId, status: VideoProcessingStatus):\n        \"\"\"Update the status of a video.\n\n        Args:\n            media_id (str): The media ID.\n            status (VideoProcessingStatus): The status of the video.\n        \"\"\"\n        entity: ExtendedVideoEntity = self.read(media_id)\n        entity.status = status\n        self.session.commit()\n\n    def get_metadata(self, media_id: MediaId) -&gt; VideoMetadata:\n        \"\"\"Get the metadata of a video.\n\n        Args:\n            media_id (MediaId): The media ID.\n\n        Returns:\n            VideoMetadata: The video metadata.\n        \"\"\"\n        entity: ExtendedVideoEntity = self.read(media_id)\n        return VideoMetadata(\n            title=entity.title,\n            description=entity.description,\n            duration=entity.duration,\n        )\n</code></pre> <p>In this example, we created a new repository class <code>ExtendedVideoRepository</code> that inherits from the <code>VideoRepository</code> class. We redefined methods <code>save</code> and <code>get_metadata</code> to handle the custom field <code>duration</code> and added two new methods <code>get_status</code> and <code>update_status</code> to handle the custom field <code>status</code>.</p> <p>Once you have created your custom model and repository classes, you need to create a migration to update the database schema. You can do this by running the following command from the package root directory:</p> <pre><code>poetry run alembic revision --autogenerate -m \"&lt;Short description of changes in sentence form.&gt;\"\n</code></pre> <p>This will create a new migration file in the <code>alembic/versions</code> directory. You can then apply with aana migrate command.</p> <p>Once the migration is applied, you can start using your custom model and repository classes in your application just like the predefined ones.</p> <pre><code>from aana.core.models import Video\n\nvideo = Video(title=\"My Video\", url=\"https://example.com/video.mp4\")\nduration = 42\nvideo_repository = ExtendedVideoRepository(session)\nvideo_repository.save(video, duration)\n</code></pre> <p>To learn more on how to use data models and repositories, see the How to Use Provided Models and Repositories section in the reference documentation.</p>"},{"location":"pages/dev_environment/","title":"Dev Environment","text":"<p>If you are using Visual Studio Code, you can run this repository in a  dev container. This lets you install and  run everything you need for the repo in an isolated environment via docker on a host system. </p>"},{"location":"pages/docker/","title":"Run with Docker","text":"<p>We provide a docker-compose configuration to run the application in a Docker container in Aana App Template.</p> <p>Requirements:</p> <ul> <li>Docker Engine &gt;= 26.1.0</li> <li>Docker Compose &gt;= 1.29.2</li> <li>NVIDIA Driver &gt;= 525.60.13</li> </ul> <p>You can edit the Dockerfile to assemble the image as you desire and and docker-compose file for container instances and their environment variables.</p> <p>To run the application, simply run the following command:</p> <pre><code>docker-compose up\n</code></pre> <p>The application will be accessible at <code>http://localhost:8000</code> on the host server.</p> <p>Warning</p> <p>If your applications requires GPU to run, you need to specify which GPU to use.</p> <p>The applications will detect the available GPU automatically but you need to make sure that <code>CUDA_VISIBLE_DEVICES</code> is set correctly.</p> <p>Sometimes <code>CUDA_VISIBLE_DEVICES</code> is set to an empty string and the application will not be able to detect the GPU. Use <code>unset CUDA_VISIBLE_DEVICES</code> to unset the variable.</p> <p>You can also set the <code>CUDA_VISIBLE_DEVICES</code> environment variable to the GPU index you want to use: <code>CUDA_VISIBLE_DEVICES=0 docker-compose up</code>.</p> <p>Tip</p> <p>Some models use Flash Attention for better performance. You can set the build argument <code>INSTALL_FLASH_ATTENTION</code> to <code>true</code> to install Flash Attention. </p> <pre><code>INSTALL_FLASH_ATTENTION=true docker-compose build\n</code></pre> <p>After building the image, you can use <code>docker-compose up</code> command to run the application.</p> <p>You can also set the <code>INSTALL_FLASH_ATTENTION</code> environment variable to <code>true</code> in the <code>docker-compose.yaml</code> file.</p>"},{"location":"pages/integrations/","title":"Deployments","text":"<p>Aana SDK comes with a set of predefined deployments that you can use out of the box to deploy models. See Model Hub for a collection of configurations for different models that can be used with the predefined deployments.</p>"},{"location":"pages/integrations/#whisper","title":"Whisper","text":"<p>Whisper deployment allows you to transcribe audio with an automatic Speech Recognition (ASR) model based on the faster-whisper. </p> <p>See WhisperDeployment to learn more about the deployment capabilities.</p> <pre><code>from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType\n\nWhisperDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=WhisperConfig(model_size=WhisperModelSize.MEDIUM, compute_type=WhisperComputeType.FLOAT16).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/integrations/#vllm","title":"vLLM","text":"<p>vLLM deployment allows you to efficiently serve Large Language Model (LLM) with the vLLM library.</p> <p>See VLLMDeployment to learn more about the deployment capabilities.</p> <pre><code>from aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\n\nVLLMDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 1},\n    user_config=VLLMConfig(\n        model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n        dtype=Dtype.AUTO,\n        gpu_memory_reserved=30000,\n        enforce_eager=True,\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n        ),\n    ).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/integrations/#idefics2","title":"Idefics2","text":"<p>Idefics 2 deployment allows you to serve the Idefics 2 models using the Hugging Face Transformers library. Idefics 2 is a vision-language model (VLM) that can answer questions about images, describe visual content, create stories grounded on multiple images, or simply behave as a pure language model without visual inputs.</p> <p>Idefics 2 deployment also supports using <code>Flash Attention 2</code> to boost the efficiency of the transformer model. You can set the value to <code>True</code> or leave it to <code>None</code>, so the deployment will check the availability of the <code>Flash Attention 2</code> on the server node, automatically.</p> <p>See Idefics2Deployment to learn more about the deployment capabilities.</p> <pre><code>from aana.deployments.idefics_2_deployment import Idefics2Config, Idefics2Deployment\n\nIdefics2Deployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.85},\n    user_config=Idefics2Config(\n        model_id=\"HuggingFaceM4/idefics2-8b\",\n        dtype=Dtype.FLOAT16,\n        enable_flash_attention_2=True,\n    ).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/integrations/#hugging-face-transformers","title":"Hugging Face Transformers","text":"<p>Hugging Face Pipeline deployment allows you to serve almost any model from the Hugging Face Hub. It is a wrapper for Hugging Face Pipelines so you can deploy and scale almost any model from the Hugging Face Hub with a few lines of code.</p> <p>See HfPipelineDeployment to learn more about the deployment capabilities.</p> <pre><code>from transformers import BitsAndBytesConfig\nfrom aana.deployments.hf_pipeline_deployment import HfPipelineConfig, HfPipelineDeployment\n\nHfPipelineDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 1},\n    user_config=HfPipelineConfig(\n        model_id=\"Salesforce/blip2-opt-2.7b\",\n        task=\"image-to-text\",\n        model_kwargs={\n            \"quantization_config\": BitsAndBytesConfig(load_in_8bit=False, load_in_4bit=True),\n        },\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>There are a few notebooks that demonstrate how to use the Hugging Face Transformers deployments:</p> <ul> <li>HF Pipeline deployment notebook</li> <li>HF Text Generation deployment notebook</li> </ul>"},{"location":"pages/integrations/#haystack","title":"Haystack","text":"<p>Haystack integration allows you to build Retrieval-Augmented Generation (RAG) systems with the Deepset Haystack. </p> <p>See Haystack integration notebook for a detailed example.</p>"},{"location":"pages/integrations/#openai-compatible-chat-completions-api","title":"OpenAI-compatible Chat Completions API","text":"<p>The OpenAI-compatible Chat Completions API allows you to access the Aana applications with any OpenAI-compatible client. See OpenAI-compatible API docs for more details.</p>"},{"location":"pages/integrations/#speaker-diarization","title":"Speaker Diarization","text":"<p>Speaker Diarization deployment allows you to classify the speakers present in the audio with their timestamp information. Speaker Diarization models are based on pyannote.audio. </p> <p>See PyannoteSpeakerDiarizationDeployment to learn more about the deployment capabilities.</p> <pre><code>from aana.deployments.pyannote_speaker_diarization_deployment import PyannoteSpeakerDiarizationDeployment, PyannoteSpeakerDiarizationConfig\n\nPyannoteSpeakerDiarizationDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.05},\n    user_config=PyannoteSpeakerDiarizationConfig(\n        model_name=(\"pyannote/speaker-diarization-3.1\"),\n        sample_rate=16000,\n    ).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/integrations/#half-quadratic-quantization-hqq","title":"Half-Quadratic Quantization (HQQ)","text":"<p>HQQ deployment allows you quantize the largest models, without calibration data, in just a few minutes.</p> <p>Aana SDK provides a deployment for quantizing and serving language models with the Half-Quadratic Quantization (HQQ) library. See HqqTextGenerationDeployment to learn more about the deployment capabilities.</p> <p>You can use this deployment to load pre-quantized models like HQQ Models as well as models from HuggingFace Hub and quantiize them on the fly. </p> <pre><code>from hqq.core.quantize import BaseQuantizeConfig\nfrom aana.deployments.hqq_text_generation_deployment import (\n    HqqBackend,\n    HqqTexGenerationConfig,\n    HqqTextGenerationDeployment,\n)\n\nHqqTextGenerationDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.5},\n    user_config=HqqTexGenerationConfig(\n        model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n        backend=HqqBackend.BITBLAS,\n        quantize_on_fly=True,\n        quantization_config=BaseQuantizeConfig(nbits=4, group_size=64, axis=1),\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=512\n        ),\n        model_kwargs={\n            \"attn_implementation\": \"sdpa\"\n        },\n    ).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/openai_api/","title":"OpenAI-compatible API","text":"<p>Aana SDK provides an OpenAI-compatible Chat Completions API that allows you to integrate Aana with any OpenAI-compatible application.</p> <p>Chat Completions API is available at the <code>/chat/completions</code> endpoint.</p> <p>Tip</p> <p>The endpoint is enabled by default but can be disabled by setting the environment variable: <code>OPENAI_ENDPOINT_ENABLED=False</code>.</p> <p>It is compatible with the OpenAI client libraries and can be used as a drop-in replacement for OpenAI API.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"token\", # Any non empty string will work, we don't require an API key\n    base_url=\"http://localhost:8000\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n]\n\ncompletion = client.chat.completions.create(\n    messages=messages,\n    model=\"llm_deployment\",\n)\n\nprint(completion.choices[0].message.content)\n</code></pre> <p>The API also supports streaming:</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"token\", # Any non empty string will work, we don't require an API key\n    base_url=\"http://localhost:8000\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n]\n\nstream = client.chat.completions.create(\n    messages=messages,\n    model=\"llm_deployment\",\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n</code></pre> <p>The API requires an LLM deployment. Aana SDK provides support for vLLM and Hugging Face Transformers.</p> <p>The name of the model matches the name of the deployment. For example, if you registered a vLLM deployment with the name <code>llm_deployment</code>, you can use it with the OpenAI API as <code>model=\"llm_deployment\"</code>.</p> <pre><code>import os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nfrom aana.core.models.sampling import SamplingParams\nfrom aana.core.models.types import Dtype\nfrom aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\nfrom aana.sdk import AanaSDK\n\nllm_deployment = VLLMDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 1},\n    user_config=VLLMConfig(\n        model=\"TheBloke/Llama-2-7b-Chat-AWQ\",\n        dtype=Dtype.AUTO,\n        quantization=\"awq\",\n        gpu_memory_reserved=13000,\n        enforce_eager=True,\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n        ),\n        chat_template=\"llama2\",\n    ).model_dump(mode=\"json\"),\n)\n\naana_app = AanaSDK(name=\"llm_app\")\naana_app.register_deployment(name=\"llm_deployment\", instance=llm_deployment)\n\nif __name__ == \"__main__\":\n    aana_app.connect()\n    aana_app.migrate()\n    aana_app.deploy()\n</code></pre> <p>You can also use the example project <code>llama2</code> to deploy Llama-2-7b Chat model.</p> <pre><code>CUDA_VISIBLE_DEVICES=0 aana deploy aana.projects.llama2.app:aana_app\n</code></pre>"},{"location":"pages/serve_config_files/","title":"Serve Config Files","text":"<p>The Serve Config Files is the recommended way to deploy and update your applications in production. Aana SDK provides a way to build the Serve Config Files for the Aana applications.</p>"},{"location":"pages/serve_config_files/#building-serve-config-files","title":"Building Serve Config Files","text":"<p>To build the Serve config file, run the following command:</p> <pre><code>aana build &lt;app_module&gt;:&lt;app_name&gt;\n</code></pre> <p>For example:</p> <pre><code>aana build aana_chat_with_video.app:aana_app\n</code></pre> <p>The command will generate the Serve Config file and App Config file and save them in the project directory. You can then use these files to deploy the application using the Ray Serve CLI.</p>"},{"location":"pages/serve_config_files/#deploying-with-serve-config-files","title":"Deploying with Serve Config Files","text":"<p>When you are running the Aana application using the Serve config files, you need to run the migrations to create the database tables for the application. To run the migrations, use the following command:</p> <pre><code>aana migrate &lt;app_module&gt;:&lt;app_name&gt;\n</code></pre> <p>For example:</p> <pre><code>aana migrate aana_chat_with_video.app:aana_app\n</code></pre> <p>Before deploying the application, make sure you have the Ray cluster running. If you want to start a new Ray cluster on a single machine, you can use the following command:</p> <pre><code>ray start --head\n</code></pre> <p>For more info on how to start a Ray cluster, see the Ray documentation.</p> <p>To deploy the application using the Serve config files, use <code>serve deploy</code> command provided by Ray Serve. For example:</p> <pre><code>serve deploy config.yaml\n</code></pre>"},{"location":"pages/settings/","title":"Settings","text":""},{"location":"pages/settings/#settings","title":"Settings","text":"<p>Here are the environment variables that can be used to configure the Aaana SDK:</p> <ul> <li>TMP_DATA_DIR: The directory to store temporary data. Default: <code>/tmp/aana</code>.</li> <li>NUM_WORKERS: The number of request workers. Default: <code>2</code>.</li> <li>DB_CONFIG: The database configuration in the format <code>{\"datastore_type\": \"sqlite\", \"datastore_config\": {\"path\": \"/path/to/sqlite.db\"}}</code>. Currently only SQLite and PostgreSQL are supported. Default: <code>{\"datastore_type\": \"sqlite\", \"datastore_config\": {\"path\": \"/var/lib/aana_data\"}}</code>.</li> <li>HF_HUB_ENABLE_HF_TRANSFER: If set to <code>1</code>, the HuggingFace Transformers will use the HF Transfer library to download the models from HuggingFace Hub to speed up the process. Recommended to always set to it <code>1</code>. Default: <code>0</code>.</li> <li>HF_TOKEN: The HuggingFace API token to download the models from HuggingFace Hub, required for private or gated models.</li> <li>TEST__SAVE_EXPECTED_OUTPUT: If set to <code>True</code>, the expected output will be saved when running the tests. Useful for creating new development tests. Default: <code>False</code>.</li> </ul> <p>See reference documentation for more advanced settings.</p>"},{"location":"pages/testing/","title":"Testing","text":"<p>The project uses pytest for testing. To run the tests, use the following command:</p> <pre><code>poetry run pytest\n</code></pre> <p>Most deployment tests require GPU to run. They are skipped if the GPU is not available. Right now we don't GPU runner for our CI/CD pipeline, so if you change anything related to deployments, make sure to run the tests locally with GPU and mention it in the PR description.</p> <p>If you are using VS Code, you can run the tests using the Test Explorer that is installed with the Python extension.</p>"},{"location":"pages/testing/#testing-deployments","title":"Testing Deployments","text":"<p>This guide explains how to test the deployments. Aana SDK provides a set of fixtures and utilities that help you to write tests for the deployments. </p>"},{"location":"pages/testing/#goals","title":"Goals","text":"<p>The goal is to verify that the deployment, wrapper around the model, works as expected. The goal is NOT to test the model itself. That's why we only test 1-2 deployment configurations, just to make sure that the deployment works as expected.</p>"},{"location":"pages/testing/#setup-deployment-fixture","title":"Setup Deployment Fixture","text":"<p>The <code>setup_deployment</code> fixture is used to start Aana SDK application with the given deployment configuration. The fixture is parametrized with two parameters: - <code>deployment_name</code>: The name of the deployment. This is used to identify the deployment in the test. - <code>deployment</code>: The deployment configuration. </p> <p>We use indirect parametrization to pass the deployment configuration to the fixture. The deployment configurations are defined as a list of tuples. Each tuple contains the deployment name and the deployment configuration.</p> <p>The fixture returns a tuple with three elements: - <code>deployment_name</code>: The name of the deployment, same as the passed to the fixture. - <code>handle_name</code>: The name of the deployment handle. It is used to interact with the deployment. - <code>app</code>: The Aana SDK application instance. Most of the time, you don't need to use it.</p> <pre><code>deployments = [(\"your_deployment_name\", your_deployment_config), ...]\n\n@pytest.mark.parametrize(\"setup_deployment\", deployments, indirect=True)\nclass TestYourDeployment:\n    \"\"\"Test your deployment.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_your_deployment(setup_deployment):\n        deployment_name, handle_name, _ = setup_deployment\n    ...\n</code></pre> <p>You don't need to import the <code>setup_deployment</code> fixture because it is automatically imported from conftest.py.</p>"},{"location":"pages/testing/#test-class","title":"Test Class","text":"<p>Deployment tests are organized in test classes. The reason is that we want to setup the deployment only once for all tests in the class. That's why we use the <code>setup_deployment</code> fixture as a parameter to the test class.</p>"},{"location":"pages/testing/#deployment-handle","title":"Deployment Handle","text":"<p>The <code>AanaDeploymentHandle</code> class is used to interact with the deployment. The class allows you to call the methods on the deployment remotely. </p> <p>To create an instance of the <code>AanaDeploymentHandle</code> class, use the class method <code>create</code>. The method takes the handle name as an argument.</p> <pre><code>handle = await AanaDeploymentHandle.create(handle_name)\n</code></pre>"},{"location":"pages/testing/#verify-results-utility","title":"Verify Results Utility","text":"<p>The <code>aana.tests.utils.verify_deployment_results</code> utility is used to compare the expected output with the actual output. The utility takes two arguments: - <code>expected_output_path (Path)</code>: The path to the expected output file. - <code>output (Dict)</code>: The actual output of the deployment.</p> <p>The expected outputs are stored in the <code>aana/tests/files/expected</code> directory. There is a convention to store the expected output files for each deployment in a separate directory.</p> <p>For example,</p> <pre><code>expected_output_path = (\n    resources.files(\"aana.tests.files.expected\")\n    / \"whisper\"\n    / f\"{deployment_name}_{audio_file}.json\"\n)\n</code></pre>"},{"location":"pages/testing/#test__save_expected_output-environment-variable","title":"TEST__SAVE_EXPECTED_OUTPUT Environment Variable","text":"<p><code>verify_deployment_results</code> has a built-in mechanism to save the actual output as the expected output. If you set the environment variable <code>TEST__SAVE_EXPECTED_OUTPUT</code> to <code>True</code> and <code>verify_deployment_results</code> does not find the expected output file, it will save the actual output as the expected output. It is useful when you are writing tests for a new deployment because you don't need to create the expected output files manually but only need to verify the output. But remember to set the environment variable back to <code>False</code> after you have created the expected output files and check created files manually to make sure they are correct.</p>"},{"location":"pages/testing/#gpu-availability","title":"GPU Availability","text":"<p>If the deployment requires a GPU, the <code>setup_deployment</code> fixture will skip the test if the GPU is not available. It uses <code>num_gpus</code> from the deployment configuration to check if the GPU is required.</p>"},{"location":"pages/testing/#example","title":"Example","text":"<p>Here is an example of the test for the Whisper deployment. You can find the full test in the <code>aana/tests/deployments/test_whisper_deployment.py</code>.</p> <pre><code>import pytest\nfrom importlib import resources\nfrom aana.core.models.audio import Audio\nfrom aana.core.models.whisper import WhisperParams\nfrom aana.deployments.aana_deployment_handle import AanaDeploymentHandle\nfrom aana.deployments.whisper_deployment import WhisperComputeType, WhisperConfig, WhisperDeployment, WhisperModelSize\nfrom aana.tests.utils import verify_deployment_results\n\n# Define the deployments to test as a list of tuples.\ndeployments = [\n    (\n        \"whisper_tiny\",\n        WhisperDeployment.options(\n            num_replicas=1,\n            user_config=WhisperConfig(\n                model_size=WhisperModelSize.TINY,\n                compute_type=WhisperComputeType.FLOAT32,\n            ).model_dump(mode=\"json\"),\n        ),\n    ),\n    (\n        \"whisper_medium\",\n        WhisperDeployment.options(\n            num_replicas=1,\n            max_ongoing_requests=1000,\n            ray_actor_options={\"num_gpus\": 0.25},\n            user_config=WhisperConfig(\n                model_size=WhisperModelSize.MEDIUM,\n                compute_type=WhisperComputeType.FLOAT16,\n            ).model_dump(mode=\"json\"),\n        ),\n    )\n]\n\n\n# Parametrize the test with the deployments.\n@pytest.mark.parametrize(\"setup_deployment\", deployments, indirect=True)\nclass TestWhisperDeployment:\n    \"\"\"Test Whisper deployment.\"\"\"\n\n    # The test is asynchronous because it interacts with the deployment.\n    @pytest.mark.asyncio\n    # Parametrize the test with the audio files (this can be anything else like prompts etc.).\n    @pytest.mark.parametrize(\"audio_file\", [\"squirrel.wav\", \"physicsworks.wav\"])\n    # Define the test function, add `setup_deployment` fixture, and parameterized arguments to the function.\n    async def test_transcribe(self, setup_deployment, audio_file):\n        \"\"\"Test transcribe methods.\"\"\"\n        # Get deployment name, handle name, and app instance from the setup_deployment fixture.\n        deployment_name, handle_name, app = setup_deployment\n\n        # Create the deployment handle, use the handle name from the setup_deployment fixture.\n        handle = await AanaDeploymentHandle.create(handle_name)\n\n        # Define the path to the expected output file. \n        # There are 3 parts: \n        # - The path to the expected output directory (aana/tests/files/expected), should not be changed.\n        # - The name of the subdirectory for the deployment (whisper), should be changed for each deployment type.\n        # - File name with based on the parameters (deployment_name, audio_file, etc.).\n        expected_output_path = (\n            resources.files(\"aana.tests.files.expected\")\n            / \"whisper\"\n            / f\"{deployment_name}_{audio_file}.json\"\n        )\n\n        # Run the deployment method.\n        path = resources.files(\"aana.tests.files.audios\") / audio_file\n        assert path.exists(), f\"Audio not found: {path}\"\n\n        audio = Audio(path=path, media_id=audio_file)\n\n        output = await handle.transcribe(\n            audio=audio, params=WhisperParams(word_timestamps=True, temperature=0.0)\n        )\n\n        # Verify the results with the expected output.\n        verify_deployment_results(expected_output_path, output)\n</code></pre>"},{"location":"pages/tutorial/","title":"Tutorial","text":""},{"location":"pages/tutorial/#how-to-create-a-new-project-with-aana-sdk","title":"How to Create a New Project with Aana SDK","text":"<p>Aana SDK is a powerful framework for building multimodal applications. It facilitates the large-scale deployment of machine learning models, including those for vision, audio, and language, and supports Retrieval-Augmented Generation (RAG) systems. This enables the development of advanced applications such as search engines, recommendation systems, and data insights platforms.</p> <p>Aana SDK comes with a set of example applications that demonstrate the capabilities of the SDK. These applications can be used as a reference to build your own applications. See the projects directory for the example applications.</p> <p>If you want to start building a new application, you can use the following GitHub template: Aana App Template. It will help you get started with the Aana SDK and provide you with a basic structure for your application and its dependencies.</p> <p>In this tutorial, we will walk you through the process of creating a new project with Aana SDK. By the end of this tutorial, you will have a runnable application that transcribes a video and summarizes the transcript using a Language Model (LLM). We will use the video transcription application as a starting point and extend it to include the LLM model for summarization and a new endpoints.</p>"},{"location":"pages/tutorial/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have a working installation of Aana SDK. See the installation instructions for more information.</p>"},{"location":"pages/tutorial/#video-transcription-application","title":"Video Transcription Application","text":"<p>First, let's review a video transcription application. Here is the code for it:</p> <pre><code>from aana.api.api_generation import Endpoint\nfrom aana.core.models.video import VideoInput\nfrom aana.deployments.aana_deployment_handle import AanaDeploymentHandle\nfrom aana.deployments.whisper_deployment import (\n    WhisperComputeType,\n    WhisperConfig,\n    WhisperDeployment,\n    WhisperModelSize,\n    WhisperOutput,\n)\nfrom aana.integrations.external.yt_dlp import download_video\nfrom aana.processors.remote import run_remote\nfrom aana.processors.video import extract_audio\nfrom aana.sdk import AanaSDK\n\n\n# Define the model deployments.\nasr_deployment = WhisperDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25}, # Remove this line if you want to run Whisper on a CPU.\n    user_config=WhisperConfig(\n        model_size=WhisperModelSize.MEDIUM,\n        compute_type=WhisperComputeType.FLOAT16,\n    ).model_dump(mode=\"json\"),\n)\ndeployments = [{\"name\": \"asr_deployment\", \"instance\": asr_deployment}]\n\n\n# Define the endpoint to transcribe the video.\nclass TranscribeVideoEndpoint(Endpoint):\n    \"\"\"Transcribe video endpoint.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        await super().initialize()\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n\n    async def run(self, video: VideoInput) -&gt; WhisperOutput:\n        \"\"\"Transcribe video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        return transcription\n\nendpoints = [\n    {\n        \"name\": \"transcribe_video\",\n        \"path\": \"/video/transcribe\",\n        \"summary\": \"Transcribe a video\",\n        \"endpoint_cls\": TranscribeVideoEndpoint,\n    },\n]\n\naana_app = AanaSDK(name=\"transcribe_video_app\")\n\nfor deployment in deployments:\n    aana_app.register_deployment(**deployment)\n\nfor endpoint in endpoints:\n    aana_app.register_endpoint(**endpoint)\n\nif __name__ == \"__main__\":\n    aana_app.connect(host=\"127.0.0.1\", port=8000, show_logs=False)  # Connects to the Ray cluster or starts a new one.\n    aana_app.migrate()                                              # Runs the migrations to create the database tables.\n    aana_app.deploy(blocking=True)                                  # Deploys the application.\n</code></pre>"},{"location":"pages/tutorial/#running-the-application","title":"Running the Application","text":"<p>You have a few options to run the application:</p> <ul> <li>Copy the code above and run it in a Jupyter notebook.</li> <li>Save the code to a Python file, for example <code>app.py</code>, and run it as a Python script: <code>python app.py</code>.</li> <li>Save the code to a Python file, for example <code>app.py</code>, and run it using the Aana CLI: <code>aana deploy app:aana_app --host 127.0.0.1 --port 8000 --hide-logs</code>.</li> </ul> <p>Once the application is running, you will see the message <code>Deployed successfully.</code> in the logs. You can now send a request to the application to transcribe a video.</p> <p>To get an overview of the Ray cluster, you can use the Ray Dashboard. The Ray Dashboard is available at <code>http://127.0.0.1:8265</code> by default. You can see the status of the Ray cluster, the resources used, running applications and deployments, logs, and more. It is a useful tool for monitoring and debugging your applications. See Ray Dashboard documentation for more information.</p> <p>Let's transcribe Gordon Ramsay's perfect scrambled eggs tutorial using the application.</p> <pre><code>curl -X POST http://127.0.0.1:8000/video/transcribe -Fbody='{\"video\":{\"url\":\"https://www.youtube.com/watch?v=VhJFyyukAzA\"}}'\n</code></pre>"},{"location":"pages/tutorial/#application-components","title":"Application Components","text":"<p>The application consists of 3 main components: the deployment, the endpoint, and the application itself. </p>"},{"location":"pages/tutorial/#deployments","title":"Deployments","text":"<p>Deployments are the building blocks of Aana SDK. They represent the machine learning models that you want to deploy. Aana SDK comes with a set of predefined deployments that you can use or you can define your own deployments. See Integrations for more information about predefined deployments.</p> <p>Each deployment has a main class that defines it and a configuration class that allows you to specify the deployment parameters.</p> <p>In the example above, we define a deployment for the Whisper model that allows you to transcribe audio. The deployment is defined as follows:</p> <pre><code>from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType\n\nasr_deployment = WhisperDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=WhisperConfig(model_size=WhisperModelSize.MEDIUM, compute_type=WhisperComputeType.FLOAT16).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/tutorial/#endpoints","title":"Endpoints","text":"<p>Endpoints define the functionality of your application. They allow you to connect multiple deployments (models) to each other and define the input and output of your application.</p> <p>Each endpoint is defined as a class that inherits from the <code>Endpoint</code> class. The class has two main methods: <code>initialize</code> and <code>run</code>.</p> <p><code>initialize</code> method contains actions that need to be performed before the endpoint is run. For example, you can create handles for the deployments that the endpoint will use.</p> <p><code>run</code> method is the main method of the endpoint that is called when the endpoint receives a request. </p> <p>The <code>run</code> method should be annotated with the input and output types. The types should be pydantic models. Return type should be a typed dictionary where the keys are strings and the values are pydantic models.</p> <p>For example, you can define an endpoint that transcribes a video like this:</p> <pre><code>class TranscribeVideoEndpoint(Endpoint):\n    \"\"\"Transcribe video endpoint.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        await super().initialize()\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n\n    async def run(self, video: VideoInput) -&gt; WhisperOutput:\n        \"\"\"Transcribe video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        return transcription\n</code></pre>"},{"location":"pages/tutorial/#application","title":"Application","text":"<p>AanaSDK is the main class that represents the application. It allows you to deploy the deployments and endpoints you defined and start the application.</p> <p>For example, you can define an application that transcribes a video like this:</p> <pre><code>aana_app = AanaSDK(name=\"transcribe_video_app\")\n\n# Register the ASR deployment.\naana_app.register_deployment(name=\"asr_deployment\", instance=asr_deployment)\n\n# Register the transcribe video endpoint.\naana_app.register_endpoint(\n    name=\"transcribe_video\",\n    path=\"/video/transcribe\",\n    summary=\"Transcribe a video\",\n    endpoint_cls=TranscribeVideoEndpoint,\n)\n\naana_app.connect()  # Connects to the Ray cluster or starts a new one.\naana_app.migrate()  # Runs the migrations to create the database tables.\naana_app.deploy()   # Deploys the application.\n</code></pre>"},{"location":"pages/tutorial/#connecting-to-the-deployments","title":"Connecting to the Deployments","text":"<p>Once the application is deployed, you can also access the deployments from other processes or applications using the <code>AanaSDK</code> class. For example, you can access the ASR deployment like this:</p> <pre><code>from aana.sdk import AanaSDK\nfrom aana.deployments.aana_deployment_handle import AanaDeploymentHandle\n\naana_app = AanaSDK().connect()\nasr_handle = AanaDeploymentHandle.create(\"asr_deployment\")\n</code></pre> <p>The name of the deployment used to create the handle should match the name of the deployment when <code>register_deployment</code> was called.</p> <p>This is quite useful for testing or debugging your application.</p>"},{"location":"pages/tutorial/#transcript-summarization-application","title":"Transcript Summarization Application","text":"<p>Now that we have reviewed the video transcription application, let's build on it to create a video transcript summarization application. For the summarization we will need a few extra components:</p> <ul> <li>An LLM model to summarize the transcript.</li> <li>An endpoint to summarize the transcript.</li> </ul>"},{"location":"pages/tutorial/#llm-model","title":"LLM Model","text":"<p>LLM model can be registered as a deployment in the application. Aana SDK provides two deployments that can be used to deploy LLM models:</p> <ul> <li><code>HfTextGenerationDeployment</code>: A deployment based on Hugging Face Transformers library.</li> <li><code>VLLMDeployment</code>: A deployment based on vLLM library.</li> </ul> <p>Both deployments have the same interface and can be used interchangeably. In this example, we will use <code>HfTextGenerationDeployment</code>.</p> <p>Here is an example of how to define an LLM deployment:</p> <pre><code>from aana.deployments.hf_text_generation_deployment import HfTextGenerationConfig, HfTextGenerationDeployment\nllm_deployment = HfTextGenerationDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=HfTextGenerationConfig(\n        model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n        model_kwargs={\n            \"trust_remote_code\": True,\n            \"quantization_config\": BitsAndBytesConfig(\n                load_in_8bit=False, load_in_4bit=True\n            ),\n        },\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Let's take a closer look at the configuration options:</p> <ul> <li><code>HfTextGenerationDeployment</code> is the deployment class.</li> <li><code>num_replicas=1</code> specifies the number of replicas to deploy. If you want to scale the deployment, you can increase this number to deploy more replicas on more GPUs or nodes.</li> <li><code>ray_actor_options={\"num_gpus\": 0.25}</code> specifies the number of GPUs that each replica requires. This will be used to allocate resources on the Ray cluster but keep in mind that it will not limit the deployment to use only this amount of GPUs, it's only used for resource allocation. If you want to run the deployment on a CPU, you can remove this line.</li> <li><code>user_config</code> is the configuration object for the deployment. In this case, we are using <code>HfTextGenerationConfig</code> that is specific to <code>HfTextGenerationDeployment</code>. </li> <li><code>model_id</code> is the Hugging Face model ID that you want to deploy. You can find the model ID on the Hugging Face model hub.</li> <li><code>model_kwargs</code> is a dictionary of additional keyword arguments that you want to pass to the model. In this case, we are using <code>trust_remote_code</code> as it's required by the model and <code>quantization_config</code> to load the model in 4-bit precision. Check the model documentation for the required keyword arguments.</li> </ul>"},{"location":"pages/tutorial/#summarization-endpoint","title":"Summarization Endpoint","text":"<p>Now that we have the LLM, we can define an endpoint to summarize the transcript and use the LLM deployment in it.</p> <p>Here is an example of how to define a summarization endpoint:</p> <pre><code>class SummarizeVideoEndpointOutput(TypedDict):\n    \"\"\"Summarize video endpoint output.\"\"\"\n    summary: Annotated[str, Field(description=\"The summary of the video.\")]\n\nclass SummarizeVideoEndpoint(Endpoint):\n    \"\"\"Summarize video endpoint.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        await super().initialize()\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n        self.llm_handle = await AanaDeploymentHandle.create(\"llm_deployment\")\n\n    async def run(self, video: VideoInput) -&gt; SummarizeVideoEndpointOutput:\n        \"\"\"Summarize video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        transcription_text = transcription[\"transcription\"].text\n        dialog = ChatDialog(\n            messages=[\n                ChatMessage(\n                    role=\"system\",\n                    content=\"You are a helpful assistant that can summarize audio transcripts.\",\n                ),\n                ChatMessage(\n                    role=\"user\",\n                    content=f\"Summarize the following video transcript into a list of bullet points: {transcription_text}\",\n                ),\n            ]\n        )\n        summary_response = await self.llm_handle.chat(dialog=dialog)\n        summary_message: ChatMessage = summary_response[\"message\"]\n        summary = summary_message.content\n        return {\"summary\": summary}\n</code></pre> <p>In this example, we define a new endpoint class <code>SummarizeVideoEndpoint</code> that inherits from the <code>Endpoint</code> class. The class has two main methods: <code>initialize</code> and <code>run</code>.</p> <p>In the <code>initialize</code> method, we create handles for the ASR and LLM deployments.</p> <p><code>run</code> method is the main method of the endpoint. The <code>run</code> method should be annotated with the input and output types. In this case, we only have one input <code>video</code> of type <code>VideoInput</code> and the output is a dictionary with one key <code>summary</code> of type <code>str</code>. The output type is defined as a <code>TypedDict</code> called <code>SummarizeVideoEndpointOutput</code>.</p> <p>The <code>run</code> method performs the following steps:</p> <ul> <li>Downloading the video: We use the <code>download_video</code> function from the <code>yt_dlp</code> integration to download the video. The <code>run_remote</code> function is used to run the function as a remote task in the Ray cluster. It is useful to run heavy tasks with <code>run_remote</code> to offload the main thread.</li> <li>Extracting the audio from the video: We use the <code>extract_audio</code> function from the <code>video</code> processor to extract the audio.</li> <li>Transcribing the audio: We call ASR deployment to transcribe the audio. Here we use ASR deployment handle that we created in the <code>initialize</code> method. Calling the deployment is as simple as calling a method on the handle. The call is asynchronous so we use <code>await</code> to wait for the result.</li> <li>Creating a chat dialog: We create a chat dialog with a system message and a user message. The user message contains the transcription text that we want to summarize.</li> <li>Summarizing the transcript: We call the LLM deployment to summarize the transcript. Here we use the LLM deployment handle that we created in the <code>initialize</code> method. The call is similar to the ASR deployment call. We pass the chat dialog to the LLM deployment and wait for the response with <code>await</code>.</li> <li>Returning the summary: We return the summary as a dictionary with one key <code>summary</code>. Just like the type annotation tells us.</li> <li>The summary has type <code>Annotated[str, Field(description=\"The summary of the video.\")]</code> which means it's a string with a description. The description is used to generate the API documentation. You can use <code>Annotated</code> and <code>Field</code> to add more metadata to your types (input and output) to generate better API documentation.</li> </ul>"},{"location":"pages/tutorial/#extending-the-application","title":"Extending the Application","text":"<p>Now that we have the LLM deployment and the summarization endpoint, we can extend the application to include the LLM deployment and the summarization endpoint.</p> <p>Here is an example of how to extend the application:</p> <pre><code>aana_app = AanaSDK(name=\"summarize_video_app\")\n\n# Register the ASR deployment.\naana_app.register_deployment(name=\"asr_deployment\", instance=asr_deployment)\n# Register the LLM deployment.\naana_app.register_deployment(name=\"llm_deployment\", instance=llm_deployment)\n\n# Register the transcribe video endpoint.\naana_app.register_endpoint(\n    name=\"transcribe_video\",\n    path=\"/video/transcribe\",\n    summary=\"Transcribe a video\",\n    endpoint_cls=TranscribeVideoEndpoint,\n)\n# Register the summarize video endpoint.\naana_app.register_endpoint(\n    name=\"summarize_video\",\n    path=\"/video/summarize\",\n    summary=\"Summarize a video transcript\",\n    endpoint_cls=SummarizeVideoEndpoint,\n)\n\naana_app.connect()  # Connects to the Ray cluster or starts a new one.\naana_app.migrate()  # Runs the migrations to create the database tables.\naana_app.deploy()   # Deploys the application.\n</code></pre> <p>In this example, we define a new application called <code>summarize_video_app</code>. We register the ASR and LLM deployments and the transcribe and summarize video endpoints.</p> <p>Here is the full code for the application:</p> <pre><code>from collections.abc import AsyncGenerator\nfrom typing import Annotated, TypedDict\n\nfrom pydantic import Field\nfrom transformers import BitsAndBytesConfig\n\nfrom aana.api.api_generation import Endpoint\nfrom aana.core.models.chat import ChatDialog, ChatMessage\nfrom aana.core.models.video import VideoInput\nfrom aana.deployments.aana_deployment_handle import AanaDeploymentHandle\nfrom aana.deployments.hf_text_generation_deployment import (\n    HfTextGenerationConfig,\n    HfTextGenerationDeployment,\n)\nfrom aana.deployments.whisper_deployment import (\n    WhisperComputeType,\n    WhisperConfig,\n    WhisperDeployment,\n    WhisperModelSize,\n    WhisperOutput,\n)\nfrom aana.integrations.external.yt_dlp import download_video\nfrom aana.processors.remote import run_remote\nfrom aana.processors.video import extract_audio\nfrom aana.sdk import AanaSDK\n\n# Define the model deployments.\nasr_deployment = WhisperDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=WhisperConfig(\n        model_size=WhisperModelSize.MEDIUM,\n        compute_type=WhisperComputeType.FLOAT16,\n    ).model_dump(mode=\"json\"),\n)\n\nllm_deployment = HfTextGenerationDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=HfTextGenerationConfig(\n        model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n        model_kwargs={\n            \"trust_remote_code\": True,\n            \"quantization_config\": BitsAndBytesConfig(\n                load_in_8bit=False, load_in_4bit=True\n            ),\n        },\n    ).model_dump(mode=\"json\"),\n)\n\n\ndeployments = [\n    {\"name\": \"asr_deployment\", \"instance\": asr_deployment},\n    {\"name\": \"llm_deployment\", \"instance\": llm_deployment},\n]\n\n\nclass SummarizeVideoEndpointOutput(TypedDict):\n    \"\"\"Summarize video endpoint output.\"\"\"\n\n    summary: Annotated[str, Field(description=\"The summary of the video.\")]\n\n\n# Define the endpoint to transcribe the video.\nclass TranscribeVideoEndpoint(Endpoint):\n    \"\"\"Transcribe video endpoint.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        await super().initialize()\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n\n    async def run(self, video: VideoInput) -&gt; WhisperOutput:\n        \"\"\"Transcribe video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        return transcription\n\n\nclass SummarizeVideoEndpoint(Endpoint):\n    \"\"\"Summarize video endpoint.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        await super().initialize()\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n        self.llm_handle = await AanaDeploymentHandle.create(\"llm_deployment\")\n\n    async def run(self, video: VideoInput) -&gt; SummarizeVideoEndpointOutput:\n        \"\"\"Summarize video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        transcription_text = transcription[\"transcription\"].text\n        dialog = ChatDialog(\n            messages=[\n                ChatMessage(\n                    role=\"system\",\n                    content=\"You are a helpful assistant that can summarize audio transcripts.\",\n                ),\n                ChatMessage(\n                    role=\"user\",\n                    content=f\"Summarize the following video transcript into a list of bullet points: {transcription_text}\",\n                ),\n            ]\n        )\n        summary_response = await self.llm_handle.chat(dialog=dialog)\n        summary_message: ChatMessage = summary_response[\"message\"]\n        summary = summary_message.content\n        return {\"summary\": summary}\n\nendpoints = [\n    {\n        \"name\": \"transcribe_video\",\n        \"path\": \"/video/transcribe\",\n        \"summary\": \"Transcribe a video\",\n        \"endpoint_cls\": TranscribeVideoEndpoint,\n    },\n    {\n        \"name\": \"summarize_video\",\n        \"path\": \"/video/summarize\",\n        \"summary\": \"Summarize a video\",\n        \"endpoint_cls\": SummarizeVideoEndpoint,\n    },\n]\n\naana_app = AanaSDK(name=\"summarize_video_app\")\n\nfor deployment in deployments:\n    aana_app.register_deployment(**deployment)\n\nfor endpoint in endpoints:\n    aana_app.register_endpoint(**endpoint)\n\nif __name__ == \"__main__\":\n    aana_app.connect()  # Connects to the Ray cluster or starts a new one.\n    aana_app.migrate()  # Runs the migrations to create the database tables.\n    aana_app.deploy()  # Deploys the application.\n</code></pre> <p>Now you can run the application as described in the previous section and send a request to summarize the video transcript.</p> <pre><code>curl -X POST http://127.0.0.1:8000/video/summarize -Fbody='{\"video\":{\"url\":\"https://www.youtube.com/watch?v=VhJFyyukAzA\"}}'\n</code></pre> <p>We will get a response with the summary key containing the bullet points on how to make perfect scrambled eggs.</p> <pre><code>- Never season eggs before cooking to prevent them from turning gray and watery\n- Fill the pan 3/4 full and clean it as you go to ensure a clean cooking surface\n- Do not whisk eggs before cooking to avoid over-mixing\n- Cook eggs off the heat and continue stirring to control the cooking process\n- Use butter for flavor and to create a non-stick surface\n- Stop cooking eggs as soon as they reach the desired level of doneness to prevent rubbery texture\n- Add a teaspoon of sour cream or creme fraiche for extra creaminess\n- Season with salt and pepper at the end for taste\n- Stir continuously for fluffy and light scrambled eggs\n- Avoid seasoning or whisking eggs before cooking for the best results\n- Clean the pan throughout the cooking process for optimal results\n- Use a non-stick pan and a spatula for easy cooking and stirring\n- Cook eggs off the heat and continue stirring to maintain the desired texture\n- Add sour cream or creme fraiche for added creaminess and flavor\n</code></pre>"},{"location":"pages/tutorial/#streaming-llm-output","title":"Streaming LLM Output","text":"<p>In the example above, we used the <code>chat</code> method to interact with the LLM model. The <code>chat</code> method returns a single response message once the model has finished processing the input. However, in some cases, you may want to stream the output from the LLM model as it is being generated.</p> <p>Our LLM deployments support streaming output using the <code>chat_stream</code> method. It's implemented as an asynchronous generator that yields text as it is generated by the model. </p> <pre><code>async for chunk in llm_handle.chat_stream(dialog=dialog):\n    print(chunk)\n</code></pre> <p>Now we can modify the <code>SummarizeVideoEndpoint</code> to stream the output from the LLM model. For demonstration purposes, we will create a new endpoint <code>SummarizeVideoStreamEndpoint</code> that is a streaming version of the <code>SummarizeVideoEndpoint</code>.</p> <pre><code>class SummarizeVideoStreamEndpointOutput(TypedDict):\n    \"\"\"Summarize video endpoint output.\"\"\"\n\n    text: Annotated[str, Field(description=\"The text chunk.\")]\n\nclass SummarizeVideoStreamEndpoint(Endpoint):\n    \"\"\"Summarize video endpoint with streaming output.\"\"\"\n\n    async def initialize(self):\n        \"\"\"Initialize the endpoint.\"\"\"\n        await super().initialize()\n        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n        self.llm_handle = await AanaDeploymentHandle.create(\"llm_deployment\")\n\n    async def run(\n        self, video: VideoInput\n    ) -&gt; AsyncGenerator[SummarizeVideoStreamEndpointOutput, None]:\n        \"\"\"Summarize video.\"\"\"\n        video_obj = await run_remote(download_video)(video_input=video)\n        audio = extract_audio(video=video_obj)\n        transcription = await self.asr_handle.transcribe(audio=audio)\n        transcription_text = transcription[\"transcription\"].text\n        dialog = ChatDialog(\n            messages=[\n                ChatMessage(\n                    role=\"system\",\n                    content=\"You are a helpful assistant that can summarize audio transcripts.\",\n                ),\n                ChatMessage(\n                    role=\"user\",\n                    content=f\"Summarize the following video transcript into a list of bullet points: {transcription_text}\",\n                ),\n            ]\n        )\n        async for chunk in self.llm_handle.chat_stream(dialog=dialog):\n            chunk_text = chunk[\"text\"]\n            yield {\"text\": chunk_text}\n</code></pre> <p>The difference between the non-streaming version and the streaming version:</p> <ul> <li><code>run</code> method is now an asynchronous generator that yields <code>SummarizeVideoEndpointOutput</code> objects: <code>AsyncGenerator[SummarizeVideoEndpointOutput, None]</code>. If you want endpoint to be able to stream output, you need to define the output type as an asynchronous generator to let the SDK know that the endpoint will be streaming output.</li> <li>We use the <code>chat_stream</code> method to interact with the LLM model. The <code>chat_stream</code> method returns an asynchronous generator that yields text as it is generated by the model. We use <code>async for</code> to iterate over the generator and yield the text chunks as they are generated.</li> <li>We use <code>yield</code> to yield the text chunks as they are generated by the LLM model instead of using <code>return</code> to return a single response.</li> <li>The output dictionary now contains a single key <code>text</code> that contains the text chunk generated by the LLM model instead of a single key <code>summary</code>.</li> </ul> <p>That's it! Now you have a streaming version of the summarization endpoint and you can display the output as it is generated by the LLM model.</p> <pre><code>python -c \"\nimport requests, json;\n[print(json.loads(c)['text'], end='') \n for c in requests.post(\n    'http://127.0.0.1:8000/video/summarize_stream', \n    data={'body': json.dumps({'video': {'url': 'https://www.youtube.com/watch?v=VhJFyyukAzA'}})}, \n    stream=True).iter_content(chunk_size=None)]\n\"\n</code></pre>"},{"location":"pages/tutorial/#app-template","title":"App Template","text":"<p>The best way to start building a new application is to use Aana App Template. It is a GitHub template repository that you can use to create a new repository with the same directory structure and files as the template. It will help you get started with the Aana SDK and provide you with a basic structure for your application and its dependencies.</p> <p>Let's create a new project using the Aana App Template:</p>"},{"location":"pages/tutorial/#create-a-new-repository","title":"Create a New Repository","text":"<p>Go to the Aana App Template. Choose a name for your repository and fill in the other details. Click on the \"Create repository\" button. GitHub will create a new repository with the same directory structure and files as the template. There is a GitHub Actions workflow that will run after the repository is created to modify the project name to match the repository name. It is pretty fast but make sure it's finished before you proceed.</p>"},{"location":"pages/tutorial/#clone-the-repository","title":"Clone the Repository","text":"<p>Clone the repository to your local machine:</p> <pre><code>git clone &lt;repository-url&gt;\ncd &lt;repository-name&gt;\n</code></pre>"},{"location":"pages/tutorial/#create-the-application","title":"Create the Application","text":"<p>For our example project we need to adjust a few things.</p>"},{"location":"pages/tutorial/#deployments_1","title":"Deployments","text":"<p>Define the deployments in the <code>configs/deployments.py</code>. </p> <p>For our summarization project it looks like this: </p> <pre><code>from transformers import BitsAndBytesConfig\nfrom aana.deployments.hf_text_generation_deployment import HfTextGenerationConfig, HfTextGenerationDeployment\nfrom aana.deployments.whisper_deployment import WhisperComputeType, WhisperConfig, WhisperDeployment, WhisperModelSize\n\nasr_deployment = WhisperDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=WhisperConfig(\n        model_size=WhisperModelSize.MEDIUM,\n        compute_type=WhisperComputeType.FLOAT16,\n    ).model_dump(mode=\"json\"),\n)\n\nllm_deployment = HfTextGenerationDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=HfTextGenerationConfig(\n        model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n        model_kwargs={\n            \"trust_remote_code\": True,\n            \"quantization_config\": BitsAndBytesConfig(\n                load_in_8bit=False, load_in_4bit=True\n            ),\n        },\n    ).model_dump(mode=\"json\"),\n)\n\n\ndeployments: list[dict] = [\n    {\"name\": \"asr_deployment\", \"instance\": asr_deployment},\n    {\"name\": \"llm_deployment\", \"instance\": llm_deployment},\n]\n</code></pre> <p>See <code>aana_summarize_video/configs/deployments.py</code>.</p>"},{"location":"pages/tutorial/#endpoints_1","title":"Endpoints","text":"<p>The endpoints are defined in the <code>aana_summarize_video/endpoints</code> directory. It is a good practice to define each endpoint in a separate file. In our case, we will define 3 endpoints in 3 separate files: <code>transcribe_video.py</code>, <code>summarize_video.py</code>, and <code>summarize_video_stream.py</code>.</p> <p>We also need to register the endpoints. The list of endpoints is defined in the <code>configs/endpoints.py</code>.</p> <p>For this project we need to register 3 endpoints:</p> <pre><code>from aana_summarize_video.endpoints.summarize_video import SummarizeVideoEndpoint\nfrom aana_summarize_video.endpoints.summarize_video_stream import SummarizeVideoStreamEndpoint\nfrom aana_summarize_video.endpoints.transcribe_video import TranscribeVideoEndpoint\n\nendpoints: list[dict] = [\n    {\n        \"name\": \"transcribe_video\",\n        \"path\": \"/video/transcribe\",\n        \"summary\": \"Transcribe a video\",\n        \"endpoint_cls\": TranscribeVideoEndpoint,\n    },\n    {\n        \"name\": \"summarize_video\",\n        \"path\": \"/video/summarize\",\n        \"summary\": \"Summarize a video\",\n        \"endpoint_cls\": SummarizeVideoEndpoint,\n    },\n    {\n        \"name\": \"summarize_video_stream\",\n        \"path\": \"/video/summarize_stream\",\n        \"summary\": \"Summarize a video with streaming output\",\n        \"endpoint_cls\": SummarizeVideoStreamEndpoint,\n    },\n]\n</code></pre> <p>See <code>aana_summarize_video/configs/endpoints.py</code>.</p>"},{"location":"pages/tutorial/#run-the-application","title":"Run the Application","text":"<p>Now you can run the application:</p> <pre><code>aana deploy aana_summarize_video.app:aana_app\n</code></pre> <p>Or if you want to be on the safe side, you can run the application with <code>poetry run</code> and CUDA_VISIBLE_DEVICES set to the GPU index you want to use:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 poetry run aana deploy aana_summarize_video.app:aana_app\n</code></pre> <p>Once the application is running, you can send a request to transcribe and summarize a video as described in the previous sections.</p>"},{"location":"pages/tutorial/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we have walked you through the process of creating a new project with Aana SDK. We have reviewed the video transcription application and extended it to include the LLM model for summarization and a new endpoint. We have also demonstrated how to stream the output from the LLM model. You can use this tutorial as a reference to build your own applications with Aana SDK. </p> <p>The full code for the application is available in the projects directory.</p>"},{"location":"pages/model_hub/","title":"Model Hub","text":"<p>Model deployment is a crucial part of the machine learning workflow. Aana SDK uses concept of deployments to serve models.</p> <p>The deployments are \"recipes\" that can be used to deploy models. With the same deployment, you can deploy multiple different models by providing specific configurations.</p> <p>Aana SDK comes with a set of predefined deployments, like VLLMDeployment for serving Large Language Models (LLMs) with vLLM library or WhisperDeployment for automatic Speech Recognition (ASR) based on the faster-whisper library. </p> <p>Each deployment has its own configuration class that specifies which model to deploy and with which parameters. </p> <p>The model hub provides a collection of configurations for different models that can be used with the predefined deployments. </p> <p>The full list of predefined deployments can be found in the Deployments.</p> <p>Tip</p> <p>The Model Hub provides only a subset of the available models. You can deploy a lot more models using predefined deployments. For example, Hugging Face Pipeline Deployment is a generic deployment that can be used to deploy any model from the Hugging Face Model Hub that can be used with Hugging Face Pipelines. It would be impossible to list all the models that can be deployed with this deployment.</p> <p>Tip</p> <p>The SDK is not limited to the predefined deployments. You can create your own deployment.</p>"},{"location":"pages/model_hub/#how-to-use-the-model-hub","title":"How to Use the Model Hub","text":"<p>There are a few ways to use the Model Hub (from the simplest to the most advanced):</p> <ul> <li> <p>Find the model configuration you are interested in and copy the configuration code to your project.</p> </li> <li> <p>Use the provided examples as a starting point to create your own configurations for existing deployments.</p> </li> <li> <p>Create a new deployment with your own configuration.</p> </li> </ul> <p>See Tutorial for more information on how to use the deployments.</p>"},{"location":"pages/model_hub/#models-by-category","title":"Models by Category","text":"<ul> <li>Text Generation Models (LLMs)</li> <li>Image-to-Text Models</li> <li>Half-Quadratic Quantization Models</li> <li>Automatic Speech Recognition (ASR) Models</li> <li>Hugging Face Pipeline Models</li> </ul>"},{"location":"pages/model_hub/asr/","title":"Automatic Speech Recognition (ASR) Models","text":"<p>WhisperDeployment allows you to transcribe or translate audio with Whisper models. The deployment is based on the faster-whisper library.</p> <p>Tip</p> <p>To use Whisper deployment, install required libraries with <code>pip install faster-whisper</code> or include extra dependencies using <code>pip install aana[asr]</code>.</p> <p>WhisperConfig is used to configure the Whisper deployment.</p>"},{"location":"pages/model_hub/asr/#aana.deployments.whisper_deployment.WhisperConfig","title":"aana.deployments.whisper_deployment.WhisperConfig","text":"<p>Attributes:</p> <ul> <li> <code>model_size</code>               (<code>WhisperModelSize | str</code>)           \u2013            <p>The whisper model size. Defaults to WhisperModelSize.TURBO.</p> </li> <li> <code>compute_type</code>               (<code>WhisperComputeType</code>)           \u2013            <p>The compute type. Defaults to WhisperComputeType.FLOAT16.</p> </li> </ul>"},{"location":"pages/model_hub/asr/#example-configurations","title":"Example Configurations","text":"<p>As an example, let's see how to configure the Whisper deployment for the Whisper Medium model.</p> <p>Whisper Medium</p> <pre><code>from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType\n\nWhisperDeployment.options(\n    num_replicas=1,\n    max_ongoing_requests=1000,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=WhisperConfig(\n        model_size=WhisperModelSize.MEDIUM,\n        compute_type=WhisperComputeType.FLOAT16,\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Model size is the one of the Whisper model sizes available in the <code>faster-whisper</code> library or HuggingFace model hub in Ctranslate2 format. <code>compute_type</code> is the data type to be used for the model.</p> <p>Here are some other possible configurations for the Whisper deployment:</p> Whisper Turbo on GPU <pre><code>from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType\n\n# for CPU do not specify num_gpus and use FLOAT32 compute type\nWhisperDeployment.options(\n    num_replicas=1,\n    max_ongoing_requests=1000,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=WhisperConfig(\n        model_size=WhisperModelSize.TURBO,\n        compute_type=WhisperComputeType.FLOAT16,\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> Whisper Tiny on CPU <pre><code>from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType\n\n# for CPU do not specify num_gpus and use FLOAT32 compute type\nWhisperDeployment.options(\n    num_replicas=1,\n    user_config=WhisperConfig(\n        model_size=WhisperModelSize.TINY,\n        compute_type=WhisperComputeType.FLOAT32,\n    ).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/model_hub/asr/#available-transcription-methods-in-aana-sdk","title":"Available Transcription Methods in Aana SDK","text":"<p>Below are the different transcription methods available in the Aana SDK:</p> <ol> <li> <p><code>transcribe</code> Method</p> <ul> <li>Description: This method is used to get the complete transcription output at once after processing the entire audio.</li> <li>Usage Example:  <pre><code>transcription = await self.asr_handle.transcribe(audio=audio, params=whisper_params)\n# Further processing...\n</code></pre></li> </ul> </li> <li> <p><code>transcribe_stream</code> Method</p> <ul> <li>Description: This method allows for segment-by-segment transcription as they become available.</li> <li>Usage Example:  <pre><code>stream = handle.transcribe_stream(audio=audio, params=whisper_params)\nasync for chunk in stream:\n    # Further processing...\n</code></pre></li> </ul> </li> <li> <p><code>transcribe_in_chunks</code> Method</p> <ul> <li>Description: This method performs batched inference, returning one batch of segments at a time. It is up to 4x faster than sequential methods.</li> <li>Usage Example:  <pre><code>batched_stream = handle.transcribe_in_chunks(audio=audio, params=batched_whisper_params)\nasync for chunk in batched_stream:\n    # Further processing...\n</code></pre></li> </ul> </li> </ol>"},{"location":"pages/model_hub/asr/#differences-between-whisperparams-and-batchedwhisperparams","title":"Differences Between <code>WhisperParams</code> and <code>BatchedWhisperParams</code>","text":"<p>Both <code>WhisperParams</code> and <code>BatchedWhisperParams</code> are used to configure the Whisper speech-to-text model in sequential and batched inferences respectively.</p> <ul> <li> <p>Common Parameters:   Both classes share common attributes such as <code>language</code>, <code>beam_size</code>, <code>best_of</code>, and <code>temperature</code>.</p> </li> <li> <p>Key Differences:   WhisperParams includes additional attributes such as <code>word_timestamps</code> and <code>vad_filter</code>, which provide word-level timestamp extraction and voice activity detection filtering.</p> </li> </ul> <p>Refer to the respective class documentation for detailed attributes and usage.</p>"},{"location":"pages/model_hub/asr/#diarized-asr","title":"Diarized ASR","text":"<p>Diarized transcription can be generated by using WhisperDeployment and PyannoteSpeakerDiarizationDeployment and combining the timelines using post processing with PostProcessingForDiarizedAsr.</p> <p>Example configuration for the PyannoteSpeakerDiarization model is available at Speaker Diarization model hub.</p> <p>You can simply define the model deployments and the endpoint to transcribe the video with diarization. Below code snippet shows the how to combine the outputs from ASR and diarization deployments:</p> <p><pre><code>from aana.processors.speaker import PostProcessingForDiarizedAsr\nfrom aana.core.models.base import pydantic_to_dict\n\n\n# diarized transcript requires word_timestamps from ASR\nwhisper_params.word_timestamps = True\n\n# asr_handle is an AanaDeploymentHandle for WhisperDeployment\ntranscription = await self.asr_handle.transcribe(\n    audio=audio, params=whisper_params\n)\n\n# diar_handle is an AanaDeploymentHandle for PyannoteSpeakerDiarizationDeployment\ndiarized_output = await self.diar_handle.diarize(\n    audio=audio, params=diar_params\n)\n\nupdated_segments = PostProcessingForDiarizedAsr.process(\n    diarized_segments=diarized_output[\"segments\"],\n    transcription_segments=transcription[\"segments\"],\n)\n\n# updated_segments will have speaker information as well:\n\n# [AsrSegment(text=' Hello. Hello.', \n#            time_interval=TimeInterval(start=6.38, end=7.84), \n#            confidence=0.8329984157521475, \n#            no_speech_confidence=0.012033582665026188, \n#            words=[AsrWord(word=' Hello.', speaker='SPEAKER_01',time_interval=TimeInterval(start=6.38, end=7.0), alignment_confidence=0.6853185296058655), \n#                   AsrWord(word=' Hello.', speaker='SPEAKER_01', time_interval=TimeInterval(start=7.5, end=7.84), alignment_confidence=0.7124693989753723)], \n#           speaker='SPEAKER_01'), \n#\n# AsrSegment(text=\" Oh, hello. I didn't know you were there.\", \n#            time_interval=TimeInterval(start=8.3, end=9.68), \n#            confidence=0.8329984157521475, \n#            no_speech_confidence=0.012033582665026188, \n#            words=[AsrWord(word=' Oh,', speaker='SPEAKER_02', time_interval=TimeInterval(start=8.3, end=8.48), alignment_confidence=0.8500092029571533), \n#                   AsrWord(word=' hello.', speaker='SPEAKER_02', time_interval=TimeInterval(start=8.5, end=8.76), alignment_confidence=0.9408962726593018), ...], \n#            speaker='SPEAKER_02'), \n# ...\n# ]\n</code></pre> An example notebook on diarized transcription is available at notebooks/diarized_transcription_example.ipynb.</p>"},{"location":"pages/model_hub/hf_pipeline/","title":"Hugging Face Pipeline Models","text":"<p>Hugging Face Pipeline deployment allows you to serve almost any model from the Hugging Face Hub. It is a wrapper for Hugging Face Pipelines so you can deploy and scale almost any model from the Hugging Face Hub with a few lines of code.</p> <p>Tip</p> <p>To use HF Pipeline deployment, install required libraries with <code>pip install transformers</code> or include extra dependencies using <code>pip install aana[transformers]</code>.</p> <p>HfPipelineConfig is used to configure the Hugging Face Pipeline deployment.</p>"},{"location":"pages/model_hub/hf_pipeline/#aana.deployments.hf_pipeline_deployment.HfPipelineConfig","title":"aana.deployments.hf_pipeline_deployment.HfPipelineConfig","text":"<p>Attributes:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>The model ID on Hugging Face.</p> </li> <li> <code>task</code>               (<code>str | None</code>)           \u2013            <p>The task name. If not provided, the task is inferred from the model ID. Defaults to None.</p> </li> <li> <code>model_kwargs</code>               (<code>CustomConfig</code>)           \u2013            <p>The model keyword arguments. Defaults to {}.</p> </li> <li> <code>pipeline_kwargs</code>               (<code>CustomConfig</code>)           \u2013            <p>The pipeline keyword arguments. Defaults to {}.</p> </li> <li> <code>generation_kwargs</code>               (<code>CustomConfig</code>)           \u2013            <p>The generation keyword arguments. Defaults to {}.</p> </li> </ul>"},{"location":"pages/model_hub/hf_pipeline/#example-configurations","title":"Example Configurations","text":"<p>As an example, let's see how to configure the Hugging Face Pipeline deployment to serve Salesforce BLIP-2 OPT-2.7b model.</p> <p>BLIP-2 OPT-2.7b</p> <pre><code>from transformers import BitsAndBytesConfig\nfrom aana.deployments.hf_pipeline_deployment import HfPipelineConfig, HfPipelineDeployment\n\nHfPipelineDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=HfPipelineConfig(\n        model_id=\"Salesforce/blip2-opt-2.7b\",\n        task=\"image-to-text\",\n        model_kwargs={\n            \"quantization_config\": BitsAndBytesConfig(load_in_8bit=False, load_in_4bit=True),\n        },\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Model ID is the Hugging Face model ID. <code>task</code> is one of the Hugging Face Pipelines tasks that the model can perform. We deploy the model with 4-bit quantization by setting <code>quantization_config</code> in the <code>model_kwargs</code> dictionary. You can pass extra arguments to the model in the <code>model_kwargs</code> dictionary.</p>"},{"location":"pages/model_hub/image_to_text/","title":"Image-to-Text Models","text":"<p>Aana SDK has two deployments to serve image-to-text models:</p> <ul> <li> <p>Idefics2Deployment: used to deploy the Idefics2 models. Idefics2 is an open multimodal model that accepts arbitrary sequences of image and text inputs and produces text outputs.</p> </li> <li> <p>HFBlip2Deployment: used to deploy the BLIP-2 models. <code>HFBlip2Deployment</code> only supports image captioning capabilities of the BLIP-2 model.</p> </li> </ul> <p>Tip</p> <p>To use Idefics2 or HF BLIP2 deployments, install required libraries with <code>pip install transformers</code> or include extra dependencies using <code>pip install aana[transformers]</code>.</p>"},{"location":"pages/model_hub/image_to_text/#idefics2-deployment","title":"Idefics2 Deployment","text":"<p>Idefics2Config is used to configure the Idefics2 deployment.</p>"},{"location":"pages/model_hub/image_to_text/#aana.deployments.idefics_2_deployment.Idefics2Config","title":"aana.deployments.idefics_2_deployment.Idefics2Config","text":"<p>Attributes:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>The model ID on HuggingFace.</p> </li> <li> <code>dtype</code>               (<code>Dtype</code>)           \u2013            <p>The data type. Defaults to Dtype.AUTO.</p> </li> <li> <code>enable_flash_attention_2</code>               (<code>bool | None</code>)           \u2013            <p>Use Flash Attention 2. If None, Flash Attention 2 wii be enabled if available. Defaults to None.</p> </li> <li> <code>model_kwargs</code>               (<code>CustomConfig</code>)           \u2013            <p>The extra model keyword arguments. Defaults to {}.</p> </li> <li> <code>processor_kwargs</code>               (<code>CustomConfig</code>)           \u2013            <p>The extra processor keyword arguments. Defaults to {}.</p> </li> <li> <code>default_sampling_params</code>               (<code>SamplingParams</code>)           \u2013            <p>The default sampling parameters. Defaults to SamplingParams(temperature=1.0, max_tokens=256).</p> </li> </ul>"},{"location":"pages/model_hub/image_to_text/#example-configurations","title":"Example Configurations","text":"<p>As an example, let's see how to configure the Idefics2 deployment for the Hugging Face Idefics2 8B model.</p> <p>Hugging Face Idefics2 8B</p> <pre><code>from aana.core.models.types import Dtype\nfrom aana.deployments.idefics_2_deployment import Idefics2Config, Idefics2Deployment\n\nIdefics2Deployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.85},\n    user_config=Idefics2Config(\n        model_id=\"HuggingFaceM4/idefics2-8b\",\n        dtype=Dtype.FLOAT16,\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Model is the Hugging Face model ID. <code>dtype=Dtype.FLOAT16</code> is used to specify the data type to be used for the model. Idefics2 supports <code>Dtype.BFLOAT16</code> and it is generally faster but not supported by all GPUs. You can define other model arguments in the <code>model_kwargs</code> dictionary.</p>"},{"location":"pages/model_hub/image_to_text/#blip-2-deployment","title":"BLIP-2 Deployment","text":"<p>HFBlip2Config is used to configure the BLIP-2 deployment.    </p>"},{"location":"pages/model_hub/image_to_text/#aana.deployments.hf_blip2_deployment.HFBlip2Config","title":"aana.deployments.hf_blip2_deployment.HFBlip2Config","text":"<p>Attributes:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>The model ID on HuggingFace.</p> </li> <li> <code>dtype</code>               (<code>Dtype</code>)           \u2013            <p>The data type. Defaults to Dtype.AUTO.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>The batch size. Defaults to 1.</p> </li> <li> <code>num_processing_threads</code>               (<code>int</code>)           \u2013            <p>The number of processing threads. Defaults to 1.</p> </li> <li> <code>max_new_tokens</code>               (<code>int</code>)           \u2013            <p>The maximum numbers of tokens to generate. Defaults to 64.</p> </li> </ul>"},{"location":"pages/model_hub/image_to_text/#example-configurations_1","title":"Example Configurations","text":"<p>As an example, let's see how to configure the BLIP-2 deployment for the Salesforce BLIP-2 OPT-2.7b model.</p> <p>BLIP-2 OPT-2.7b</p> <pre><code>from aana.core.models.types import Dtype\nfrom aana.deployments.hf_blip2_deployment import HFBlip2Config, HFBlip2Deployment\n\nHFBlip2Deployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=HFBlip2Config(\n        model_id=\"Salesforce/blip2-opt-2.7b\",\n        dtype=Dtype.FLOAT16,\n        batch_size=2,\n        num_processing_threads=2,\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Model is the Hugging Face model ID. We use <code>dtype=Dtype.FLOAT16</code> to load the model in half-precision for faster inference and lower memory usage. <code>batch_size</code> and <code>num_processing_threads</code> are used to configure the batch size (the bigger the batch size, the more memory is required) and the number of processing threads respectively.</p>"},{"location":"pages/model_hub/speaker_recognition/","title":"Speaker Recognition","text":""},{"location":"pages/model_hub/speaker_recognition/#speaker-diarization-sd-models","title":"Speaker Diarization (SD) Models","text":"<p>PyannoteSpeakerDiarizationDeployment allows you to diarize the audio for speakers audio with pyannote models. The deployment is based on the pyannote.audio library.</p> <p>Tip</p> <p>To use Pyannotate Speaker Diarization deployment, install required libraries with <code>pip install pyannote-audio</code> or include extra dependencies using <code>pip install aana[asr]</code>.</p> <p>PyannoteSpeakerDiarizationConfig is used to configure the Speaker Diarization deployment.</p>"},{"location":"pages/model_hub/speaker_recognition/#aana.deployments.pyannote_speaker_diarization_deployment.PyannoteSpeakerDiarizationConfig","title":"aana.deployments.pyannote_speaker_diarization_deployment.PyannoteSpeakerDiarizationConfig","text":"<p>Attributes:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>name of the speaker diarization pipeline.</p> </li> <li> <code>sample_rate</code>               (<code>int</code>)           \u2013            <p>The sample rate of the audio. Defaults to 16000.</p> </li> </ul>"},{"location":"pages/model_hub/speaker_recognition/#accessing-gated-models","title":"Accessing Gated Models","text":"<p>The PyAnnote speaker diarization models are gated, requiring special access. To use these models:</p> <ol> <li> <p>Request Access:     Visit the PyAnnote Speaker Diarization 3.1 model page and Pyannote Speaker Segmentation 3.0 model page on Hugging Face. Log in, fil out the forms, and request access.</p> </li> <li> <p>Approval:  </p> <ul> <li>If automatic, access is granted immediately.</li> <li>If manual, wait for the model authors to approve your request.</li> </ul> </li> <li> <p>Set Up the SDK:     After approval, add your Hugging Face access token to your <code>.env</code> file by setting the <code>HF_TOKEN</code> variable:</p> <pre><code>HF_TOKEN=your_huggingface_access_token\n</code></pre> <p>To get your Hugging Face access token, visit the Hugging Face Settings - Tokens.</p> </li> </ol>"},{"location":"pages/model_hub/speaker_recognition/#example-configurations","title":"Example Configurations","text":"<p>As an example, let's see how to configure the Pyannote Speaker Diarization deployment for the Speaker Diarization-3.1 model.</p> <p>Speaker diarization-3.1</p> <pre><code>from aana.deployments.pyannote_speaker_diarization_deployment import PyannoteSpeakerDiarizationDeployment, PyannoteSpeakerDiarizationConfig\n\nPyannoteSpeakerDiarizationDeployment.options(\n    num_replicas=1,\n    max_ongoing_requests=1000,\n    ray_actor_options={\"num_gpus\": 0.05},\n    user_config=PyannoteSpeakerDiarizationConfig(\n        model_name=(\"pyannote/speaker-diarization-3.1\"),\n        sample_rate=16000,\n    ).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/model_hub/speaker_recognition/#diarized-asr","title":"Diarized ASR","text":"<p>Speaker Diarization output can be combined with ASR to generate transcription with speaker information. Further details and code snippet are available in ASR model hub.</p>"},{"location":"pages/model_hub/text_generation/","title":"Text Generation Models (LLMs)","text":"<p>Aana SDK has three deployments to serve text generation models (LLMs):</p> <ul> <li> <p>VLLMDeployment: allows you to efficiently serve Large Language Models (LLM) and Vision Language Models (VLM) with the vLLM library.</p> </li> <li> <p>HfTextGenerationDeployment: uses the Hugging Face Transformers library to deploy text generation models.</p> </li> <li> <p>HqqTextGenerationDeployment: uses Half-Quadratic Quantization (HQQ) to quantize and deploy text generation models.</p> </li> </ul> <p>All deployments have the same interface and provide similar capabilities. </p>"},{"location":"pages/model_hub/text_generation/#vllm-deployment","title":"vLLM Deployment","text":"<p>vLLM deployment allows you to efficiently serve Large Language Models (LLM) and Vision Language Models (VLM) with the vLLM library.</p> <p>Tip</p> <p>To use vLLM deployment, install required libraries with <code>pip install vllm</code> or include extra dependencies using <code>pip install aana[vllm]</code>.</p> <p>VLLMConfig is used to configure the vLLM deployment.</p>"},{"location":"pages/model_hub/text_generation/#aana.deployments.vllm_deployment.VLLMConfig","title":"aana.deployments.vllm_deployment.VLLMConfig","text":"<p>Attributes:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>The model name.</p> </li> <li> <code>dtype</code>               (<code>Dtype</code>)           \u2013            <p>The data type. Defaults to Dtype.AUTO.</p> </li> <li> <code>quantization</code>               (<code>str | None</code>)           \u2013            <p>The quantization method. Defaults to None.</p> </li> <li> <code>gpu_memory_reserved</code>               (<code>float</code>)           \u2013            <p>The GPU memory reserved for the model in MB.</p> </li> <li> <code>default_sampling_params</code>               (<code>SamplingParams</code>)           \u2013            <p>The default sampling parameters. Defaults to SamplingParams(temperature=0, max_tokens=256).</p> </li> <li> <code>max_model_len</code>               (<code>int | None</code>)           \u2013            <p>The maximum generated text length in tokens. Defaults to None.</p> </li> <li> <code>chat_template</code>               (<code>str | None</code>)           \u2013            <p>The name of the chat template. If not provided, the chat template from the model will be used. Some models may not have a chat template. Defaults to None.</p> </li> <li> <code>enforce_eager</code>               (<code>bool</code>)           \u2013            <p>Whether to enforce eager execution. Defaults to False.</p> </li> <li> <code>engine_args</code>               (<code>CustomConfig</code>)           \u2013            <p>Extra engine arguments. Defaults to {}.</p> </li> </ul>"},{"location":"pages/model_hub/text_generation/#example-configurations","title":"Example Configurations","text":"<p>As an example, let's see how to configure the vLLM deployment for the Meta Llama 3 8B Instruct model. </p> <p>Meta Llama 3 8B Instruct</p> <pre><code>from aana.core.models.sampling import SamplingParams\nfrom aana.core.models.types import Dtype\nfrom aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\n\nVLLMDeployment.options(\n    num_replicas=1,\n    max_ongoing_requests=1000,\n    ray_actor_options={\"num_gpus\": 0.45},\n    user_config=VLLMConfig(\n        model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n        dtype=Dtype.AUTO,\n        gpu_memory_reserved=30000,\n        enforce_eager=True,\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n        ),\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Model name is the Hugging Face model ID. We use <code>Dtype.AUTO</code> to let the deployment choose the best data type for the model. We reserve 30GB of GPU memory for the model. We set <code>enforce_eager=True</code> to helps to reduce memory usage but may harm performance. We also set the default sampling parameters for the model.</p> <p>VLLM deployment also supports Vision Language Models (VLM). Here is an example configuration for the Phi 3.5 Vision Instruct model.</p> <p>Phi 3.5 Vision Instruct</p> <pre><code>from aana.core.models.sampling import SamplingParams\nfrom aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\n\nVLLMDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 1.0},\n    user_config=VLLMConfig(\n        model_id=\"microsoft/Phi-3.5-vision-instruct\",\n        gpu_memory_reserved=12000,\n        enforce_eager=True,\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n        ),\n        max_model_len=2048,\n        engine_args=dict(\n            trust_remote_code=True,\n            max_num_seqs=32,\n            limit_mm_per_prompt={\"image\": 3},\n        ),\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Here are some other example configurations for the VLLM deployment. Keep in mind that the list is not exhaustive. You can deploy any model that is supported by the vLLM library.</p> Llama 2 7B Cha t with AWQ quantization <pre><code>from aana.core.models.sampling import SamplingParams\nfrom aana.core.models.types import Dtype\nfrom aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\n\nVLLMDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=VLLMConfig(\n        model_id=\"TheBloke/Llama-2-7b-Chat-AWQ\",\n        dtype=Dtype.AUTO,\n        quantization=\"awq\",\n        gpu_memory_reserved=13000,\n        enforce_eager=True,\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n        ),\n        chat_template=\"llama2\",\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> InternLM 2.5 7B Chat <pre><code>from aana.core.models.sampling import SamplingParams\nfrom aana.core.models.types import Dtype\nfrom aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\n\nVLLMDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.45},\n    user_config=VLLMConfig(\n        model_id=\"internlm/internlm2_5-7b-chat\",\n        dtype=Dtype.AUTO,\n        gpu_memory_reserved=30000,\n        max_model_len=50000,\n        enforce_eager=True,\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n        ),\n        engine_args={\"trust_remote_code\": True},\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> Phi 3 Mini 4K Instruct <pre><code>from aana.core.models.sampling import SamplingParams\nfrom aana.core.models.types import Dtype\nfrom aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\n\nVLLMDeployment.options(\n    num_replicas=1,\n    max_ongoing_requests=1000,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=VLLMConfig(\n        model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n        dtype=Dtype.AUTO,\n        gpu_memory_reserved=10000,\n        enforce_eager=True,\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n        ),\n        engine_args={\n            \"trust_remote_code\": True,\n        },\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> Qwen2-VL 7B Instruct <p>For LLaVA-NeXT-Video and Qwen2-VL, the latest release of huggingface/transformers doesn\u2019t work yet (as of 18 September 2024), so we need to use a developer version (21fac7abba2a37fae86106f87fcf9974fd1e3830) for now. This can be installed by running the following command:</p> <pre><code>pip install git+https://github.com/huggingface/transformers.git@21fac7abba2a37fae86106f87fcf9974fd1e3830\n</code></pre> <pre><code>from aana.core.models.sampling import SamplingParams\nfrom aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\n\nVLLMDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 1.0},\n    user_config=VLLMConfig(\n        model_id=\"Qwen/Qwen2-VL-7B-Instruct\",\n        gpu_memory_reserved=40000,\n        enforce_eager=True,\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n        ),\n        max_model_len=4096,\n        engine_args=dict(\n            limit_mm_per_prompt={\"image\": 3},\n        ),\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> Pixtral 12B 2409 <p>The model is gated so you need to the model page, request access to the model and set <code>HF_TOKEN</code> environment variable to your Hugging Face API token.</p> <pre><code>from aana.core.models.sampling import SamplingParams\nfrom aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\n\nVLLMDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 1.0},\n    user_config=VLLMConfig(\n        model_id=\"mistralai/Pixtral-12B-2409\",\n        gpu_memory_reserved=40000,\n        enforce_eager=True,\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n        ),\n        max_model_len=4096,\n        engine_args=dict(\n            tokenizer_mode=\"mistral\",\n            limit_mm_per_prompt={\"image\": 3},\n        ),\n    ).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/model_hub/text_generation/#structured-generation","title":"Structured Generation","text":"<p>Structured generation is a feature that allows you to generate structured data using the vLLM deployment forcing LLM to adhere to a specific JSON schema or regular expression pattern.</p> <p>Structured generation is supported only for the vLLM deployment at the moment. </p> <p>To enable structured generation, you need to pass JSON schema or regular expression pattern to <code>SamplingParams</code> object.</p> <pre><code># For JSON schema set json_schema parameter to the JSON schema string\nsampling_params = SamplingParams(json_schema=schema, temperature=0.0, max_tokens=512)\n\n# For regular expression set regex_string parameter to the regular expression pattern\nsampling_params = SamplingParams(regex_string=regex_pattern, temperature=0.0, max_tokens=512)\n\n# Pass the sampling_params to one of the vLLM deployment methods like chat or chat_stream\n# Here handle is an AanaDeploymentHandle for the vLLM deployment.\nresponse = await handle.chat(dialog, sampling_params=sampling_params)\n</code></pre> <p>You can use Pydantic models to generate JSON schema.</p> <pre><code>import json\nfrom pydantic import BaseModel\n\nclass CityDescription(BaseModel):\n    city: str\n    country: str\n    description: str\n\nschema = json.dumps(CityDescription.model_json_schema())\n# {\"properties\": {\"city\": {\"title\": \"City\", \"type\": \"string\"}, \"country\": {\"title\": \"Country\", \"type\": \"string\"}, \"description\": {\"title\": \"Description\", \"type\": \"string\"}}, \"required\": [\"city\", \"country\", \"description\"], \"title\": \"CityDescription\", \"type\": \"object\"}\n</code></pre> <p>You can find detailed tutorials on how to use structured generation in the Structured Generation notebook.</p>"},{"location":"pages/model_hub/text_generation/#hugging-face-text-generation-deployment","title":"Hugging Face Text Generation Deployment","text":"<p>HfTextGenerationDeployment uses the Hugging Face Transformers library to deploy text generation models.</p> <p>Tip</p> <p>To use HF Text Generation deployment, install required libraries with <code>pip install transformers</code> or include extra dependencies using <code>pip install aana[transformers]</code>.</p> <p>HfTextGenerationConfig is used to configure the Hugging Face Text Generation deployment.</p>"},{"location":"pages/model_hub/text_generation/#aana.deployments.hf_text_generation_deployment.HfTextGenerationConfig","title":"aana.deployments.hf_text_generation_deployment.HfTextGenerationConfig","text":"<p>Attributes:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>The model ID on Hugging Face.</p> </li> <li> <code>model_kwargs</code>               (<code>CustomConfig</code>)           \u2013            <p>The extra model keyword arguments. Defaults to {}.</p> </li> <li> <code>default_sampling_params</code>               (<code>SamplingParams</code>)           \u2013            <p>The default sampling parameters. Defaults to SamplingParams(temperature=0, max_tokens=256).</p> </li> <li> <code>chat_template</code>               (<code>str | None</code>)           \u2013            <p>The name of the chat template. If not provided, the chat template from the model will be used. Some models may not have a chat template. Defaults to None.</p> </li> </ul>"},{"location":"pages/model_hub/text_generation/#example-configurations_1","title":"Example Configurations","text":"<p>As an example, let's see how to configure the Hugging Face Text Generation deployment for the Phi 3 Mini 4K Instruct model.</p> <p>Phi 3 Mini 4K Instruct</p> <pre><code>from aana.deployments.hf_text_generation_deployment import HfTextGenerationConfig, HfTextGenerationDeployment\n\nHfTextGenerationDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=HfTextGenerationConfig(\n        model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n        model_kwargs={\n            \"trust_remote_code\": True,\n        },\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Model ID is the Hugging Face model ID. <code>trust_remote_code=True</code> is required to load the model from the Hugging Face model hub. You can define other model arguments in the <code>model_kwargs</code> dictionary.</p> <p>Here are other example configurations for the Hugging Face Text Generation deployment. Keep in mind that the list is not exhaustive. You can deploy other text generation models that are supported by the Hugging Face Transformers library.</p> Phi 3 Mini 4K Instruct with 4-bit quantization <pre><code>from transformers import BitsAndBytesConfig\nfrom aana.deployments.hf_text_generation_deployment import HfTextGenerationConfig, HfTextGenerationDeployment\n\nHfTextGenerationDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.25},\n    user_config=HfTextGenerationConfig(\n        model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n        model_kwargs={\n            \"trust_remote_code\": True,\n            \"quantization_config\": BitsAndBytesConfig(\n                load_in_8bit=False, load_in_4bit=True\n            ),\n        },\n    ).model_dump(mode=\"json\"),\n)\n</code></pre>"},{"location":"pages/model_hub/text_generation/#half-quadratic-quantization-hqq-text-generation-deployment-deprecated","title":"Half-Quadratic Quantization (HQQ) Text Generation Deployment (Deprecated)","text":"<p>HqqTexGenerationDeployment uses Half-Quadratic Quantization (HQQ) to quantize and deploy text generation models from the Hugging Face Hub.</p> <p>Tip</p> <p>To use HQQ Text Generation deployment, install required libraris with <code>pip install hqq transformers</code> or include extra dependencies using <code>pip install aana[hqq]</code>.</p> <p>Warning</p> <p>HQQ Text Generation deployment is currently deprecated and might be removed in future versions of the Aana SDK. We recommend using the VLLM deployment for text generation models.</p> <p>It supports already quantized models as well as quantizing models on the fly. The quantization is blazing fast and can be done on the fly with minimal overhead. Check out the the collections of already quantized models from Mobius Labs.</p> <p>HqqTexGenerationConfig is used to configure the HQQ Text Generation deployment.</p>"},{"location":"pages/model_hub/text_generation/#aana.deployments.hqq_text_generation_deployment.HqqTexGenerationConfig","title":"aana.deployments.hqq_text_generation_deployment.HqqTexGenerationConfig","text":"<p>Attributes:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>The model ID on Hugging Face.</p> </li> <li> <code>quantize_on_fly</code>               (<code>bool</code>)           \u2013            <p>Whether to quantize the model or it is already pre-quantized. Defaults to False.</p> </li> <li> <code>backend</code>               (<code>HqqBackend</code>)           \u2013            <p>The backend library to use. Defaults to HqqBackend.BITBLAS.</p> </li> <li> <code>compile</code>               (<code>bool</code>)           \u2013            <p>Whether to compile the model with torch.compile. Defaults to False.</p> </li> <li> <code>dtype</code>               (<code>Dtype</code>)           \u2013            <p>The data type. Defaults to Dtype.AUTO.</p> </li> <li> <code>quantization_config</code>               (<code>dict</code>)           \u2013            <p>The quantization configuration.</p> </li> <li> <code>model_kwargs</code>               (<code>CustomConfig</code>)           \u2013            <p>The extra model keyword arguments. Defaults to {}.</p> </li> <li> <code>default_sampling_params</code>               (<code>SamplingParams</code>)           \u2013            <p>The default sampling parameters. Defaults to SamplingParams(temperature=0, max_tokens=256).</p> </li> <li> <code>chat_template</code>               (<code>str | None</code>)           \u2013            <p>The name of the chat template. If not provided, the chat template from the model will be used. Some models may not have a chat template. Defaults to None.</p> </li> </ul>"},{"location":"pages/model_hub/text_generation/#hqq-backends","title":"HQQ Backends","text":"<p>The HQQ Text Generation framework supports two backends, each optimized for specific scenarios:</p> <ol> <li> <p>HqqBackend.BITBLAS (Default)</p> <ul> <li>Library Installation: Install via:  <pre><code>pip install bitblas\n</code></pre>  More details can be found on the BitBLAS GitHub page.</li> <li>Compatibility: Works on a broader range of GPUs, including older models.</li> <li>Precision Support: Supports both 4-bit and 2-bit quantization, allowing for more compact models and efficient inference.</li> <li>Strengths: BitBLAS excels in handling large batch sizes, especially when properly configured. But HQQ is optimized for decoding with a batch size of 1 leading to slower inference times compared to the <code>TORCHAO_INT4</code> backend.</li> <li>Limitations: Slower initialization due to the need for per-shape and per-GPU compilation.</li> </ul> </li> <li> <p>HqqBackend.TORCHAO_INT4</p> <ul> <li>Library Installation: No additional installation required.</li> <li>Compatibility: Only works on Ampere and newer GPUs, limiting its usage to more recent hardware.</li> <li>Precision Support: Supports only 4-bit quantization.</li> <li>Strengths: Much faster to initialize compared to BitBLAS, making it a good choice for situations where quick startup times are crucial. Faster inference times compared to the <code>BITBLAS</code> backend.</li> <li>Limitations: It doesn't support 2-bit quantization.</li> </ul> </li> </ol>"},{"location":"pages/model_hub/text_generation/#example-configurations_2","title":"Example Configurations","text":""},{"location":"pages/model_hub/text_generation/#on-the-fly-quantization","title":"On-the-fly Quantization","text":"<p>As an example, let's see how to configure HQQ Text Generation deployment to quantize and deploy the Meta-Llama-3.1-8B-Instruct model.</p> <p>Meta-Llama-3.1-8B-Instruct</p> <pre><code>from hqq.core.quantize import BaseQuantizeConfig\nfrom aana.deployments.hqq_text_generation_deployment import (\n    HqqBackend,\n    HqqTexGenerationConfig,\n    HqqTextGenerationDeployment,\n)\n\nHqqTextGenerationDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.5},\n    user_config=HqqTexGenerationConfig(\n        model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n        backend=HqqBackend.BITBLAS,\n        quantize_on_fly=True,\n        quantization_config=BaseQuantizeConfig(nbits=4, group_size=64, axis=1),\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=512\n        ),\n        model_kwargs={\n            \"attn_implementation\": \"sdpa\"\n        },\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Model ID is the Hugging Face model ID. We set <code>quantize_on_fly=True</code> to quantize the model on the fly since the model is not pre-quantized. We deploy the model with 4-bit quantization by setting <code>quantization_config</code> in the <code>HqqConfig</code>. We use <code>HqqBackend.BITBLAS</code> as the backend for quantization, it is optional as BitBLAS is the default backend. You can pass extra arguments to the model in the <code>model_kwargs</code> dictionary.</p>"},{"location":"pages/model_hub/text_generation/#pre-quantized-models","title":"Pre-quantized Models","text":"<p>You can also deploy already quantized models with HQQ Text Generation deployment. Here is an example of deploying the </p> <p>Quantized Meta-Llama-3.1-8B-Instruct</p> <pre><code>from hqq.core.quantize import BaseQuantizeConfig\nfrom aana.deployments.hqq_text_generation_deployment import (\n    HqqBackend,\n    HqqTexGenerationConfig,\n    HqqTextGenerationDeployment,\n)\n\nHqqTextGenerationDeployment.options(\n    num_replicas=1,\n    ray_actor_options={\"num_gpus\": 0.5},\n    user_config=HqqTexGenerationConfig(\n        model_id=\"mobiuslabsgmbh/Llama-3.1-8b-instruct_4bitgs64_hqq_calib\",\n        backend=HqqBackend.BITBLAS,\n        quantization_config=BaseQuantizeConfig(\n            nbits=4,\n            group_size=64,\n            quant_scale=False,\n            quant_zero=False,\n            axis=1,\n        ),\n        default_sampling_params=SamplingParams(\n            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=512\n        ),\n    ).model_dump(mode=\"json\"),\n)\n</code></pre> <p>Model ID is the Hugging Face model ID of a pre-quantized model. We use <code>HqqBackend.BITBLAS</code> as the backend for quantization, it is optional as BitBLAS is the default backend. We set the quantization configuration according to the model page.</p>"},{"location":"pages/model_hub/vad/","title":"Voice Activity Detection (VAD) Models","text":""},{"location":"pages/model_hub/vad/#todo-make-vad-deployment-more-generic","title":"TODO: Make VAD deployment more generic","text":""},{"location":"reference/","title":"Reference Documentation (Code API)","text":"<p>This section contains the reference documentation for the public API of the project. Quick links to the most important classes and functions are provided below.</p>"},{"location":"reference/#sdk","title":"SDK","text":"<p><code>aana.AanaSDK</code> - The main class for interacting with the Aana SDK. Use it to register endpoints and deployments and to start the server.</p>"},{"location":"reference/#endpoint","title":"Endpoint","text":"<p><code>aana.api.Endpoint</code> - The base class for defining endpoints in the Aana SDK.</p>"},{"location":"reference/#deployments","title":"Deployments","text":"<p>Deployments contains information about how to deploy models with a number of predefined deployments for such models as Whisper, LLMs, Hugging Face models, and more.</p>"},{"location":"reference/#models","title":"Models","text":"<ul> <li>Media Models - Models for working with media types like audio, video, and images.</li> <li>Automatic Speech Recognition (ASR) Models - Models for working with automatic speech recognition (ASR) models.</li> <li>Caption Models - Models for working with captions.</li> <li>Chat Models - Models for working with chat models.</li> <li>Image Chat Models - Models for working with visual-text content for visual-language models.</li> <li>Custom Config - Custom Config model can be used to pass arbitrary configuration to the deployment.</li> <li>Sampling Models - Contains Sampling Parameters model which can be used to pass sampling parameters to the LLM models.</li> <li>Time Models - Contains time models like TimeInterval.</li> <li>Types Models - Contains types models like Dtype.</li> <li>Video Models - Models for working with video files.</li> <li>Whisper Models - Models for working with whispers.</li> </ul>"},{"location":"reference/deployments/","title":"Deployments","text":""},{"location":"reference/deployments/#aana.deployments","title":"aana.deployments","text":""},{"location":"reference/deployments/#aana.deployments.AanaDeploymentHandle","title":"AanaDeploymentHandle","text":"<pre><code>AanaDeploymentHandle(deployment_name, num_retries=3, retry_exceptions=False, retry_delay=0.2, retry_max_delay=2.0)\n</code></pre> <p>A handle to interact with a deployed Aana deployment.</p> <p>Use create method to create a deployment handle.</p> <pre><code>deployment_handle = await AanaDeploymentHandle.create(\"deployment_name\")\n</code></pre> ATTRIBUTE DESCRIPTION <code>handle</code> <p>Ray Serve deployment handle.</p> <p> TYPE: <code>DeploymentHandle</code> </p> <code>deployment_name</code> <p>The name of the deployment.</p> <p> TYPE: <code>str</code> </p> PARAMETER DESCRIPTION <code>deployment_name</code> <p>The name of the deployment.</p> <p> TYPE: <code>str</code> </p> <code>num_retries</code> <p>The maximum number of retries for the method.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>retry_exceptions</code> <p>Whether to retry on application-level errors or a list of exceptions to retry on.</p> <p> TYPE: <code>bool | list[Exception]</code> DEFAULT: <code>False</code> </p> <code>retry_delay</code> <p>The initial delay between retries.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>retry_max_delay</code> <p>The maximum delay between retries.</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> Source code in <code>aana/deployments/aana_deployment_handle.py</code> <pre><code>def __init__(\n    self,\n    deployment_name: str,\n    num_retries: int = 3,\n    retry_exceptions: bool | list[Exception] = False,\n    retry_delay: float = 0.2,\n    retry_max_delay: float = 2.0,\n):\n    \"\"\"A handle to interact with a deployed Aana deployment.\n\n    Args:\n        deployment_name (str): The name of the deployment.\n        num_retries (int): The maximum number of retries for the method.\n        retry_exceptions (bool | list[Exception]): Whether to retry on application-level errors or a list of exceptions to retry on.\n        retry_delay (float): The initial delay between retries.\n        retry_max_delay (float): The maximum delay between retries.\n    \"\"\"\n    self.handle = serve.get_app_handle(deployment_name)\n    self.deployment_name = deployment_name\n    self.__methods = None\n    self.num_retries = num_retries\n    self.retry_exceptions = retry_exceptions\n    self.retry_delay = retry_delay\n    self.retry_max_delay = retry_max_delay\n</code></pre>"},{"location":"reference/deployments/#aana.deployments.AanaDeploymentHandle.create","title":"create","text":"<pre><code>create(deployment_name, num_retries=3, retry_exceptions=False, retry_delay=0.2, retry_max_delay=2.0)\n</code></pre> <p>Create a deployment handle.</p> PARAMETER DESCRIPTION <code>deployment_name</code> <p>The name of the deployment to interact with.</p> <p> TYPE: <code>str</code> </p> <code>num_retries</code> <p>The maximum number of retries for the method.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>retry_exceptions</code> <p>Whether to retry on application-level errors or a list of exceptions to retry on.</p> <p> TYPE: <code>bool | list[Exception]</code> DEFAULT: <code>False</code> </p> <code>retry_delay</code> <p>The initial delay between retries.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>retry_max_delay</code> <p>The maximum delay between retries.</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.0</code> </p> Source code in <code>aana/deployments/aana_deployment_handle.py</code> <pre><code>@classmethod\nasync def create(\n    cls,\n    deployment_name: str,\n    num_retries: int = 3,\n    retry_exceptions: bool | list[Exception] = False,\n    retry_delay: float = 0.2,\n    retry_max_delay: float = 2.0,\n):\n    \"\"\"Create a deployment handle.\n\n    Args:\n        deployment_name (str): The name of the deployment to interact with.\n        num_retries (int): The maximum number of retries for the method.\n        retry_exceptions (bool | list[Exception]): Whether to retry on application-level errors or a list of exceptions to retry on.\n        retry_delay (float): The initial delay between retries.\n        retry_max_delay (float): The maximum delay between retries.\n    \"\"\"\n    handle = cls(\n        deployment_name=deployment_name,\n        num_retries=num_retries,\n        retry_exceptions=retry_exceptions,\n        retry_delay=retry_delay,\n        retry_max_delay=retry_max_delay,\n    )\n    await handle.__load_methods()\n    return handle\n</code></pre>"},{"location":"reference/deployments/#aana.deployments.BaseDeployment","title":"BaseDeployment","text":"<pre><code>BaseDeployment()\n</code></pre> <p>Base class for all deployments.</p> <p>To create a new deployment, inherit from this class and implement the <code>apply_config</code> method and your custom methods like <code>generate</code>, <code>predict</code>, etc.</p> Source code in <code>aana/deployments/base_deployment.py</code> <pre><code>def __init__(self):\n    \"\"\"Inits to unconfigured state.\"\"\"\n    self.config = None\n    self._configured = False\n    self.num_requests_since_last_health_check = 0\n    self.raised_exceptions = []\n    self.restart_exceptions = [InferenceException]\n</code></pre>"},{"location":"reference/deployments/#aana.deployments.BaseDeployment.check_health","title":"check_health","text":"<pre><code>check_health()\n</code></pre> <p>Check the health of the deployment.</p> Source code in <code>aana/deployments/base_deployment.py</code> <pre><code>async def check_health(self):\n    \"\"\"Check the health of the deployment.\n\n    Raises:\n        Raises the exception that caused the deployment to be unhealthy.\n    \"\"\"\n    raised_restart_exceptions = [\n        exception\n        for exception in self.raised_exceptions\n        if exception.__class__ in self.restart_exceptions\n    ]\n    # Restart the deployment if more than 50% of the requests raised restart exceptions\n    if self.num_requests_since_last_health_check != 0:\n        ratio_restart_exceptions = (\n            len(raised_restart_exceptions)\n            / self.num_requests_since_last_health_check\n        )\n        if ratio_restart_exceptions &gt; 0.5:\n            raise raised_restart_exceptions[0]\n\n    self.raised_exceptions = []\n    self.num_requests_since_last_health_check = 0\n</code></pre>"},{"location":"reference/deployments/#aana.deployments.BaseDeployment.apply_config","title":"apply_config","text":"<pre><code>apply_config(config)\n</code></pre> <p>Apply the configuration.</p> <p>This method is called when the deployment is created or updated.</p> <p>Define the logic to load the model and configure it here.</p> PARAMETER DESCRIPTION <code>config</code> <p>the configuration</p> <p> TYPE: <code>dict</code> </p> Source code in <code>aana/deployments/base_deployment.py</code> <pre><code>async def apply_config(self, config: dict[str, Any]):\n    \"\"\"Apply the configuration.\n\n    This method is called when the deployment is created or updated.\n\n    Define the logic to load the model and configure it here.\n\n    Args:\n        config (dict): the configuration\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/endpoint/","title":"Endpoint","text":""},{"location":"reference/endpoint/#aana.api.Endpoint","title":"aana.api.Endpoint","text":"<pre><code>Endpoint(name, path, summary, admin_required=False, active_subscription_required=False, defer_option=DeferOption.OPTIONAL, initialized=False, event_handlers=None)\n</code></pre> <p>Class used to represent an endpoint.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Name of the endpoint.</p> <p> TYPE: <code>str</code> </p> <code>path</code> <p>Path of the endpoint (e.g. \"/video/transcribe\").</p> <p> TYPE: <code>str</code> </p> <code>summary</code> <p>Description of the endpoint that will be shown in the API documentation.</p> <p> TYPE: <code>str</code> </p> <code>admin_required</code> <p>Flag indicating if the endpoint requires admin access.</p> <p> TYPE: <code>bool</code> </p> <code>active_subscription_required</code> <p>Flag indicating if the endpoint requires an active subscription.</p> <p> TYPE: <code>bool</code> </p> <code>defer_option</code> <p>Defer option for the endpoint (always, never, optional).</p> <p> TYPE: <code>DeferOption</code> </p> <code>event_handlers</code> <p>The list of event handlers to register for the endpoint.</p> <p> TYPE: <code>list[EventHandler] | None</code> </p>"},{"location":"reference/endpoint/#aana.api.Endpoint.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initialize the endpoint.</p> <p>Redefine this method to add initialization logic for the endpoint (e.g. create a handle to the deployment). Call super().initialize() to ensure the endpoint is initialized.</p> Example <pre><code>async def initialize(self):\n    await super().initialize()\n    self.asr_handle = await AanaDeploymentHandle.create(\"whisper_deployment\")\n</code></pre> Source code in <code>aana/api/api_generation.py</code> <pre><code>async def initialize(self):\n    \"\"\"Initialize the endpoint.\n\n    Redefine this method to add initialization logic for the endpoint (e.g. create a handle to the deployment).\n    Call super().initialize() to ensure the endpoint is initialized.\n\n    Example:\n        ```python\n        async def initialize(self):\n            await super().initialize()\n            self.asr_handle = await AanaDeploymentHandle.create(\"whisper_deployment\")\n        ```\n    \"\"\"\n    self.initialized = True\n</code></pre>"},{"location":"reference/endpoint/#aana.api.Endpoint.run","title":"run","text":"<pre><code>run(*args, **kwargs)\n</code></pre> <p>The main method of the endpoint that is called when the endpoint receives a request.</p> <p>Redefine this method to implement the logic of the endpoint.</p> Example <pre><code>async def run(self, video: VideoInput) -&gt; WhisperOutput:\n    video_obj: Video = await run_remote(download_video)(video_input=video)\n    transcription = await self.asr_handle.transcribe(audio=audio)\n    return transcription\n</code></pre> Source code in <code>aana/api/api_generation.py</code> <pre><code>async def run(self, *args, **kwargs):\n    \"\"\"The main method of the endpoint that is called when the endpoint receives a request.\n\n    Redefine this method to implement the logic of the endpoint.\n\n    Example:\n        ```python\n        async def run(self, video: VideoInput) -&gt; WhisperOutput:\n            video_obj: Video = await run_remote(download_video)(video_input=video)\n            transcription = await self.asr_handle.transcribe(audio=audio)\n            return transcription\n        ```\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/endpoint/#aana.api.Endpoint.register","title":"register","text":"<pre><code>register(app, custom_schemas, event_manager)\n</code></pre> <p>Register the endpoint in the FastAPI application.</p> PARAMETER DESCRIPTION <code>app</code> <p>FastAPI application.</p> <p> TYPE: <code>FastAPI</code> </p> <code>custom_schemas</code> <p>Dictionary containing custom schemas.</p> <p> TYPE: <code>dict[str, dict]</code> </p> <code>event_manager</code> <p>Event manager for the application.</p> <p> TYPE: <code>EventManager</code> </p> Source code in <code>aana/api/api_generation.py</code> <pre><code>def register(\n    self, app: FastAPI, custom_schemas: dict[str, dict], event_manager: EventManager\n):\n    \"\"\"Register the endpoint in the FastAPI application.\n\n    Args:\n        app (FastAPI): FastAPI application.\n        custom_schemas (dict[str, dict]): Dictionary containing custom schemas.\n        event_manager (EventManager): Event manager for the application.\n    \"\"\"\n    RequestModel = self.get_request_model()\n    ResponseModel = self.get_response_model()\n\n    if self.event_handlers:\n        for handler in self.event_handlers:\n            event_manager.register_handler_for_events(handler, [self.path])\n\n    route_func = self.__create_endpoint_func(\n        RequestModel=RequestModel,\n        event_manager=event_manager,\n    )\n\n    app.post(\n        self.path,\n        name=self.name,\n        summary=self.summary,\n        operation_id=self.name,\n        response_model=ResponseModel,\n        responses={\n            400: {\"model\": ExceptionResponseModel},\n        },\n    )(route_func)\n    custom_schemas[self.name] = RequestModel.model_json_schema()\n</code></pre>"},{"location":"reference/endpoint/#aana.api.Endpoint.get_request_model","title":"get_request_model","text":"<pre><code>get_request_model()\n</code></pre> <p>Generate the request Pydantic model for the endpoint.</p> RETURNS DESCRIPTION <code>type[BaseModel]</code> <p>type[BaseModel]: Request Pydantic model.</p> Source code in <code>aana/api/api_generation.py</code> <pre><code>def get_request_model(self) -&gt; type[BaseModel]:\n    \"\"\"Generate the request Pydantic model for the endpoint.\n\n    Returns:\n        type[BaseModel]: Request Pydantic model.\n    \"\"\"\n    model_name = self.__generate_model_name(\"Request\")\n    input_fields = self.__get_input_fields()\n    return create_model(model_name, **input_fields)\n</code></pre>"},{"location":"reference/endpoint/#aana.api.Endpoint.get_response_model","title":"get_response_model","text":"<pre><code>get_response_model()\n</code></pre> <p>Generate the response Pydantic model for the endpoint.</p> RETURNS DESCRIPTION <code>type[BaseModel]</code> <p>type[BaseModel]: Response Pydantic model.</p> Source code in <code>aana/api/api_generation.py</code> <pre><code>def get_response_model(self) -&gt; type[BaseModel]:\n    \"\"\"Generate the response Pydantic model for the endpoint.\n\n    Returns:\n        type[BaseModel]: Response Pydantic model.\n    \"\"\"\n    model_name = self.__generate_model_name(\"Response\")\n    output_fields = self.__get_output_fields()\n    return create_model(\n        model_name, **output_fields, __config__=ConfigDict(extra=\"forbid\")\n    )\n</code></pre>"},{"location":"reference/endpoint/#aana.api.Endpoint.is_streaming_response","title":"is_streaming_response","text":"<pre><code>is_streaming_response()\n</code></pre> <p>Check if the endpoint returns a streaming response.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the endpoint returns a streaming response, False otherwise.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>aana/api/api_generation.py</code> <pre><code>@classmethod\ndef is_streaming_response(cls) -&gt; bool:\n    \"\"\"Check if the endpoint returns a streaming response.\n\n    Returns:\n        bool: True if the endpoint returns a streaming response, False otherwise.\n    \"\"\"\n    return isasyncgenfunction(cls.run)\n</code></pre>"},{"location":"reference/endpoint/#aana.api.Endpoint.send_usage_event","title":"send_usage_event","text":"<pre><code>send_usage_event(api_key, metric, properties)\n</code></pre> <p>Send a usage event to the LAGO API service.</p> PARAMETER DESCRIPTION <code>api_key</code> <p>The API key information.</p> <p> TYPE: <code>ApiKey</code> </p> <code>metric</code> <p>The metric code.</p> <p> TYPE: <code>str</code> </p> <code>properties</code> <p>The properties of the event (usage data, e.g. {\"count\": 10}).</p> <p> TYPE: <code>dict</code> </p> Source code in <code>aana/api/api_generation.py</code> <pre><code>def send_usage_event(\n    self, api_key: ApiKey, metric: str, properties: dict[str, Any]\n):\n    \"\"\"Send a usage event to the LAGO API service.\n\n    Args:\n        api_key (ApiKey): The API key information.\n        metric (str): The metric code.\n        properties (dict): The properties of the event (usage data, e.g. {\"count\": 10}).\n    \"\"\"\n    from lago_python_client.client import Client\n    from lago_python_client.models import Event\n    from tenacity import (\n        retry,\n        retry_if_exception_type,\n        stop_after_attempt,\n    )\n\n    @retry(\n        stop=stop_after_attempt(3),\n        retry=retry_if_exception_type(Exception),\n    )\n    def send_event_with_retry(client, event):\n        return client.events.create(event)\n\n    if not aana_settings.api_service.enabled:\n        logger.warning(\"API service is not enabled. Skipping usage event.\")\n        return\n\n    if (\n        not aana_settings.api_service.lago_url\n        or not aana_settings.api_service.lago_api_key\n    ):\n        logger.warning(\n            \"LAGO API service URL or API key is not set. Skipping usage event.\"\n        )\n        return\n\n    try:\n        client = Client(\n            api_key=aana_settings.api_service.lago_api_key,\n            api_url=aana_settings.api_service.lago_url,\n        )\n\n        event = Event(\n            transaction_id=str(uuid.uuid4()),\n            code=metric,\n            external_subscription_id=api_key.subscription_id,\n            timestamp=time.time(),\n            properties=properties,\n        )\n\n        send_event_with_retry(client, event)\n    except Exception as e:\n        logger.error(\n            f\"Failed to send usage event after retries: {e}\", exc_info=True\n        )\n</code></pre>"},{"location":"reference/exceptions/","title":"Exceptions","text":""},{"location":"reference/exceptions/#aana.exceptions","title":"aana.exceptions","text":""},{"location":"reference/exceptions/#aana.exceptions.BaseException","title":"BaseException","text":"<pre><code>BaseException(**kwargs)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Base class for SDK exceptions.</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"Initialise Exception.\"\"\"\n    super().__init__()\n    self.extra = kwargs\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.BaseException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.BaseException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime","title":"aana.exceptions.runtime","text":""},{"location":"reference/exceptions/#aana.exceptions.runtime.InferenceException","title":"InferenceException","text":"<pre><code>InferenceException(model_name, message=None)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when there is an error during inference.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>name of the model that caused the exception</p> <p> TYPE: <code>str</code> </p> <code>message</code> <p>the message to display</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>aana/exceptions/runtime.py</code> <pre><code>def __init__(self, model_name: str, message: str | None = None):\n    \"\"\"Initialize the exception.\n\n    Args:\n        model_name (str): name of the model that caused the exception\n        message (str): the message to display\n    \"\"\"\n    message = message or f\"Inference failed for model: {model_name}\"\n    super().__init__(\n        model_name=model_name,\n        message=message,\n    )\n    self.model_name = model_name\n    self.message = message\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.InferenceException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.InferenceException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.PromptTooLongException","title":"PromptTooLongException","text":"<pre><code>PromptTooLongException(prompt_len, max_len)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when the prompt is too long.</p> ATTRIBUTE DESCRIPTION <code>prompt_len</code> <p>the length of the prompt in tokens</p> <p> TYPE: <code>int</code> </p> <code>max_len</code> <p>the maximum allowed length of the prompt in tokens</p> <p> TYPE: <code>int</code> </p> PARAMETER DESCRIPTION <code>prompt_len</code> <p>the length of the prompt in tokens</p> <p> TYPE: <code>int</code> </p> <code>max_len</code> <p>the maximum allowed length of the prompt in tokens</p> <p> TYPE: <code>int</code> </p> Source code in <code>aana/exceptions/runtime.py</code> <pre><code>def __init__(self, prompt_len: int, max_len: int):\n    \"\"\"Initialize the exception.\n\n    Args:\n        prompt_len (int): the length of the prompt in tokens\n        max_len (int): the maximum allowed length of the prompt in tokens\n    \"\"\"\n    super().__init__(prompt_len=prompt_len, max_len=max_len)\n    self.prompt_len = prompt_len\n    self.max_len = max_len\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.PromptTooLongException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.PromptTooLongException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.EndpointNotFoundException","title":"EndpointNotFoundException","text":"<pre><code>EndpointNotFoundException(target, endpoint)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when an endpoint is not found.</p> ATTRIBUTE DESCRIPTION <code>target</code> <p>the name of the target deployment</p> <p> TYPE: <code>str</code> </p> <code>endpoint</code> <p>the endpoint path</p> <p> TYPE: <code>str</code> </p> PARAMETER DESCRIPTION <code>target</code> <p>the name of the target deployment</p> <p> TYPE: <code>str</code> </p> <code>endpoint</code> <p>the endpoint path</p> <p> TYPE: <code>str</code> </p> Source code in <code>aana/exceptions/runtime.py</code> <pre><code>def __init__(self, target: str, endpoint: str):\n    \"\"\"Initialize the exception.\n\n    Args:\n        target (str): the name of the target deployment\n        endpoint (str): the endpoint path\n    \"\"\"\n    super().__init__(target=target, endpoint=endpoint)\n    self.target = target\n    self.endpoint = endpoint\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.EndpointNotFoundException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.EndpointNotFoundException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.TooManyRequestsException","title":"TooManyRequestsException","text":"<pre><code>TooManyRequestsException(rate_limit, rate_duration)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when calling a rate-limited resource too often.</p> ATTRIBUTE DESCRIPTION <code>rate_limit</code> <p>The limit amount.</p> <p> TYPE: <code>int</code> </p> <code>rate_duration</code> <p>The duration for the limit in seconds.</p> <p> TYPE: <code>float</code> </p> PARAMETER DESCRIPTION <code>rate_limit</code> <p>The limit amount.</p> <p> TYPE: <code>int</code> </p> <code>rate_duration</code> <p>The duration for the limit in seconds.</p> <p> TYPE: <code>float</code> </p> Source code in <code>aana/exceptions/runtime.py</code> <pre><code>def __init__(self, rate_limit: int, rate_duration: float):\n    \"\"\"Constructor.\n\n    Args:\n        rate_limit (int): The limit amount.\n        rate_duration (float): The duration for the limit in seconds.\n    \"\"\"\n    super().__init__(rate_limit=rate_limit, rate_duration=rate_duration)\n    self.rate_limit = rate_limit\n    self.rate_duration = rate_duration\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.TooManyRequestsException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.TooManyRequestsException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.HandlerAlreadyRegisteredException","title":"HandlerAlreadyRegisteredException","text":"<pre><code>HandlerAlreadyRegisteredException()\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when registering a handler that is already registered.</p> Source code in <code>aana/exceptions/runtime.py</code> <pre><code>def __init__(self):\n    \"\"\"Constructor.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.HandlerAlreadyRegisteredException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.HandlerAlreadyRegisteredException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.HandlerNotRegisteredException","title":"HandlerNotRegisteredException","text":"<pre><code>HandlerNotRegisteredException()\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception removing a handler that has not been registered.</p> Source code in <code>aana/exceptions/runtime.py</code> <pre><code>def __init__(self):\n    \"\"\"Constructor.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.HandlerNotRegisteredException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.HandlerNotRegisteredException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.EmptyMigrationsException","title":"EmptyMigrationsException","text":"<pre><code>EmptyMigrationsException(**kwargs)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when there are no migrations to apply.</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"Initialise Exception.\"\"\"\n    super().__init__()\n    self.extra = kwargs\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.EmptyMigrationsException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.EmptyMigrationsException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.DeploymentException","title":"DeploymentException","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for deployment errors.</p>"},{"location":"reference/exceptions/#aana.exceptions.runtime.InsufficientResources","title":"InsufficientResources","text":"<p>               Bases: <code>DeploymentException</code></p> <p>Exception raised when there are insufficient resources for a deployment.</p>"},{"location":"reference/exceptions/#aana.exceptions.runtime.FailedDeployment","title":"FailedDeployment","text":"<p>               Bases: <code>DeploymentException</code></p> <p>Exception raised when there is an error during deployment.</p>"},{"location":"reference/exceptions/#aana.exceptions.runtime.UploadedFileNotFound","title":"UploadedFileNotFound","text":"<pre><code>UploadedFileNotFound(filename)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when the uploaded file is not found.</p> PARAMETER DESCRIPTION <code>filename</code> <p>the name of the file that was not found</p> <p> TYPE: <code>str</code> </p> Source code in <code>aana/exceptions/runtime.py</code> <pre><code>def __init__(self, filename: str):\n    \"\"\"Initialize the exception.\n\n    Args:\n        filename (str): the name of the file that was not found\n    \"\"\"\n    super().__init__()\n    self.filename = filename\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.InvalidWebhookEventType","title":"InvalidWebhookEventType","text":"<pre><code>InvalidWebhookEventType(event_type)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when an invalid webhook event type is provided.</p> PARAMETER DESCRIPTION <code>event_type</code> <p>the invalid event type</p> <p> TYPE: <code>str</code> </p> Source code in <code>aana/exceptions/runtime.py</code> <pre><code>def __init__(self, event_type: str):\n    \"\"\"Initialize the exception.\n\n    Args:\n        event_type (str): the invalid event type\n    \"\"\"\n    super().__init__(event_type=event_type)\n    self.event_type = event_type\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.InvalidWebhookEventType.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.runtime.InvalidWebhookEventType.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io","title":"aana.exceptions.io","text":""},{"location":"reference/exceptions/#aana.exceptions.io.ImageReadingException","title":"ImageReadingException","text":"<pre><code>ImageReadingException(image)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when there is an error reading an image.</p> ATTRIBUTE DESCRIPTION <code>image</code> <p>the image that caused the exception</p> <p> TYPE: <code>Image</code> </p> PARAMETER DESCRIPTION <code>image</code> <p>the image that caused the exception</p> <p> TYPE: <code>Image</code> </p> Source code in <code>aana/exceptions/io.py</code> <pre><code>def __init__(self, image: \"Image\"):\n    \"\"\"Initialize the exception.\n\n    Args:\n        image (Image): the image that caused the exception\n    \"\"\"\n    super().__init__(image=image)\n    self.image = image\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.ImageReadingException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.ImageReadingException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.AudioReadingException","title":"AudioReadingException","text":"<pre><code>AudioReadingException(audio)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when there is an error reading an audio.</p> ATTRIBUTE DESCRIPTION <code>audio</code> <p>the audio that caused the exception</p> <p> TYPE: <code>Audio</code> </p> PARAMETER DESCRIPTION <code>audio</code> <p>the audio that caused the exception</p> <p> TYPE: <code>Audio</code> </p> Source code in <code>aana/exceptions/io.py</code> <pre><code>def __init__(self, audio: \"Audio\"):\n    \"\"\"Initialize the exception.\n\n    Args:\n        audio (Audio): the audio that caused the exception\n    \"\"\"\n    super().__init__(audio=audio)\n    self.audio = audio\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.AudioReadingException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.AudioReadingException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.DownloadException","title":"DownloadException","text":"<pre><code>DownloadException(url, msg='')\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when there is an error downloading a file.</p> ATTRIBUTE DESCRIPTION <code>url</code> <p>the URL of the file that caused the exception</p> <p> TYPE: <code>str</code> </p> PARAMETER DESCRIPTION <code>url</code> <p>the URL of the file that caused the exception</p> <p> TYPE: <code>str</code> </p> <code>msg</code> <p>the error message</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> Source code in <code>aana/exceptions/io.py</code> <pre><code>def __init__(self, url: str, msg: str = \"\"):\n    \"\"\"Initialize the exception.\n\n    Args:\n        url (str): the URL of the file that caused the exception\n        msg (str): the error message\n    \"\"\"\n    super().__init__(url=url)\n    self.url = url\n    self.msg = msg\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.DownloadException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.DownloadException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.VideoException","title":"VideoException","text":"<pre><code>VideoException(video)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when working with videos.</p> ATTRIBUTE DESCRIPTION <code>video</code> <p>the video that caused the exception</p> <p> TYPE: <code>Video</code> </p> PARAMETER DESCRIPTION <code>video</code> <p>the video that caused the exception</p> <p> TYPE: <code>Video</code> </p> Source code in <code>aana/exceptions/io.py</code> <pre><code>def __init__(self, video: \"Video\"):\n    \"\"\"Initialize the exception.\n\n    Args:\n        video (Video): the video that caused the exception\n    \"\"\"\n    super().__init__(video=video)\n    self.video = video\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.VideoException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.VideoException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.VideoReadingException","title":"VideoReadingException","text":"<pre><code>VideoReadingException(video)\n</code></pre> <p>               Bases: <code>VideoException</code></p> <p>Exception raised when there is an error reading a video.</p> ATTRIBUTE DESCRIPTION <code>video</code> <p>the video that caused the exception</p> <p> TYPE: <code>Video</code> </p> PARAMETER DESCRIPTION <code>video</code> <p>the video that caused the exception</p> <p> TYPE: <code>Video</code> </p> Source code in <code>aana/exceptions/io.py</code> <pre><code>def __init__(self, video: \"Video\"):\n    \"\"\"Initialize the exception.\n\n    Args:\n        video (Video): the video that caused the exception\n    \"\"\"\n    super().__init__(video=video)\n    self.video = video\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.VideoReadingException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.VideoReadingException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.VideoTooLongException","title":"VideoTooLongException","text":"<pre><code>VideoTooLongException(video, video_len, max_len)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Exception raised when the video is too long.</p> ATTRIBUTE DESCRIPTION <code>video</code> <p>the video that caused the exception</p> <p> TYPE: <code>Video</code> </p> <code>video_len</code> <p>the length of the video in seconds</p> <p> TYPE: <code>float</code> </p> <code>max_len</code> <p>the maximum allowed length of the video in seconds</p> <p> TYPE: <code>float</code> </p> PARAMETER DESCRIPTION <code>video</code> <p>the video that caused the exception</p> <p> TYPE: <code>Video</code> </p> <code>video_len</code> <p>the length of the video in seconds</p> <p> TYPE: <code>float</code> </p> <code>max_len</code> <p>the maximum allowed length of the video in seconds</p> <p> TYPE: <code>float</code> </p> Source code in <code>aana/exceptions/io.py</code> <pre><code>def __init__(self, video: \"Video\", video_len: float, max_len: float):\n    \"\"\"Initialize the exception.\n\n    Args:\n        video (Video): the video that caused the exception\n        video_len (float): the length of the video in seconds\n        max_len (float): the maximum allowed length of the video in seconds\n    \"\"\"\n    super().__init__(video=video, video_len=video_len, max_len=max_len)\n    self.video = video\n    self.video_len = video_len\n    self.max_len = max_len\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.VideoTooLongException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.io.VideoTooLongException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.db","title":"aana.exceptions.db","text":""},{"location":"reference/exceptions/#aana.exceptions.db.NotFoundException","title":"NotFoundException","text":"<pre><code>NotFoundException(table_name, id)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Raised when an item searched by id is not found.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>the name of the table being queried.</p> <p> TYPE: <code>str</code> </p> <code>id</code> <p>the id of the item to be retrieved.</p> <p> TYPE: <code>int | MediaId</code> </p> Source code in <code>aana/exceptions/db.py</code> <pre><code>def __init__(self, table_name: str, id: int | MediaId):  # noqa: A002\n    \"\"\"Constructor.\n\n    Args:\n        table_name (str): the name of the table being queried.\n        id (int | MediaId): the id of the item to be retrieved.\n    \"\"\"\n    super().__init__(table=table_name, id=id)\n    self.table_name = table_name\n    self.id = id\n    self.http_status_code = 404\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.db.NotFoundException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.db.NotFoundException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.db.MediaIdAlreadyExistsException","title":"MediaIdAlreadyExistsException","text":"<pre><code>MediaIdAlreadyExistsException(table_name, media_id)\n</code></pre> <p>               Bases: <code>BaseException</code></p> <p>Raised when a media_id already exists.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>the name of the table being queried.</p> <p> TYPE: <code>str</code> </p> <code>media_id</code> <p>the id of the item to be retrieved.</p> <p> TYPE: <code>MediaId</code> </p> Source code in <code>aana/exceptions/db.py</code> <pre><code>def __init__(self, table_name: str, media_id: MediaId):\n    \"\"\"Constructor.\n\n    Args:\n        table_name (str): the name of the table being queried.\n        media_id (MediaId): the id of the item to be retrieved.\n    \"\"\"\n    super().__init__(table=table_name, id=media_id)\n    self.table_name = table_name\n    self.media_id = media_id\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.db.MediaIdAlreadyExistsException.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Get the data to be returned to the client.</p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: data to be returned to the client</p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Get the data to be returned to the client.\n\n    Returns:\n        Dict[str, Any]: data to be returned to the client\n    \"\"\"\n    data = self.extra.copy()\n    return data\n</code></pre>"},{"location":"reference/exceptions/#aana.exceptions.db.MediaIdAlreadyExistsException.add_extra","title":"add_extra","text":"<pre><code>add_extra(data)\n</code></pre> <p>Add extra data to the exception.</p> <p>This data will be returned to the user as part of the response.</p> <p>How to use: in the exception handler, add the extra data to the exception and raise it again.</p> Example <pre><code>try:\n    ...\nexcept BaseException as e:\n    e.add_extra({'extra_key': 'extra_value'})\n    raise e\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>dictionary containing the extra data</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>aana/exceptions/core.py</code> <pre><code>def add_extra(self, data: dict[str, Any]) -&gt; None:\n    \"\"\"Add extra data to the exception.\n\n    This data will be returned to the user as part of the response.\n\n    How to use: in the exception handler, add the extra data to the exception and raise it again.\n\n    Example:\n        ```\n        try:\n            ...\n        except BaseException as e:\n            e.add_extra({'extra_key': 'extra_value'})\n            raise e\n        ```\n\n    Args:\n        data (dict[str, Any]): dictionary containing the extra data\n    \"\"\"\n    self.extra.update(data)\n</code></pre>"},{"location":"reference/integrations/","title":"Integrations","text":""},{"location":"reference/integrations/#aana.integrations.haystack","title":"aana.integrations.haystack","text":""},{"location":"reference/integrations/#aana.integrations.haystack.AanaDeploymentComponent","title":"AanaDeploymentComponent","text":"<pre><code>AanaDeploymentComponent(deployment_handle, method_name)\n</code></pre> <p>Wrapper for Aana deployments to run as HayStack Components.</p> Example <pre><code>deployment_handle = await AanaDeploymentHandle.create(\"my_deployment\")\nhaystack_component = AanaDeploymentComponent(deployment_handle, \"my_method\")\nhaystack_component.warm_up()  # This is currently a no-op, but subject to change.\ncomponent_result = haystack_component.run(my_input_prompt=\"This is an input prompt\")\n</code></pre> PARAMETER DESCRIPTION <code>deployment_handle</code> <p>the Aana Ray deployment to be wrapped (must be a class Deployment)</p> <p> TYPE: <code>AanaDeploymentHandle</code> </p> <code>method_name</code> <p>the name of the method on the deployment to call inside the component's <code>run()</code> method.</p> <p> TYPE: <code>str</code> </p> Source code in <code>aana/integrations/haystack/deployment_component.py</code> <pre><code>def __init__(self, deployment_handle: AanaDeploymentHandle, method_name: str):\n    \"\"\"Constructor.\n\n    Arguments:\n        deployment_handle (AanaDeploymentHandle): the Aana Ray deployment to be wrapped (must be a class Deployment)\n        method_name (str): the name of the method on the deployment to call inside the component's `run()` method.\n    \"\"\"\n    self._deployment_handle = deployment_handle\n\n    # Determine input and output types for `run()`\n    # Will raise if the function is not defined (e.g. if you pass a function deployment)\n    self.run_method = self._get_method(method_name)\n    if not self.run_method:\n        raise AttributeError(name=method_name, obj=self._deployment_handle)\n    hints = get_type_hints(self.run_method)\n    input_types, output_types = typehints_to_component_types(hints)\n    # The functions `set_input_types()` and `set_output_types()`\n    # take an positional instance argument and keyword arguments\n    component.set_input_types(self, **input_types)\n    component.set_output_types(self, **output_types)\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.haystack.AanaDeploymentComponent.warm_up","title":"warm_up","text":"<pre><code>warm_up()\n</code></pre> <p>Warms up the deployment to a ready state.</p> <p>As we run off an existing deployment handle, this is currently a no-op.</p> Source code in <code>aana/integrations/haystack/deployment_component.py</code> <pre><code>def warm_up(self):\n    \"\"\"Warms up the deployment to a ready state.\n\n    As we run off an existing deployment handle, this is currently a no-op.\n    \"\"\"\n    self._warm = True\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.haystack.AanaDeploymentComponent.run","title":"run","text":"<pre><code>run(*args, **kwargs)\n</code></pre> <p>Run the component. This is the primary interface for Haystack Components.</p> PARAMETER DESCRIPTION <code>*args</code> <p>the arguments to pass to the deployment run function</p> <p> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>the keyword arguments to pass to the deployment run function</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <p>The return value of the deployment's run function</p> Source code in <code>aana/integrations/haystack/deployment_component.py</code> <pre><code>def run(self, *args, **kwargs):\n    \"\"\"Run the component. This is the primary interface for Haystack Components.\n\n    Arguments:\n        *args: the arguments to pass to the deployment run function\n        **kwargs: the keyword arguments to pass to the deployment run function\n\n    Returns:\n        The return value of the deployment's run function\n    \"\"\"\n    # Function may (must?) be a coroutine. Resolve it if so.\n    return run_async(self._call(*args, **kwargs))\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.av","title":"aana.integrations.external.av","text":""},{"location":"reference/integrations/#aana.integrations.external.av.pyAVWrapper","title":"pyAVWrapper","text":"<p>               Bases: <code>AbstractAudioLibrary</code></p> <p>Class for audio handling using PyAV library.</p>"},{"location":"reference/integrations/#aana.integrations.external.av.pyAVWrapper.read_file","title":"read_file","text":"<pre><code>read_file(path, sample_rate=16000)\n</code></pre> <p>Read an audio file from path and return it as a numpy array.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path of the file to read.</p> <p> TYPE: <code>Path</code> </p> <code>sample_rate</code> <p>sample rate of the audio, default is 16000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16000</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: The audio file as a numpy array.</p> Source code in <code>aana/integrations/external/av.py</code> <pre><code>@classmethod\ndef read_file(cls, path: Path, sample_rate: int = 16000) -&gt; np.ndarray:\n    \"\"\"Read an audio file from path and return it as a numpy array.\n\n    Args:\n        path (Path): The path of the file to read.\n        sample_rate (int): sample rate of the audio, default is 16000.\n\n    Returns:\n        np.ndarray: The audio file as a numpy array.\n    \"\"\"\n    resampler = av.audio.resampler.AudioResampler(\n        format=\"s16\",\n        layout=\"mono\",\n        rate=sample_rate,\n    )\n\n    raw_buffer = io.BytesIO()\n    dtype = None\n\n    with av.open(str(path), mode=\"r\", metadata_errors=\"ignore\") as container:\n        frames = container.decode(audio=0)\n        frames = ignore_invalid_frames(frames)\n        frames = group_frames(frames, 500000)\n        frames = resample_frames(frames, resampler)\n\n        for frame in frames:\n            array = frame.to_ndarray()\n            dtype = array.dtype\n            raw_buffer.write(array)\n\n    # It appears that some objects related to the resampler are not freed\n    # unless the garbage collector is manually run.\n    del resampler\n    gc.collect()\n\n    audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)\n    # Convert s16 back to f32.\n    audio = audio.astype(np.float32) / 32768.0\n    return audio\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.av.pyAVWrapper.read_from_bytes","title":"read_from_bytes","text":"<pre><code>read_from_bytes(content, sample_rate=16000)\n</code></pre> <p>Read audio bytes and return as a numpy array.</p> PARAMETER DESCRIPTION <code>content</code> <p>The content of the file to read.</p> <p> TYPE: <code>bytes</code> </p> <code>sample_rate</code> <p>sample rate of the audio, default is 16000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16000</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: The file as a numpy array.</p> Source code in <code>aana/integrations/external/av.py</code> <pre><code>@classmethod\ndef read_from_bytes(cls, content: bytes, sample_rate: int = 16000) -&gt; np.ndarray:\n    \"\"\"Read audio bytes and return as a numpy array.\n\n    Args:\n        content (bytes): The content of the file to read.\n        sample_rate (int): sample rate of the audio, default is 16000.\n\n    Returns:\n        np.ndarray: The file as a numpy array.\n    \"\"\"\n    # Create an in-memory file-like object\n    content_io = io.BytesIO(content)\n\n    resampler = av.audio.resampler.AudioResampler(\n        format=\"s16\",\n        layout=\"mono\",\n        rate=sample_rate,\n    )\n\n    raw_buffer = io.BytesIO()\n    dtype = None\n\n    with av.open(content_io, mode=\"r\", metadata_errors=\"ignore\") as container:\n        frames = container.decode(audio=0)\n        frames = ignore_invalid_frames(frames)\n        frames = group_frames(frames, 500000)\n        frames = resample_frames(frames, resampler)\n\n        for frame in frames:\n            array = frame.to_ndarray()\n            dtype = array.dtype\n            raw_buffer.write(array)\n\n    # It appears that some objects related to the resampler are not freed\n    # unless the garbage collector is manually run.\n    del resampler\n    gc.collect()\n\n    audio = np.frombuffer(raw_buffer.getbuffer(), dtype=dtype)\n    # Convert s16 back to f32.\n    audio = audio.astype(np.float32) / 32768.0\n    return audio\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.av.pyAVWrapper.write_file","title":"write_file","text":"<pre><code>write_file(path, audio, sample_rate=16000)\n</code></pre> <p>Write an audio file in wav format to the path from numpy array.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path of the file to write.</p> <p> TYPE: <code>Path</code> </p> <code>audio</code> <p>The audio to write.</p> <p> TYPE: <code>ndarray</code> </p> <code>sample_rate</code> <p>The sample rate of the audio to save, default is 16000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16000</code> </p> Source code in <code>aana/integrations/external/av.py</code> <pre><code>@classmethod\ndef write_file(cls, path: Path, audio: np.ndarray, sample_rate: int = 16000):\n    \"\"\"Write an audio file in wav format to the path from numpy array.\n\n    Args:\n        path (Path): The path of the file to write.\n        audio (np.ndarray): The audio to write.\n        sample_rate (int): The sample rate of the audio to save, default is 16000.\n    \"\"\"\n    audio = (audio * 32768.0).astype(np.int16)\n    # Create an AV container\n    container = av.open(str(path), \"w\", format=\"wav\")\n    # Add an audio stream\n    stream = container.add_stream(\"pcm_s16le\", rate=sample_rate)\n    stream.channels = 1\n    # Write audio frames to the stream\n    for frame in av.AudioFrame.from_ndarray(\n        audio, format=\"s16\", layout=\"mono\", rate=sample_rate\n    ):\n        for packet in stream.encode(frame):\n            container.mux(packet)\n    for packet in stream.encode(None):\n        container.mux(packet)\n    container.close()\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.av.pyAVWrapper.write_to_bytes","title":"write_to_bytes","text":"<pre><code>write_to_bytes(audio)\n</code></pre> <p>Write bytes using the audio library from numpy array.</p> PARAMETER DESCRIPTION <code>audio</code> <p>The audio to write.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>bytes</code> <p>The audio as bytes.</p> <p> TYPE: <code>bytes</code> </p> Source code in <code>aana/integrations/external/av.py</code> <pre><code>@classmethod\ndef write_to_bytes(cls, audio: np.ndarray) -&gt; bytes:\n    \"\"\"Write bytes using the audio library from numpy array.\n\n    Args:\n        audio (np.ndarray): The audio to write.\n\n    Returns:\n        bytes: The audio as bytes.\n    \"\"\"\n    frame = av.AudioFrame(format=\"s16\", layout=\"mono\", samples=len(audio))\n    frame.planes[0].update(audio.astype(np.int16).tobytes())\n    return frame.planes[0].to_bytes()\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.av.pyAVWrapper.write_audio_bytes","title":"write_audio_bytes","text":"<pre><code>write_audio_bytes(path, audio, sample_rate=16000)\n</code></pre> <p>Write an audio file in wav format to path from the normalized audio bytes.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path of the file to write.</p> <p> TYPE: <code>Path</code> </p> <code>audio</code> <p>The audio to in 16-bit PCM byte write.</p> <p> TYPE: <code>bytes</code> </p> <code>sample_rate</code> <p>The sample rate of the audio, default is 16000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16000</code> </p> Source code in <code>aana/integrations/external/av.py</code> <pre><code>@classmethod\ndef write_audio_bytes(cls, path: Path, audio: bytes, sample_rate: int = 16000):\n    \"\"\"Write an audio file in wav format to path from the normalized audio bytes.\n\n    Args:\n        path (Path): The path of the file to write.\n        audio (bytes): The audio to in 16-bit PCM byte write.\n        sample_rate (int): The sample rate of the audio, default is 16000.\n    \"\"\"\n    with wave.open(str(path), \"wb\") as wav_file:\n        wav_file.setnchannels(1)  # Mono audio\n        wav_file.setsampwidth(2)  # 16-bit audio\n        wav_file.setframerate(sample_rate)  # Sample rate\n        wav_file.writeframes(audio)\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.av.load_audio","title":"load_audio","text":"<pre><code>load_audio(file, sample_rate=16000)\n</code></pre> <p>Open an audio file and read as mono waveform, resampling as necessary.</p> PARAMETER DESCRIPTION <code>file</code> <p>The audio/video file to open.</p> <p> TYPE: <code>Path</code> </p> <code>sample_rate</code> <p>The sample rate to resample the audio if necessary.</p> <p> TYPE: <code>int</code> DEFAULT: <code>16000</code> </p> RETURNS DESCRIPTION <code>bytes</code> <p>The content of the audio as bytes.</p> <p> TYPE: <code>bytes</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>if ffmpeg fails to convert and load the audio.</p> Source code in <code>aana/integrations/external/av.py</code> <pre><code>def load_audio(file: Path, sample_rate: int = 16000) -&gt; bytes:\n    \"\"\"Open an audio file and read as mono waveform, resampling as necessary.\n\n    Args:\n        file (Path): The audio/video file to open.\n        sample_rate (int): The sample rate to resample the audio if necessary.\n\n    Returns:\n        bytes: The content of the audio as bytes.\n\n    Raises:\n        RuntimeError: if ffmpeg fails to convert and load the audio.\n    \"\"\"\n    resampler = av.audio.resampler.AudioResampler(\n        format=\"s16\",\n        layout=\"mono\",\n        rate=sample_rate,\n    )\n\n    raw_buffer = io.BytesIO()\n\n    # Try loading audio and check for empty audio in one shot.\n    try:\n        with av.open(str(file), mode=\"r\", metadata_errors=\"ignore\") as container:\n            # check for empty audio\n            if container.streams.audio == tuple():\n                return b\"\"\n\n            frames = container.decode(audio=0)\n            frames = ignore_invalid_frames(frames)\n            frames = group_frames(frames, 500000)\n            frames = resample_frames(frames, resampler)\n\n            for frame in frames:\n                array = frame.to_ndarray()\n                raw_buffer.write(array)\n\n        # It appears that some objects related to the resampler are not freed\n        # unless the garbage collector is manually run.\n        del resampler\n        gc.collect()\n\n        return raw_buffer.getvalue()\n\n    except Exception as e:\n        raise RuntimeError(f\"{e!s}\") from e\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.av.ignore_invalid_frames","title":"ignore_invalid_frames","text":"<pre><code>ignore_invalid_frames(frames)\n</code></pre> <p>Filter out invalid frames from the input generator.</p> PARAMETER DESCRIPTION <code>frames</code> <p>The input generator of frames.</p> <p> TYPE: <code>Generator</code> </p> YIELDS DESCRIPTION <code>Generator</code> <p>av.audio.frame.AudioFrame: Valid audio frames.</p> RAISES DESCRIPTION <code>StopIteration</code> <p>When the input generator is exhausted.</p> Source code in <code>aana/integrations/external/av.py</code> <pre><code>def ignore_invalid_frames(frames: Generator) -&gt; Generator:\n    \"\"\"Filter out invalid frames from the input generator.\n\n    Args:\n        frames (Generator): The input generator of frames.\n\n    Yields:\n        av.audio.frame.AudioFrame: Valid audio frames.\n\n    Raises:\n        StopIteration: When the input generator is exhausted.\n    \"\"\"\n    iterator = iter(frames)\n\n    while True:\n        try:\n            yield next(iterator)\n        except StopIteration:  # noqa: PERF203\n            break\n        except av.error.InvalidDataError:\n            continue\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.av.group_frames","title":"group_frames","text":"<pre><code>group_frames(frames, num_samples=None)\n</code></pre> <p>Group audio frames and yield groups of frames based on the specified number of samples.</p> PARAMETER DESCRIPTION <code>frames</code> <p>The input generator of audio frames.</p> <p> TYPE: <code>Generator</code> </p> <code>num_samples</code> <p>The target number of samples for each group.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> YIELDS DESCRIPTION <code>Generator</code> <p>av.audio.frame.AudioFrame: Grouped audio frames.</p> Source code in <code>aana/integrations/external/av.py</code> <pre><code>def group_frames(frames: Generator, num_samples: int | None = None) -&gt; Generator:\n    \"\"\"Group audio frames and yield groups of frames based on the specified number of samples.\n\n    Args:\n        frames (Generator): The input generator of audio frames.\n        num_samples (int | None): The target number of samples for each group.\n\n    Yields:\n        av.audio.frame.AudioFrame: Grouped audio frames.\n    \"\"\"\n    fifo = av.audio.fifo.AudioFifo()\n\n    for frame in frames:\n        frame.pts = None  # Ignore timestamp check.\n        fifo.write(frame)\n\n        if num_samples is not None and fifo.samples &gt;= num_samples:\n            yield fifo.read()\n\n    if fifo.samples &gt; 0:\n        yield fifo.read()\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.av.resample_frames","title":"resample_frames","text":"<pre><code>resample_frames(frames, resampler)\n</code></pre> <p>Resample audio frames using the provided resampler.</p> PARAMETER DESCRIPTION <code>frames</code> <p>The input generator of audio frames.</p> <p> TYPE: <code>Generator</code> </p> <code>resampler</code> <p>The audio resampler.</p> <p> </p> YIELDS DESCRIPTION <code>Generator</code> <p>av.audio.frame.AudioFrame: Resampled audio frames.</p> Source code in <code>aana/integrations/external/av.py</code> <pre><code>def resample_frames(frames: Generator, resampler) -&gt; Generator:\n    \"\"\"Resample audio frames using the provided resampler.\n\n    Args:\n        frames (Generator): The input generator of audio frames.\n        resampler: The audio resampler.\n\n    Yields:\n        av.audio.frame.AudioFrame: Resampled audio frames.\n    \"\"\"\n    # Add None to flush the resampler.\n    for frame in itertools.chain(frames, [None]):\n        yield from resampler.resample(frame)\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.decord","title":"aana.integrations.external.decord","text":""},{"location":"reference/integrations/#aana.integrations.external.decord.FramesDict","title":"FramesDict","text":"<p>               Bases: <code>TypedDict</code></p> <p>Represents a set of frames with ids, timestamps and total duration.</p> ATTRIBUTE DESCRIPTION <code>frames</code> <p>the extracted frames</p> <p> TYPE: <code>list[Image]</code> </p> <code>timestamps</code> <p>the timestamps of the extracted frames</p> <p> TYPE: <code>list[float]</code> </p> <code>duration</code> <p>the total duration of the video</p> <p> TYPE: <code>float</code> </p> <code>frame_ids</code> <p>the ids of the extracted frames</p> <p> TYPE: <code>list[int]</code> </p>"},{"location":"reference/integrations/#aana.integrations.external.decord.extract_frames","title":"extract_frames","text":"<pre><code>extract_frames(video, params)\n</code></pre> <p>Extract frames from a video using decord.</p> PARAMETER DESCRIPTION <code>video</code> <p>the video to extract frames from</p> <p> TYPE: <code>Video</code> </p> <code>params</code> <p>the parameters of the video extraction</p> <p> TYPE: <code>VideoParams</code> </p> RETURNS DESCRIPTION <code>FramesDict</code> <p>a dictionary containing the extracted frames, frame_ids, timestamps, and duration</p> <p> TYPE: <code>FramesDict</code> </p> Source code in <code>aana/integrations/external/decord.py</code> <pre><code>def extract_frames(video: Video, params: VideoParams) -&gt; FramesDict:\n    \"\"\"Extract frames from a video using decord.\n\n    Args:\n        video (Video): the video to extract frames from\n        params (VideoParams): the parameters of the video extraction\n\n    Returns:\n        FramesDict: a dictionary containing the extracted frames, frame_ids, timestamps, and duration\n    \"\"\"\n    device = decord.cpu(0)\n    num_threads = 1  # TODO: see if we can use more threads\n\n    num_fps: float = params.extract_fps\n    try:\n        video_reader = decord.VideoReader(\n            str(video.path), ctx=device, num_threads=num_threads\n        )\n    except DECORDError as video_reader_exception:\n        try:\n            audio_reader = decord.AudioReader(str(video.path), ctx=device)\n            return FramesDict(\n                frames=[],\n                timestamps=[],\n                duration=audio_reader.duration(),\n                frame_ids=[],\n            )\n        except DECORDError:\n            raise VideoReadingException(video) from video_reader_exception\n\n    video_fps = video_reader.get_avg_fps()\n    num_frames = len(video_reader)\n    duration = num_frames / video_fps\n\n    if params.fast_mode_enabled:\n        indexes = video_reader.get_key_indices()\n    else:\n        # num_fps can be smaller than 1 (e.g. 0.5 means 1 frame every 2 seconds)\n        indexes = np.arange(0, num_frames, int(video_fps / num_fps))\n    timestamps = video_reader.get_frame_timestamp(indexes)[:, 0].tolist()\n\n    frames_array = video_reader.get_batch(indexes).asnumpy()\n    frames = []\n    for frame_id, frame in enumerate(frames_array):\n        img = Image(numpy=frame, media_id=f\"{video.media_id}_frame_{frame_id}\")\n        frames.append(img)\n\n    return FramesDict(\n        frames=frames,\n        timestamps=timestamps,\n        duration=duration,\n        frame_ids=list(range(len(frames))),\n    )\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.decord.get_video_duration","title":"get_video_duration","text":"<pre><code>get_video_duration(video)\n</code></pre> <p>Extract video duration using decord.</p> PARAMETER DESCRIPTION <code>video</code> <p>the video to get its duration</p> <p> TYPE: <code>Video</code> </p> RETURNS DESCRIPTION <code>float</code> <p>duration of the video</p> <p> TYPE: <code>float</code> </p> RAISES DESCRIPTION <code>VideoReadingException</code> <p>if the file is not readable or a valid multimedia file</p> Source code in <code>aana/integrations/external/decord.py</code> <pre><code>def get_video_duration(video: Video) -&gt; float:\n    \"\"\"Extract video duration using decord.\n\n    Args:\n        video (Video): the video to get its duration\n\n    Returns:\n        float: duration of the video\n\n    Raises:\n        VideoReadingException: if the file is not readable or a valid multimedia file\n    \"\"\"\n    device = decord.cpu(0)\n    try:\n        video_reader = decord.VideoReader(str(video.path), ctx=device, num_threads=1)\n    except DECORDError as video_reader_exception:\n        try:\n            audio_reader = decord.AudioReader(str(video.path), ctx=device)\n            return audio_reader.duration()\n        except DECORDError:\n            raise VideoReadingException(video) from video_reader_exception\n\n    video_fps = video_reader.get_avg_fps()\n    num_frames = len(video_reader)\n    duration = num_frames / video_fps\n    return duration\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.decord.generate_frames","title":"generate_frames","text":"<pre><code>generate_frames(video, params, batch_size=8)\n</code></pre> <p>Generate frames from a video using decord.</p> PARAMETER DESCRIPTION <code>video</code> <p>the video to extract frames from</p> <p> TYPE: <code>Video</code> </p> <code>params</code> <p>the parameters of the video extraction</p> <p> TYPE: <code>VideoParams</code> </p> <code>batch_size</code> <p>the number of frames to yield at each iteration</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> YIELDS DESCRIPTION <code>FramesDict</code> <p>a dictionary containing the extracted frames, frame ids, timestamps,         and duration for each batch</p> <p> TYPE:: <code>FramesDict</code> </p> <p>Raises:     VideoReadingException: if the file is not readable or a valid multimedia file</p> Source code in <code>aana/integrations/external/decord.py</code> <pre><code>def generate_frames(\n    video: Video, params: VideoParams, batch_size: int = 8\n) -&gt; Generator[FramesDict, None, None]:\n    \"\"\"Generate frames from a video using decord.\n\n    Args:\n        video (Video): the video to extract frames from\n        params (VideoParams): the parameters of the video extraction\n        batch_size (int): the number of frames to yield at each iteration\n\n    Yields:\n        FramesDict: a dictionary containing the extracted frames, frame ids, timestamps,\n                    and duration for each batch\n    Raises:\n        VideoReadingException: if the file is not readable or a valid multimedia file\n    \"\"\"\n    device = decord.cpu(0)\n    num_threads = 1  # TODO: see if we can use more threads\n\n    num_fps: float = params.extract_fps\n    is_audio_only = False\n    try:\n        video_reader = decord.VideoReader(\n            str(video.path), ctx=device, num_threads=num_threads\n        )\n    except DECORDError as video_reader_exception:\n        try:\n            audio_reader = decord.AudioReader(str(video.path), ctx=device)\n            is_audio_only = True\n            yield FramesDict(\n                frames=[],\n                timestamps=[],\n                duration=audio_reader.duration(),\n                frame_ids=[],\n            )\n\n        except DECORDError:\n            raise VideoReadingException(video) from video_reader_exception\n\n    if is_audio_only:\n        return\n\n    video_fps = video_reader.get_avg_fps()\n    num_frames = len(video_reader)\n    duration = num_frames / video_fps\n\n    if params.fast_mode_enabled:\n        indexes = video_reader.get_key_indices()\n    else:\n        # num_fps can be smaller than 1 (e.g. 0.5 means 1 frame every 2 seconds)\n        indexes = np.arange(0, num_frames, int(video_fps / num_fps))\n    timestamps = video_reader.get_frame_timestamp(indexes)[:, 0].tolist()\n\n    for i in range(0, len(indexes), batch_size):\n        batch = indexes[i : i + batch_size]\n        batch_frames_array = video_reader.get_batch(batch).asnumpy()\n        batch_frames = []\n        for frame_id, frame in enumerate(batch_frames_array):\n            img = Image(numpy=frame, media_id=f\"{video.media_id}_frame_{i+frame_id}\")\n            batch_frames.append(img)\n\n        batch_timestamps = timestamps[i : i + batch_size]\n        yield FramesDict(\n            frames=batch_frames,\n            frame_ids=list(range(i, i + len(batch_frames))),\n            timestamps=batch_timestamps,\n            duration=duration,\n        )\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.decord.is_audio","title":"is_audio","text":"<pre><code>is_audio(path)\n</code></pre> <p>Checks if it's a valid audio.</p> Source code in <code>aana/integrations/external/decord.py</code> <pre><code>def is_audio(path: Path) -&gt; bool:\n    \"\"\"Checks if it's a valid audio.\"\"\"\n    try:\n        decord.AudioReader(str(path))\n    except DECORDError:\n        return False\n    return True\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.opencv","title":"aana.integrations.external.opencv","text":""},{"location":"reference/integrations/#aana.integrations.external.opencv.OpenCVWrapper","title":"OpenCVWrapper","text":"<p>               Bases: <code>AbstractImageLibrary</code></p> <p>Wrapper class for OpenCV functions.</p>"},{"location":"reference/integrations/#aana.integrations.external.opencv.OpenCVWrapper.read_file","title":"read_file","text":"<pre><code>read_file(path)\n</code></pre> <p>Read a file using OpenCV.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path of the file to read.</p> <p> TYPE: <code>Path</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: The file as a numpy array in RGB format.</p> Source code in <code>aana/integrations/external/opencv.py</code> <pre><code>@classmethod\ndef read_file(cls, path: Path) -&gt; np.ndarray:\n    \"\"\"Read a file using OpenCV.\n\n    Args:\n        path (Path): The path of the file to read.\n\n    Returns:\n        np.ndarray: The file as a numpy array in RGB format.\n    \"\"\"\n    img = cv2.imread(str(path))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.opencv.OpenCVWrapper.read_from_bytes","title":"read_from_bytes","text":"<pre><code>read_from_bytes(content)\n</code></pre> <p>Read bytes using OpenCV.</p> PARAMETER DESCRIPTION <code>content</code> <p>The content of the file to read.</p> <p> TYPE: <code>bytes</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>np.ndarray: The file as a numpy array in RGB format.</p> Source code in <code>aana/integrations/external/opencv.py</code> <pre><code>@classmethod\ndef read_from_bytes(cls, content: bytes) -&gt; np.ndarray:\n    \"\"\"Read bytes using OpenCV.\n\n    Args:\n        content (bytes): The content of the file to read.\n\n    Returns:\n        np.ndarray: The file as a numpy array in RGB format.\n    \"\"\"\n    img = cv2.imdecode(np.frombuffer(content, np.uint8), cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.opencv.OpenCVWrapper.write_file","title":"write_file","text":"<pre><code>write_file(path, img)\n</code></pre> <p>Write a file using OpenCV.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path of the file to write.</p> <p> TYPE: <code>Path</code> </p> <code>img</code> <p>The image to write.</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>aana/integrations/external/opencv.py</code> <pre><code>@classmethod\ndef write_file(cls, path: Path, img: np.ndarray):\n    \"\"\"Write a file using OpenCV.\n\n    Args:\n        path (Path): The path of the file to write.\n        img (np.ndarray): The image to write.\n    \"\"\"\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    cv2.imwrite(str(path), img)\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.opencv.OpenCVWrapper.write_to_bytes","title":"write_to_bytes","text":"<pre><code>write_to_bytes(img)\n</code></pre> <p>Write bytes using OpenCV.</p> PARAMETER DESCRIPTION <code>img</code> <p>The image to write.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>bytes</code> <p>The image as bytes.</p> <p> TYPE: <code>bytes</code> </p> Source code in <code>aana/integrations/external/opencv.py</code> <pre><code>@classmethod\ndef write_to_bytes(cls, img: np.ndarray) -&gt; bytes:\n    \"\"\"Write bytes using OpenCV.\n\n    Args:\n        img (np.ndarray): The image to write.\n\n    Returns:\n        bytes: The image as bytes.\n    \"\"\"\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    _, buffer = cv2.imencode(\".bmp\", img)\n    return buffer.tobytes()\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.yt_dlp","title":"aana.integrations.external.yt_dlp","text":""},{"location":"reference/integrations/#aana.integrations.external.yt_dlp.get_video_metadata","title":"get_video_metadata","text":"<pre><code>get_video_metadata(video_url)\n</code></pre> <p>Fetch video's metadata for a url.</p> PARAMETER DESCRIPTION <code>video_url</code> <p>the video input url</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>metadata</code> <p>the metadata of the video</p> <p> TYPE: <code>VideoMetadata</code> </p> RAISES DESCRIPTION <code>DownloadException</code> <p>Request does not succeed.</p> Source code in <code>aana/integrations/external/yt_dlp.py</code> <pre><code>def get_video_metadata(video_url: str) -&gt; VideoMetadata:\n    \"\"\"Fetch video's metadata for a url.\n\n    Args:\n        video_url (str): the video input url\n\n    Returns:\n        metadata (VideoMetadata): the metadata of the video\n\n    Raises:\n        DownloadException: Request does not succeed.\n    \"\"\"\n    ydl_options = {\n        \"extract_flat\": True,\n        \"hls_prefer_native\": True,\n        \"extractor_args\": {\"youtube\": {\"skip\": [\"hls\", \"dash\"]}},\n    }\n    try:\n        with yt_dlp.YoutubeDL(ydl_options) as ydl:\n            info = ydl.extract_info(video_url, download=False)\n            title = info.get(\"title\", \"\")\n            description = info.get(\"description\", \"\")\n            duration = info.get(\"duration\")\n            return VideoMetadata(\n                title=title,\n                description=description,\n                duration=duration,\n            )\n    except DownloadError as e:\n        error_message = e.msg.split(\";\")[0]\n        raise DownloadException(url=video_url, msg=error_message) from e\n</code></pre>"},{"location":"reference/integrations/#aana.integrations.external.yt_dlp.download_video","title":"download_video","text":"<pre><code>download_video(video_input)\n</code></pre> <p>Downloads videos for a VideoInput object.</p> PARAMETER DESCRIPTION <code>video_input</code> <p>the video input to download</p> <p> TYPE: <code>VideoInput</code> </p> RETURNS DESCRIPTION <code>Video</code> <p>the video object</p> <p> TYPE: <code>Video</code> </p> RAISES DESCRIPTION <code>DownloadException</code> <p>Request does not succeed.</p> Source code in <code>aana/integrations/external/yt_dlp.py</code> <pre><code>def download_video(video_input: VideoInput | Video) -&gt; Video:\n    \"\"\"Downloads videos for a VideoInput object.\n\n    Args:\n        video_input (VideoInput): the video input to download\n\n    Returns:\n        Video: the video object\n\n    Raises:\n        DownloadException: Request does not succeed.\n    \"\"\"\n    if isinstance(video_input, Video):\n        return video_input\n    if video_input.url is not None:\n        video_dir = settings.video_dir\n        url_hash = hashlib.md5(\n            video_input.url.encode(), usedforsecurity=False\n        ).hexdigest()\n\n        # we use yt_dlp to download the video\n        # it works not only for youtube videos, but also for other websites and direct links\n        ydl_options = {\n            \"outtmpl\": f\"{video_dir}/{url_hash}.%(ext)s\",\n            \"extract_flat\": True,\n            \"hls_prefer_native\": True,\n            \"extractor_args\": {\"youtube\": {\"skip\": [\"hls\", \"dash\"]}},\n        }\n        try:\n            with yt_dlp.YoutubeDL(ydl_options) as ydl:\n                info = ydl.extract_info(video_input.url, download=False)\n                title = info.get(\"title\", \"\")\n                description = info.get(\"description\", \"\")\n                path = Path(ydl.prepare_filename(info))\n                if not path.exists():\n                    ydl.download([video_input.url])\n                if not path.exists():\n                    raise DownloadException(video_input.url)\n                return Video(\n                    path=path,\n                    url=video_input.url,\n                    media_id=video_input.media_id,\n                    title=title,\n                    description=description,\n                )\n        except DownloadError as e:\n            # removes the yt-dlp request to file an issue\n            error_message = e.msg.split(\";\")[0]\n            raise DownloadException(url=video_input.url, msg=error_message) from e\n    else:\n        return video_input.convert_input_to_object()\n</code></pre>"},{"location":"reference/processors/","title":"Processors","text":""},{"location":"reference/processors/#aana.processors.remote","title":"aana.processors.remote","text":""},{"location":"reference/processors/#aana.processors.remote.run_remote","title":"run_remote","text":"<pre><code>run_remote(func)\n</code></pre> <p>Wrap a function to run it remotely on Ray.</p> PARAMETER DESCRIPTION <code>func</code> <p>the function to wrap</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>the wrapped function</p> <p> TYPE: <code>Callable</code> </p> Source code in <code>aana/processors/remote.py</code> <pre><code>def run_remote(func: Callable) -&gt; Callable:\n    \"\"\"Wrap a function to run it remotely on Ray.\n\n    Args:\n        func (Callable): the function to wrap\n\n    Returns:\n        Callable: the wrapped function\n    \"\"\"\n\n    async def generator_wrapper(*args, **kwargs):\n        async for item in ray.remote(func).remote(*args, **kwargs):\n            yield await item\n\n    if inspect.isgeneratorfunction(func):\n        return generator_wrapper\n    else:\n        return ray.remote(func).remote\n</code></pre>"},{"location":"reference/processors/#aana.processors.video","title":"aana.processors.video","text":""},{"location":"reference/processors/#aana.processors.video.extract_audio","title":"extract_audio","text":"<pre><code>extract_audio(video)\n</code></pre> <p>Extract the audio file from a Video and return an Audio object.</p> PARAMETER DESCRIPTION <code>video</code> <p>The video file to extract audio.</p> <p> TYPE: <code>Video</code> </p> RETURNS DESCRIPTION <code>Audio</code> <p>an Audio object containing the extracted audio.</p> <p> TYPE: <code>Audio</code> </p> Source code in <code>aana/processors/video.py</code> <pre><code>def extract_audio(video: Video) -&gt; Audio:\n    \"\"\"Extract the audio file from a Video and return an Audio object.\n\n    Args:\n        video (Video): The video file to extract audio.\n\n    Returns:\n        Audio: an Audio object containing the extracted audio.\n    \"\"\"\n    audio_bytes = load_audio(video.path)\n\n    # Only difference will be in path where WAV file will be stored\n    # and in content but has same media_id\n    return Audio(\n        url=video.url,\n        media_id=f\"audio_{video.media_id}\",\n        content=audio_bytes,\n        title=video.title,\n        description=video.description,\n    )\n</code></pre>"},{"location":"reference/processors/#aana.processors.batch","title":"aana.processors.batch","text":""},{"location":"reference/processors/#aana.processors.batch.BatchProcessor","title":"BatchProcessor","text":"<pre><code>BatchProcessor(process_batch, batch_size, num_threads)\n</code></pre> <p>Class for parallel processing data in chunks.</p> <p>The BatchProcessor class encapsulates the logic required to take a large collection of data, split it into manageable batches, process these batches in parallel, and then combine the results into a single cohesive output.</p> <p>Batching works by iterating through the input request, which is a dictionary where each key maps to a list-like collection of data. The class splits each collection into sublists of length up to <code>batch_size</code>, ensuring that corresponding elements across the collections remain grouped together in their respective batches.</p> <p>Merging takes the output from each processed batch, which is also a dictionary structure, and combines these into a single dictionary. Lists are extended, numpy arrays are concatenated, and dictionaries are updated. If a new data type is encountered, an error is raised prompting the implementer to specify how these types should be merged.</p> <p>This class is particularly useful for batching of requests to a machine learning model.</p> <p>The thread pool for parallel processing is managed internally and is shut down automatically when the BatchProcessor instance is garbage collected.</p> ATTRIBUTE DESCRIPTION <code>process_batch</code> <p>A function to process each batch.</p> <p> TYPE: <code>Callable</code> </p> <code>batch_size</code> <p>The size of each batch to be processed.</p> <p> TYPE: <code>int</code> </p> <code>num_threads</code> <p>The number of threads in the thread pool for parallel processing.</p> <p> TYPE: <code>int</code> </p> PARAMETER DESCRIPTION <code>process_batch</code> <p>Function that processes each batch.</p> <p> TYPE: <code>Callable</code> </p> <code>batch_size</code> <p>Size of the batches.</p> <p> TYPE: <code>int</code> </p> <code>num_threads</code> <p>Number of threads in the pool.</p> <p> TYPE: <code>int</code> </p> Source code in <code>aana/processors/batch.py</code> <pre><code>def __init__(self, process_batch: Callable, batch_size: int, num_threads: int):\n    \"\"\"Constructor.\n\n    Args:\n        process_batch (Callable): Function that processes each batch.\n        batch_size (int): Size of the batches.\n        num_threads (int): Number of threads in the pool.\n    \"\"\"\n    self.process_batch = process_batch\n    self.batch_size = batch_size\n    self.pool = ThreadPoolExecutor(num_threads)\n</code></pre>"},{"location":"reference/processors/#aana.processors.batch.BatchProcessor.batch_iterator","title":"batch_iterator","text":"<pre><code>batch_iterator(request)\n</code></pre> <p>Converts request into an iterator of batches.</p> <p>Iterates over the input request, breaking it into smaller batches for processing. Each batch is a dictionary with the same keys as the input request, but the values are sublists containing only the elements for that batch.</p> <p>Example: <pre><code>request = {\n    'images': [img1, img2, img3, img4, img5],\n    'texts': ['text1', 'text2', 'text3', 'text4', 'text5']\n}\n# Assuming a batch size of 2, this iterator would yield:\n# 1st iteration: {'images': [img1, img2], 'texts': ['text1', 'text2']}\n# 2nd iteration: {'images': [img3, img4], 'texts': ['text3', 'text4']}\n# 3rd iteration: {'images': [img5], 'texts': ['text5']}\n</code></pre></p> PARAMETER DESCRIPTION <code>request</code> <p>The request data to split into batches.</p> <p> TYPE: <code>dict[str, list[Any]]</code> </p> YIELDS DESCRIPTION <code>dict[str, list[Any]]</code> <p>Iterator[dict[str, list[Any]]]: An iterator over the batched requests.</p> Source code in <code>aana/processors/batch.py</code> <pre><code>def batch_iterator(self, request: dict[str, Any]) -&gt; Iterator[dict[str, list[Any]]]:\n    \"\"\"Converts request into an iterator of batches.\n\n    Iterates over the input request, breaking it into smaller batches for processing.\n    Each batch is a dictionary with the same keys as the input request, but the values\n    are sublists containing only the elements for that batch.\n\n    Example:\n    ```python\n    request = {\n        'images': [img1, img2, img3, img4, img5],\n        'texts': ['text1', 'text2', 'text3', 'text4', 'text5']\n    }\n    # Assuming a batch size of 2, this iterator would yield:\n    # 1st iteration: {'images': [img1, img2], 'texts': ['text1', 'text2']}\n    # 2nd iteration: {'images': [img3, img4], 'texts': ['text3', 'text4']}\n    # 3rd iteration: {'images': [img5], 'texts': ['text5']}\n    ```\n\n    Args:\n        request (dict[str, list[Any]]): The request data to split into batches.\n\n    Yields:\n        Iterator[dict[str, list[Any]]]: An iterator over the batched requests.\n    \"\"\"\n    lengths = [len(value) for value in request.values()]\n    if len(set(lengths)) &gt; 1:\n        raise ValueError(\"All inputs must have the same length\")  # noqa: TRY003\n\n    total_batches = (max(lengths) + self.batch_size - 1) // self.batch_size\n    for i in range(total_batches):\n        start = i * self.batch_size\n        end = start + self.batch_size\n        yield {key: value[start:end] for key, value in request.items()}\n</code></pre>"},{"location":"reference/processors/#aana.processors.batch.BatchProcessor.process","title":"process","text":"<pre><code>process(request)\n</code></pre> <p>Process a request.</p> <p>Splits the input request into batches, processes each batch in parallel, and then merges the results into a single dictionary.</p> PARAMETER DESCRIPTION <code>request</code> <p>The request data to process.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>Dict[str, Any]: The merged results from processing all batches.</p> Source code in <code>aana/processors/batch.py</code> <pre><code>async def process(self, request: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Process a request.\n\n    Splits the input request into batches, processes each batch in parallel, and then merges\n    the results into a single dictionary.\n\n    Args:\n        request (Dict[str, Any]): The request data to process.\n\n    Returns:\n        Dict[str, Any]: The merged results from processing all batches.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    futures = [\n        loop.run_in_executor(self.pool, self.process_batch, batch)\n        for batch in self.batch_iterator(request)\n    ]\n    outputs = await asyncio.gather(*futures)\n    return self.merge_outputs(outputs)\n</code></pre>"},{"location":"reference/processors/#aana.processors.batch.BatchProcessor.merge_outputs","title":"merge_outputs","text":"<pre><code>merge_outputs(outputs)\n</code></pre> <p>Merge output.</p> <p>Combine processed batch outputs into a single dictionary. It handles various data types by extending lists, updating dictionaries, and concatenating numpy arrays.</p> <p>Example: <pre><code>outputs = [\n    {'images': [processed_img1, processed_img2], 'labels': ['cat', 'dog']},\n    {'images': [processed_img3, processed_img4], 'labels': ['bird', 'mouse']},\n    {'images': [processed_img5], 'labels': ['fish']}\n]\n# The merged result would be:\n# {\n#     'images': [processed_img1, processed_img2, processed_img3, processed_img4, processed_img5],\n#     'labels': ['cat', 'dog', 'bird', 'mouse', 'fish']\n# }\n</code></pre></p> PARAMETER DESCRIPTION <code>outputs</code> <p>List of outputs from the processed batches.</p> <p> TYPE: <code>list[dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>dict[str, Any]: The merged result.</p> Source code in <code>aana/processors/batch.py</code> <pre><code>def merge_outputs(self, outputs: list[dict[str, Any]]) -&gt; dict[str, Any]:\n    \"\"\"Merge output.\n\n    Combine processed batch outputs into a single dictionary. It handles various data types\n    by extending lists, updating dictionaries, and concatenating numpy arrays.\n\n    Example:\n    ```python\n    outputs = [\n        {'images': [processed_img1, processed_img2], 'labels': ['cat', 'dog']},\n        {'images': [processed_img3, processed_img4], 'labels': ['bird', 'mouse']},\n        {'images': [processed_img5], 'labels': ['fish']}\n    ]\n    # The merged result would be:\n    # {\n    #     'images': [processed_img1, processed_img2, processed_img3, processed_img4, processed_img5],\n    #     'labels': ['cat', 'dog', 'bird', 'mouse', 'fish']\n    # }\n    ```\n\n    Args:\n        outputs (list[dict[str, Any]]): List of outputs from the processed batches.\n\n    Returns:\n        dict[str, Any]: The merged result.\n    \"\"\"\n    merged_output = {}\n    for output in outputs:\n        for key, value in output.items():\n            if key not in merged_output:\n                merged_output[key] = value\n            else:\n                if isinstance(value, list):\n                    merged_output[key].extend(value)\n                elif isinstance(value, dict):\n                    merged_output[key].update(value)\n                elif isinstance(value, np.ndarray):\n                    if key in merged_output:\n                        merged_output[key] = np.concatenate(\n                            (merged_output[key], value)\n                        )\n                    else:\n                        merged_output[key] = value\n                else:\n                    raise NotImplementedError(\n                        \"Merging of this data type is not implemented\"\n                    )\n    return merged_output\n</code></pre>"},{"location":"reference/processors/#aana.processors.speaker.PostProcessingForDiarizedAsr","title":"aana.processors.speaker.PostProcessingForDiarizedAsr","text":"<p>Class to handle post-processing for diarized ASR output by combining diarization and transcription segments.</p> <p>The post-processing involves assigning speaker labels to transcription segments and words, aligning speakers with punctuation, optionally merging homogeneous speaker segments, and reassigning confidence information to the segments.</p> ATTRIBUTE DESCRIPTION <code>diarized_segments</code> <p>Contains speaker diarization segments.</p> <p> TYPE: <code>list[SpeakerDiarizationSegment]</code> </p> <code>transcription_segments</code> <p>Transcription segments.</p> <p> TYPE: <code>list[AsrSegment]</code> </p> <code>merge</code> <p>Whether to merge the same speaker segments in the final output.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"reference/processors/#aana.processors.speaker.PostProcessingForDiarizedAsr.process","title":"process","text":"<pre><code>process(diarized_segments, transcription_segments, merge=False)\n</code></pre> <p>Executes the post-processing pipeline that combines diarization and transcription segments.</p> <p>This method performs the following steps: 1. Assign speaker labels to each segment and word in the transcription based on the diarization output. 2. Align speakers with punctuation. 3. Create new transcription segments by combining the speaker-labeled words. 4. Optionally, merge consecutive speaker segments. 5. Add confidence and no_speech_confidence to the new segments.</p> PARAMETER DESCRIPTION <code>diarized_segments</code> <p>Contains speaker diarization segments.</p> <p> TYPE: <code>list[SpeakerDiarizationSegment]</code> </p> <code>transcription_segments</code> <p>Transcription segments.</p> <p> TYPE: <code>list[AsrSegment]</code> </p> <code>merge</code> <p>If True, merges consecutive speaker segments in the final output. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>list[AsrSegment]</code> <p>list[AsrSegment]: Updated transcription segments with speaker information per segment and word.</p> Source code in <code>aana/processors/speaker.py</code> <pre><code>@classmethod\ndef process(\n    cls,\n    diarized_segments: list[SpeakerDiarizationSegment],\n    transcription_segments: list[AsrSegment],\n    merge: bool = False,\n) -&gt; list[AsrSegment]:\n    \"\"\"Executes the post-processing pipeline that combines diarization and transcription segments.\n\n    This method performs the following steps:\n    1. Assign speaker labels to each segment and word in the transcription based on the diarization output.\n    2. Align speakers with punctuation.\n    3. Create new transcription segments by combining the speaker-labeled words.\n    4. Optionally, merge consecutive speaker segments.\n    5. Add confidence and no_speech_confidence to the new segments.\n\n    Args:\n        diarized_segments (list[SpeakerDiarizationSegment]): Contains speaker diarization segments.\n        transcription_segments (list[AsrSegment]): Transcription segments.\n        merge (bool): If True, merges consecutive speaker segments in the final output. Defaults to False.\n\n    Returns:\n        list[AsrSegment]: Updated transcription segments with speaker information per segment and word.\n    \"\"\"\n    # intro: validation checks\n    if not transcription_segments or not diarized_segments:\n        return transcription_segments\n\n    # Check if inputs are valid:\n    for segment in transcription_segments:\n        if segment.text and not segment.words:\n            raise ValueError(\"Word-level timestamps are required for diarized ASR.\")  # noqa: TRY003\n\n    # 1. Assign speaker labels to each segment and word\n    speaker_labelled_transcription = cls._assign_word_speakers(\n        diarized_segments, transcription_segments\n    )\n\n    # 2. Align speakers with punctuation\n    word_speaker_mapping = cls._align_with_punctuation(\n        speaker_labelled_transcription\n    )\n\n    # 3. Create new transcription segments with speaker information\n    segments = cls._create_speaker_segments(word_speaker_mapping)\n\n    # 4. Assign confidence variables to the new segments\n    segments = cls._add_segment_variables(segments, transcription_segments)\n\n    # Optional: Merge consecutive speaker segments\n    if merge:\n        segments = cls._combine_homeogeneous_speaker_asr_segments(segments)\n\n    return segments\n</code></pre>"},{"location":"reference/sdk/","title":"SDK","text":""},{"location":"reference/sdk/#aana.sdk.AanaSDK","title":"aana.sdk.AanaSDK","text":"<pre><code>AanaSDK(name='app', migration_func=None, retryable_exceptions=None)\n</code></pre> <p>Aana SDK to deploy and manage Aana deployments and endpoints.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the application. Defaults to \"app\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'app'</code> </p> <code>migration_func</code> <p>The migration function to run. Defaults to None.</p> <p> TYPE: <code>Callable | None</code> DEFAULT: <code>None</code> </p> <code>retryable_exceptions</code> <p>The exceptions that can be retried in the task queue.                                                 Defaults to ['InferenceException', 'ActorDiedError', 'OutOfMemoryError'].</p> <p> TYPE: <code>list[Exception, str] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"app\",\n    migration_func: Callable | None = None,\n    retryable_exceptions: list[Exception, str] | None = None,\n):\n    \"\"\"Aana SDK to deploy and manage Aana deployments and endpoints.\n\n    Args:\n        name (str, optional): The name of the application. Defaults to \"app\".\n        migration_func (Callable | None): The migration function to run. Defaults to None.\n        retryable_exceptions (list[Exception, str] | None): The exceptions that can be retried in the task queue.\n                                                            Defaults to ['InferenceException', 'ActorDiedError', 'OutOfMemoryError'].\n    \"\"\"\n    self.name = name\n    self.migration_func = migration_func\n    self.endpoints: dict[str, Endpoint] = {}\n    self.deployments: dict[str, Deployment] = {}\n    self.routers: dict[str, APIRouter] = {}\n\n    if retryable_exceptions is None:\n        self.retryable_exceptions = [\n            \"InferenceException\",\n            \"ActorDiedError\",\n            \"OutOfMemoryError\",\n        ]\n    else:\n        self.retryable_exceptions = retryable_exceptions\n    # Convert exceptions to string if they are not already\n    # to avoid serialization issues\n    self.retryable_exceptions = [\n        exc if isinstance(exc, str) else exc.__name__\n        for exc in self.retryable_exceptions\n    ]\n\n    if aana_settings.task_queue.enabled:\n        self.add_task_queue(deploy=False)\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.connect","title":"connect","text":"<pre><code>connect(port=8000, host='127.0.0.1', address='auto', dashboard_host='127.0.0.1', dashboard_port=8265, show_logs=False, num_cpus=None, num_gpus=None)\n</code></pre> <p>Connect to a Ray cluster or start a new Ray cluster and Ray Serve.</p> PARAMETER DESCRIPTION <code>port</code> <p>The port to run the Aana server on. Defaults to 8000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8000</code> </p> <code>host</code> <p>The host to run the Aana server on. Defaults to \"127.0.0.1\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'127.0.0.1'</code> </p> <code>address</code> <p>The address of the Ray cluster. Defaults to \"auto\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'auto'</code> </p> <code>dashboard_host</code> <p>The host to bind the dashboard server to. Can either be localhost (127.0.0.1) or 0.0.0.0 (available from all interfaces). By default, this is set to localhost to prevent access from external machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'127.0.0.1'</code> </p> <code>dashboard_port</code> <p>The port to bind the dashboard server to. Defaults to 8265.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8265</code> </p> <code>show_logs</code> <p>If True, the logs will be shown, otherwise they will be hidden but can be accessed in the Ray dashboard. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_cpus</code> <p>Number of CPUs the user wishes to assign to each raylet. By default, this is set based on virtual cores.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>num_gpus</code> <p>Number of GPUs the user wishes to assign to each raylet. By default, this is set based on detected GPUs.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def connect(\n    self,\n    port: int = 8000,\n    host: str = \"127.0.0.1\",\n    address: str = \"auto\",\n    dashboard_host: str = \"127.0.0.1\",\n    dashboard_port: int = 8265,\n    show_logs: bool = False,\n    num_cpus: int | None = None,\n    num_gpus: int | None = None,\n) -&gt; \"AanaSDK\":\n    \"\"\"Connect to a Ray cluster or start a new Ray cluster and Ray Serve.\n\n    Args:\n        port (int, optional): The port to run the Aana server on. Defaults to 8000.\n        host (str, optional): The host to run the Aana server on. Defaults to \"127.0.0.1\".\n        address (str, optional): The address of the Ray cluster. Defaults to \"auto\".\n        dashboard_host (str, optional):  The host to bind the dashboard server to. Can either be\n            localhost (127.0.0.1) or 0.0.0.0 (available from all interfaces).\n            By default, this is set to localhost to prevent access from external machines.\n        dashboard_port (int, optional): The port to bind the dashboard server to. Defaults to 8265.\n        show_logs (bool, optional): If True, the logs will be shown, otherwise\n            they will be hidden but can be accessed in the Ray dashboard. Defaults to False.\n        num_cpus (int, optional): Number of CPUs the user wishes to assign to each\n            raylet. By default, this is set based on virtual cores.\n        num_gpus (int, optional): Number of GPUs the user wishes to assign to each\n            raylet. By default, this is set based on detected GPUs.\n    \"\"\"\n    self.port = port\n    self.host = host\n\n    try:\n        # Try to connect to an existing Ray cluster\n        ray.init(\n            address=address,\n            ignore_reinit_error=True,\n            log_to_driver=show_logs,\n        )\n    except ConnectionError:\n        # If connection fails, start a new Ray cluster and serve instance\n        ray.init(\n            ignore_reinit_error=True,\n            log_to_driver=show_logs,\n            num_cpus=num_cpus,\n            num_gpus=num_gpus,\n            include_dashboard=True,\n            dashboard_host=dashboard_host,\n            dashboard_port=dashboard_port,\n        )\n\n    serve_status = serve.status()\n    if serve_status.proxies == {}:  # If serve is not running yet\n        # TODO: check if the port is already in use if serve is not running yet or\n        # check if the port is the same as an existing serve instance if serve is running\n        serve.start(http_options=HTTPOptions(port=self.port, host=self.host))\n\n    return self\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.migrate","title":"migrate","text":"<pre><code>migrate()\n</code></pre> <p>Run Alembic migrations.</p> Source code in <code>aana/sdk.py</code> <pre><code>def migrate(self):\n    \"\"\"Run Alembic migrations.\"\"\"\n    if self.migration_func:\n        try:\n            self.migration_func(aana_settings)\n        except EmptyMigrationsException:\n            print(\n                \"No versions found in the custom migrations. Using default migrations.\"\n            )\n            run_alembic_migrations(aana_settings)\n    else:\n        run_alembic_migrations(aana_settings)\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.print_app_status","title":"print_app_status","text":"<pre><code>print_app_status(app_name, app_status)\n</code></pre> <p>Show the status of the application using simple ASCII formatting.</p> PARAMETER DESCRIPTION <code>app_name</code> <p>The name of the application.</p> <p> TYPE: <code>str</code> </p> <code>app_status</code> <p>The status of the application.</p> <p> TYPE: <code>ApplicationStatusOverview</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def print_app_status(self, app_name: str, app_status: ApplicationStatusOverview):\n    \"\"\"Show the status of the application using simple ASCII formatting.\n\n    Args:\n        app_name (str): The name of the application.\n        app_status (ApplicationStatusOverview): The status of the application.\n    \"\"\"\n\n    def print_separator(end=\"\\n\"):\n        print(\"=\" * 60, end=end)\n\n    def print_header(title):\n        print_separator()\n        print(title)\n        print_separator()\n\n    def print_key_value(key, value, indent=0):\n        print(f\"{' ' * indent}{key}: {value}\")\n\n    if app_status.deployments:\n        for deployment_name, deployment_status in app_status.deployments.items():\n            print_header(f\"{deployment_name} ({app_name})\")\n            print_key_value(\"Status\", deployment_status.status.value, indent=0)\n            print_key_value(\"Message\", deployment_status.message, indent=0)\n    print_separator()\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.add_task_queue","title":"add_task_queue","text":"<pre><code>add_task_queue(deploy=False)\n</code></pre> <p>Add a task queue deployment.</p> PARAMETER DESCRIPTION <code>deploy</code> <p>If True, the deployment will be deployed immediately,     otherwise it will be registered and can be deployed later when deploy() is called. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def add_task_queue(self, deploy: bool = False):\n    \"\"\"Add a task queue deployment.\n\n    Args:\n        deploy (bool, optional): If True, the deployment will be deployed immediately,\n                otherwise it will be registered and can be deployed later when deploy() is called. Defaults to False.\n    \"\"\"\n    from aana.deployments.task_queue_deployment import (\n        TaskQueueConfig,\n        TaskQueueDeployment,\n    )\n\n    task_queue_deployment = TaskQueueDeployment.options(\n        num_replicas=1,\n        user_config=TaskQueueConfig(\n            app_name=self.name,\n            retryable_exceptions=self.retryable_exceptions,\n        ).model_dump(mode=\"json\"),\n    )\n    self.register_deployment(\n        \"task_queue_deployment\",\n        task_queue_deployment,\n        deploy=deploy,\n    )\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.register_deployment","title":"register_deployment","text":"<pre><code>register_deployment(name, instance, deploy=False)\n</code></pre> <p>Register a deployment.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the deployment.</p> <p> TYPE: <code>str</code> </p> <code>instance</code> <p>The instance of the deployment to be registered.</p> <p> TYPE: <code>Deployment</code> </p> <code>deploy</code> <p>If True, the deployment will be deployed immediately,     otherwise it will be registered and can be deployed later when deploy() is called. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def register_deployment(\n    self,\n    name: str,\n    instance: Deployment,\n    deploy: bool = False,\n):\n    \"\"\"Register a deployment.\n\n    Args:\n        name (str): The name of the deployment.\n        instance (Deployment): The instance of the deployment to be registered.\n        deploy (bool, optional): If True, the deployment will be deployed immediately,\n                otherwise it will be registered and can be deployed later when deploy() is called. Defaults to False.\n    \"\"\"\n    if deploy:\n        try:\n            serve.api._run(\n                instance.bind(),\n                name=name,\n                route_prefix=f\"/{name}\",\n                _blocking=False,\n            )\n            self.wait_for_deployment()\n        except FailedDeployment:\n            status = serve.status()\n            app_status = status.applications[name]\n            self.print_app_status(name, app_status)\n    else:\n        self.deployments[name] = instance\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.get_deployment_app","title":"get_deployment_app","text":"<pre><code>get_deployment_app(name)\n</code></pre> <p>Get the application instance for the deployment.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the deployment.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Application</code> <p>The application instance for the deployment.</p> <p> TYPE: <code>Application</code> </p> RAISES DESCRIPTION <code>KeyError</code> <p>If the deployment is not found.</p> Source code in <code>aana/sdk.py</code> <pre><code>def get_deployment_app(self, name: str) -&gt; Application:\n    \"\"\"Get the application instance for the deployment.\n\n    Args:\n        name (str): The name of the deployment.\n\n    Returns:\n        Application: The application instance for the deployment.\n\n    Raises:\n        KeyError: If the deployment is not found.\n    \"\"\"\n    if name in self.deployments:\n        return self.deployments[name].bind()\n    else:\n        raise KeyError(f\"Deployment {name} not found.\")  # noqa: TRY003\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.unregister_deployment","title":"unregister_deployment","text":"<pre><code>unregister_deployment(name)\n</code></pre> <p>Unregister a deployment.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the deployment to be unregistered.</p> <p> TYPE: <code>str</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def unregister_deployment(self, name: str):\n    \"\"\"Unregister a deployment.\n\n    Args:\n        name (str): The name of the deployment to be unregistered.\n    \"\"\"\n    if name in self.deployments:\n        del self.deployments[name]\n    serve.delete(name)\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.get_main_app","title":"get_main_app","text":"<pre><code>get_main_app()\n</code></pre> <p>Get the main application instance.</p> RETURNS DESCRIPTION <code>Application</code> <p>The main application instance.</p> <p> TYPE: <code>Application</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def get_main_app(self) -&gt; Application:\n    \"\"\"Get the main application instance.\n\n    Returns:\n        Application: The main application instance.\n    \"\"\"\n    return RequestHandler.options(num_replicas=aana_settings.num_workers).bind(\n        app_name=self.name,\n        endpoints=self.endpoints.values(),\n        deployments=list(self.deployments.keys()),\n        routers=list(self.routers.values()),\n    )\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.register_endpoint","title":"register_endpoint","text":"<pre><code>register_endpoint(name, path, summary, endpoint_cls, admin_required=False, active_subscription_required=False, defer_option=DeferOption.OPTIONAL, event_handlers=None)\n</code></pre> <p>Register an endpoint.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the endpoint.</p> <p> TYPE: <code>str</code> </p> <code>path</code> <p>The path of the endpoint.</p> <p> TYPE: <code>str</code> </p> <code>summary</code> <p>The summary of the endpoint.</p> <p> TYPE: <code>str</code> </p> <code>endpoint_cls</code> <p>The class of the endpoint.</p> <p> TYPE: <code>Type[Endpoint]</code> </p> <code>admin_required</code> <p>If True, the endpoint requires admin access. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>active_subscription_required</code> <p>If True, the endpoint requires an active subscription. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>defer_option</code> <p>Defer option for the endpoint (always, never, optional).</p> <p> TYPE: <code>DeferOption</code> DEFAULT: <code>OPTIONAL</code> </p> <code>event_handlers</code> <p>The event handlers to register for the endpoint.</p> <p> TYPE: <code>list[EventHandler]</code> DEFAULT: <code>None</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def register_endpoint(\n    self,\n    name: str,\n    path: str,\n    summary: str,\n    endpoint_cls: type[Endpoint],\n    admin_required: bool = False,\n    active_subscription_required: bool = False,\n    defer_option: DeferOption = DeferOption.OPTIONAL,\n    event_handlers: list[EventHandler] | None = None,\n):\n    \"\"\"Register an endpoint.\n\n    Args:\n        name (str): The name of the endpoint.\n        path (str): The path of the endpoint.\n        summary (str): The summary of the endpoint.\n        endpoint_cls (Type[Endpoint]): The class of the endpoint.\n        admin_required (bool, optional): If True, the endpoint requires admin access. Defaults to False.\n        active_subscription_required (bool, optional): If True, the endpoint requires an active subscription. Defaults to False.\n        defer_option (DeferOption): Defer option for the endpoint (always, never, optional).\n        event_handlers (list[EventHandler], optional): The event handlers to register for the endpoint.\n    \"\"\"\n    endpoint = endpoint_cls(\n        name=name,\n        path=path,\n        summary=summary,\n        admin_required=admin_required,\n        active_subscription_required=active_subscription_required,\n        defer_option=defer_option,\n        event_handlers=event_handlers,\n    )\n    self.endpoints[name] = endpoint\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.unregister_endpoint","title":"unregister_endpoint","text":"<pre><code>unregister_endpoint(name)\n</code></pre> <p>Unregister an endpoint.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the endpoint to be unregistered.</p> <p> TYPE: <code>str</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def unregister_endpoint(self, name: str):\n    \"\"\"Unregister an endpoint.\n\n    Args:\n        name (str): The name of the endpoint to be unregistered.\n    \"\"\"\n    if name in self.endpoints:\n        del self.endpoints[name]\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.register_router","title":"register_router","text":"<pre><code>register_router(name, router)\n</code></pre> <p>Register a FastAPI router.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the router.</p> <p> TYPE: <code>str</code> </p> <code>router</code> <p>The instance of the APIRouter to be registered.</p> <p> TYPE: <code>APIRouter</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def register_router(self, name: str, router: APIRouter):\n    \"\"\"Register a FastAPI router.\n\n    Args:\n        name (str): The name of the router.\n        router (APIRouter): The instance of the APIRouter to be registered.\n    \"\"\"\n    self.routers[name] = router\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.unregister_router","title":"unregister_router","text":"<pre><code>unregister_router(name)\n</code></pre> <p>Unregister a FastAPI router.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the router to be unregistered.</p> <p> TYPE: <code>str</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def unregister_router(self, name: str):\n    \"\"\"Unregister a FastAPI router.\n\n    Args:\n        name (str): The name of the router to be unregistered.\n    \"\"\"\n    if name in self.routers:\n        del self.routers[name]\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.wait_for_deployment","title":"wait_for_deployment","text":"<pre><code>wait_for_deployment()\n</code></pre> <p>Wait for the deployment to complete.</p> Source code in <code>aana/sdk.py</code> <pre><code>def wait_for_deployment(self):  # noqa: C901\n    \"\"\"Wait for the deployment to complete.\"\"\"\n    consecutive_resource_unavailable = 0\n    # Number of consecutive checks before raising an resource unavailable error\n    resource_unavailable_threshold = 5\n\n    while True:\n        status = serve.status()\n        if all(\n            application.status == \"RUNNING\"\n            for application in status.applications.values()\n        ):\n            break\n        if any(\n            application.status == \"DEPLOY_FAILED\"\n            or application.status == \"UNHEALTHY\"\n            for application in status.applications.values()\n        ):\n            error_messages = []\n            for app_name, app_status in status.applications.items():\n                if (\n                    app_status.status == \"DEPLOY_FAILED\"\n                    or app_status.status == \"UNHEALTHY\"\n                ):\n                    for (\n                        deployment_name,\n                        deployment_status,\n                    ) in app_status.deployments.items():\n                        error_messages.append(\n                            f\"Error: {deployment_name} ({app_name}): {deployment_status.message}\"\n                        )\n            raise FailedDeployment(\"\\n\".join(error_messages))\n\n        gcs_address = ray.get_runtime_context().gcs_address\n        cluster_status = get_cluster_status(gcs_address)\n        demands = (\n            cluster_status.resource_demands.cluster_constraint_demand\n            + cluster_status.resource_demands.ray_task_actor_demand\n            + cluster_status.resource_demands.placement_group_demand\n        )\n\n        resource_unavailable = False\n        for demand in demands:\n            if isinstance(demand, ResourceDemand) and demand.bundles_by_count:\n                error_message = f\"Error: No available node types can fulfill resource request {demand.bundles_by_count[0].bundle}. \"\n                if \"GPU\" in demand.bundles_by_count[0].bundle:\n                    error_message += \"Might be due to insufficient or misconfigured CPU or GPU resources.\"\n                resource_unavailable = True\n            else:\n                error_message = f\"Error: {demand}\"\n                resource_unavailable = True\n\n        if resource_unavailable:\n            consecutive_resource_unavailable += 1\n            if consecutive_resource_unavailable &gt;= resource_unavailable_threshold:\n                raise InsufficientResources(error_message)\n        else:\n            consecutive_resource_unavailable = 0\n\n        time.sleep(1)  # Wait for 1 second before checking again\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.deploy","title":"deploy","text":"<pre><code>deploy(blocking=False, sequential=False)\n</code></pre> <p>Deploy the application with the registered endpoints and deployments.</p> PARAMETER DESCRIPTION <code>blocking</code> <p>If True, the function will block until interrupted. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sequential</code> <p>If True, the deployments will be deployed sequentially. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def deploy(self, blocking: bool = False, sequential: bool = False):\n    \"\"\"Deploy the application with the registered endpoints and deployments.\n\n    Args:\n        blocking (bool, optional): If True, the function will block until interrupted. Defaults to False.\n        sequential (bool, optional): If True, the deployments will be deployed sequentially. Defaults to False.\n    \"\"\"\n    try:\n        for deployment_name in self.deployments:\n            serve.api._run(\n                self.get_deployment_app(deployment_name),\n                name=deployment_name,\n                route_prefix=f\"/{deployment_name}\",\n                _blocking=False,\n            )\n            if sequential:\n                self.wait_for_deployment()\n\n        serve.api._run(\n            self.get_main_app(),\n            name=self.name,\n            route_prefix=\"/\",\n            _blocking=False,  # blocking manually after to display the message \"Deployed successfully.\"\n        )\n\n        self.wait_for_deployment()\n\n        rprint(\"[green]Deployed successfully.[/green]\")\n        rprint(\n            f\"Documentation is available at \"\n            f\"[link=http://{self.host}:{self.port}/docs]http://{self.host}:{self.port}/docs[/link] and \"\n            f\"[link=http://{self.host}:{self.port}/redoc]http://{self.host}:{self.port}/redoc[/link]\"\n        )\n        while blocking:\n            time.sleep(10)\n    except KeyboardInterrupt:\n        print(\"Got KeyboardInterrupt, shutting down...\")\n        serve.shutdown()\n        sys.exit()\n    except DeploymentException as e:\n        status = serve.status()\n        serve.shutdown()\n        for app_name, app_status in status.applications.items():\n            if (\n                app_status.status == \"DEPLOY_FAILED\"\n                or app_status.status == \"UNHEALTHY\"\n            ):\n                self.print_app_status(app_name, app_status)\n        if isinstance(e, InsufficientResources):\n            rprint(f\"[red] {e} [/red]\")\n        raise\n    except Exception:\n        serve.shutdown()\n        traceback.print_exc()\n        print(\n            \"Received unexpected error, see console logs for more details. \"\n            \"Shutting down...\"\n        )\n        raise\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.shutdown","title":"shutdown","text":"<pre><code>shutdown()\n</code></pre> <p>Shutdown the Aana server.</p> Source code in <code>aana/sdk.py</code> <pre><code>def shutdown(self):\n    \"\"\"Shutdown the Aana server.\"\"\"\n    serve.shutdown()\n    ray.shutdown()\n</code></pre>"},{"location":"reference/sdk/#aana.sdk.AanaSDK.build","title":"build","text":"<pre><code>build(import_path, host='0.0.0.0', port=8000, app_config_name='app_config', config_name='config')\n</code></pre> <p>Build the application configuration file.</p> <p>Two files will be created: app_config (.py) and config (.yaml).s</p> PARAMETER DESCRIPTION <code>import_path</code> <p>The import path of the application.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>The host to run the application on. Defaults to \"0.0.0.0\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'0.0.0.0'</code> </p> <code>port</code> <p>The port to run the application on. Defaults to 8000.</p> <p> TYPE: <code>int</code> DEFAULT: <code>8000</code> </p> <code>app_config_name</code> <p>The name of the application config file. Defaults to \"app_config\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'app_config'</code> </p> <code>config_name</code> <p>The name of the config file. Defaults to \"config\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'config'</code> </p> Source code in <code>aana/sdk.py</code> <pre><code>def build(\n    self,\n    import_path: str,\n    host: str = \"0.0.0.0\",  # noqa: S104\n    port: int = 8000,\n    app_config_name: str = \"app_config\",\n    config_name: str = \"config\",\n):\n    \"\"\"Build the application configuration file.\n\n    Two files will be created: app_config (.py) and config (.yaml).s\n\n    Args:\n        import_path (str): The import path of the application.\n        host (str): The host to run the application on. Defaults to \"0.0.0.0\".\n        port (int): The port to run the application on. Defaults to 8000.\n        app_config_name (str): The name of the application config file. Defaults to \"app_config\".\n        config_name (str): The name of the config file. Defaults to \"config\".\n    \"\"\"\n    # Split the import path into module and variable.\n    # For example, aana.projects.whisper.app:aana_app will be split into\n    # module \"aana.projects.whisper.app\" and variable \"aana_app\".\n    app_module, app_var = import_path.split(\":\")\n\n    # Use location of the app module as the output directory\n    output_dir = Path(importlib.util.find_spec(app_module).origin).parent\n\n    # Import AanaSDK app from the given import path\n    aana_app = import_from_path(import_path)\n    if not isinstance(aana_app, AanaSDK):\n        raise TypeError(  # noqa: TRY003\n            f\"Error: {import_path} is not an AanaSDK instance, got {type(aana_app)}\"\n        )\n\n    # Generate the app config file\n    # Example:\n    #    from aana.projects.whisper.app import aana_app\n    #    asr_deployment = aana_app.get_deployment_app(\"asr_deployment\")\n    #    vad_deployment = aana_app.get_deployment_app(\"vad_deployment\")\n    #    whisper_app = aana_app.get_main_app()\n    app_config = \"\"\n    app_config += f\"from {app_module} import {app_var}\\n\\n\"\n    for deployment_name in aana_app.deployments:\n        app_config += f\"{deployment_name} = {app_var}.get_deployment_app('{deployment_name}')\\n\"\n    app_config += f\"{aana_app.name} = {app_var}.get_main_app()\\n\"\n\n    # Output path for the app config file\n    app_config_path = output_dir / f\"{app_config_name}.py\"\n    # Import path for the app config file, for example aana.projects.whisper.app_config\n    app_config_import_path = f\"{app_module.rsplit('.', 1)[0]}.{app_config_name}\"\n\n    # Write the app config file\n    with app_config_path.open(\"w\") as f:\n        f.write(app_config)\n\n    # Build \"serve build\" command to generate config.yaml\n    # For example,\n    # serve build aana.projects.whisper.app_config:vad_deployment\n    #             aana.projects.whisper.app_config:asr_deployment\n    #             aana.projects.whisper.app_config:whisper_app\n    #             -o /workspaces/aana_sdk/aana/projects/whisper/config.yaml\n    config_path = output_dir / f\"{config_name}.yaml\"\n    serve_options = []\n    for deployment_name in aana_app.deployments:\n        serve_options.append(f\"{app_config_import_path}:{deployment_name}\")  # noqa: PERF401\n    serve_options += [\n        f\"{app_config_import_path}:{aana_app.name}\",\n        \"--output-path\",\n        str(config_path),\n        \"--app-dir\",\n        output_dir,\n    ]\n\n    # Execute \"serve build\" with click CliRuuner\n    from click.testing import CliRunner\n    from ray.serve.scripts import ServeDeploySchemaDumper, build\n\n    result = CliRunner().invoke(build, serve_options)\n    if result.exception:\n        raise result.exception\n\n    # Update the config file with the host and port and rename apps\n    with config_path.open() as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)  # noqa: S506\n\n    config[\"http_options\"] = {\"host\": host, \"port\": port}\n\n    for app in config[\"applications\"]:\n        app[\"name\"] = app[\"import_path\"].split(\":\")[-1]\n\n    with config_path.open(\"w\") as f:\n        yaml.dump(\n            config,\n            f,\n            Dumper=ServeDeploySchemaDumper,\n            default_flow_style=False,\n            sort_keys=False,\n        )\n\n    print(f\"App config successfully saved to {app_config_path}\")\n    print(f\"Config successfully saved to {config_path}\")\n</code></pre>"},{"location":"reference/settings/","title":"Settings","text":""},{"location":"reference/settings/#aana.configs","title":"aana.configs","text":""},{"location":"reference/settings/#aana.configs.DbSettings","title":"DbSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Database configuration.</p> ATTRIBUTE DESCRIPTION <code>datastore_type</code> <p>The type of the datastore. Default is DbType.SQLITE.</p> <p> TYPE: <code>DbType | str</code> </p> <code>datastore_config</code> <p>The configuration for the datastore. Default is SQLiteConfig(path=\"/var/lib/aana_data\").</p> <p> TYPE: <code>SQLiteConfig | PostgreSQLConfig</code> </p> <code>pool_size</code> <p>The number of connections to keep in the pool. Default is 5.</p> <p> TYPE: <code>int</code> </p> <code>max_overflow</code> <p>The number of connections that can be created when the pool is exhausted. Default is 10.</p> <p> TYPE: <code>int</code> </p> <code>pool_recycle</code> <p>The number of seconds a connection can be idle in the pool before it is invalidated. Default is 3600.</p> <p> TYPE: <code>int</code> </p>"},{"location":"reference/settings/#aana.configs.DbSettings.get_engine","title":"get_engine","text":"<pre><code>get_engine()\n</code></pre> <p>Gets engine. Each instance of DbSettings will create a max.of 1 engine.</p> Source code in <code>aana/configs/db.py</code> <pre><code>def get_engine(self):\n    \"\"\"Gets engine. Each instance of DbSettings will create a max.of 1 engine.\"\"\"\n    if not self._engine:\n        self._engine = create_database_engine(self)\n    return self._engine\n</code></pre>"},{"location":"reference/settings/#aana.configs.PostgreSQLConfig","title":"PostgreSQLConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Config values for PostgreSQL.</p> ATTRIBUTE DESCRIPTION <code>host</code> <p>The host of the PostgreSQL server.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>The port of the PostgreSQL server.</p> <p> TYPE: <code>int</code> </p> <code>user</code> <p>The user to connect to the PostgreSQL server.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>The password to connect to the PostgreSQL server.</p> <p> TYPE: <code>str</code> </p> <code>database</code> <p>The database name.</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/settings/#aana.configs.SQLiteConfig","title":"SQLiteConfig","text":"<p>               Bases: <code>TypedDict</code></p> <p>Config values for SQLite.</p> ATTRIBUTE DESCRIPTION <code>path</code> <p>The path to the SQLite database file.</p> <p> TYPE: <code>PathLike</code> </p>"},{"location":"reference/settings/#aana.configs.Settings","title":"Settings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>A pydantic model for SDK settings.</p> ATTRIBUTE DESCRIPTION <code>tmp_data_dir</code> <p>The temporary data directory.</p> <p> TYPE: <code>Path</code> </p> <code>image_dir</code> <p>The temporary image directory.</p> <p> TYPE: <code>Path</code> </p> <code>video_dir</code> <p>The temporary video directory.</p> <p> TYPE: <code>Path</code> </p> <code>audio_dir</code> <p>The temporary audio directory.</p> <p> TYPE: <code>Path</code> </p> <code>model_dir</code> <p>The temporary model directory.</p> <p> TYPE: <code>Path</code> </p> <code>num_workers</code> <p>The number of web workers.</p> <p> TYPE: <code>int</code> </p> <code>openai_endpoint_enabled</code> <p>Flag indicating if the OpenAI-compatible endpoint is enabled. Enabled by default.</p> <p> TYPE: <code>bool</code> </p> <code>include_stacktrace</code> <p>Flag indicating if stacktrace should be included in error messages. Enabled by default.</p> <p> TYPE: <code>bool</code> </p> <code>task_queue</code> <p>The task queue settings.</p> <p> TYPE: <code>TaskQueueSettings</code> </p> <code>db_config</code> <p>The database configuration.</p> <p> TYPE: <code>DbSettings</code> </p> <code>test</code> <p>The test settings.</p> <p> TYPE: <code>TestSettings</code> </p>"},{"location":"reference/settings/#aana.configs.Settings.setup_resource_directories","title":"setup_resource_directories","text":"<pre><code>setup_resource_directories()\n</code></pre> <p>Create the resource directories if they do not exist.</p> Source code in <code>aana/configs/settings.py</code> <pre><code>@model_validator(mode=\"after\")\ndef setup_resource_directories(self):\n    \"\"\"Create the resource directories if they do not exist.\"\"\"\n    if self.image_dir is None:\n        self.image_dir = self.tmp_data_dir / \"images\"\n    if self.video_dir is None:\n        self.video_dir = self.tmp_data_dir / \"videos\"\n    if self.audio_dir is None:\n        self.audio_dir = self.tmp_data_dir / \"audios\"\n    if self.model_dir is None:\n        self.model_dir = self.tmp_data_dir / \"models\"\n\n    self.tmp_data_dir.mkdir(parents=True, exist_ok=True)\n    self.image_dir.mkdir(parents=True, exist_ok=True)\n    self.video_dir.mkdir(parents=True, exist_ok=True)\n    self.audio_dir.mkdir(parents=True, exist_ok=True)\n    self.model_dir.mkdir(parents=True, exist_ok=True)\n    return self\n</code></pre>"},{"location":"reference/settings/#aana.configs.TaskQueueSettings","title":"TaskQueueSettings","text":"<p>               Bases: <code>BaseModel</code></p> <p>A pydantic model for task queue settings.</p> ATTRIBUTE DESCRIPTION <code>enabled</code> <p>Flag indicating if the task queue is enabled.</p> <p> TYPE: <code>bool</code> </p> <code>num_workers</code> <p>The number of workers in the task queue.</p> <p> TYPE: <code>int</code> </p> <code>execution_timeout</code> <p>The maximum execution time for a task in seconds. After this time, if the task is still running, it will be considered as stuck and will be reassign to another worker.</p> <p> TYPE: <code>int</code> </p> <code>heartbeat_timeout</code> <p>The maximum time between heartbeats in seconds.</p> <p> TYPE: <code>int</code> </p> <code>max_retries</code> <p>The maximum number of retries for a task.</p> <p> TYPE: <code>int</code> </p> <code>maximum_active_tasks_per_user</code> <p>The maximum number of active tasks per user (only applicable in the API service).</p> <p> TYPE: <code>int</code> </p>"},{"location":"reference/settings/#aana.configs.TestSettings","title":"TestSettings","text":"<p>               Bases: <code>BaseModel</code></p> <p>A pydantic model for test settings.</p> ATTRIBUTE DESCRIPTION <code>test_mode</code> <p>Flag indicating if the SDK is in test mode.</p> <p> TYPE: <code>bool</code> </p> <code>save_expected_output</code> <p>Flag indicating if the expected output should be saved (to create test cases).</p> <p> TYPE: <code>bool</code> </p>"},{"location":"reference/settings/#aana.configs.DbType","title":"DbType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Engine types for relational database.</p> ATTRIBUTE DESCRIPTION <code>POSTGRESQL</code> <p>PostgreSQL database.</p> <p> </p> <code>SQLITE</code> <p>SQLite database.</p> <p> </p>"},{"location":"reference/utils/","title":"Utility functions","text":""},{"location":"reference/utils/#aana.utils.asyncio","title":"aana.utils.asyncio","text":""},{"location":"reference/utils/#aana.utils.asyncio.run_async","title":"run_async","text":"<pre><code>run_async(coro)\n</code></pre> <p>Run a coroutine in a thread if the current thread is running an event loop.</p> <p>Otherwise, run the coroutine in the current asyncio loop.</p> <p>Useful when you want to run an async function in a non-async context.</p> <p>From: https://stackoverflow.com/a/75094151</p> PARAMETER DESCRIPTION <code>coro</code> <p>The coroutine to run.</p> <p> TYPE: <code>Coroutine</code> </p> RETURNS DESCRIPTION <code>T</code> <p>The result of the coroutine.</p> <p> TYPE: <code>T</code> </p> Source code in <code>aana/utils/asyncio.py</code> <pre><code>def run_async(coro: Coroutine[Any, Any, T]) -&gt; T:\n    \"\"\"Run a coroutine in a thread if the current thread is running an event loop.\n\n    Otherwise, run the coroutine in the current asyncio loop.\n\n    Useful when you want to run an async function in a non-async context.\n\n    From: https://stackoverflow.com/a/75094151\n\n    Args:\n        coro (Coroutine): The coroutine to run.\n\n    Returns:\n        T: The result of the coroutine.\n    \"\"\"\n\n    class RunThread(threading.Thread):\n        \"\"\"Run a coroutine in a thread.\"\"\"\n\n        def __init__(self, coro: Coroutine[Any, Any, T]):\n            \"\"\"Initialize the thread.\"\"\"\n            self.coro = coro\n            self.result: T | None = None\n            self.exception: Exception | None = None\n            super().__init__()\n\n        def run(self):\n            \"\"\"Run the coroutine.\"\"\"\n            try:\n                self.result = asyncio.run(self.coro)\n            except Exception as e:\n                self.exception = e\n\n    try:\n        loop = asyncio.get_running_loop()\n    except RuntimeError:\n        loop = None\n\n    if loop and loop.is_running():\n        thread = RunThread(coro)\n        thread.start()\n        thread.join()\n        if thread.exception:\n            raise thread.exception\n        return thread.result\n    else:\n        return asyncio.run(coro)\n</code></pre>"},{"location":"reference/utils/#aana.utils.json","title":"aana.utils.json","text":""},{"location":"reference/utils/#aana.utils.json.json_serializer_default","title":"json_serializer_default","text":"<pre><code>json_serializer_default(obj)\n</code></pre> <p>Default function for json serializer to handle custom objects.</p> <p>If json serializer does not know how to serialize an object, it calls the default function.</p> <p>For example, if we see that the object is a pydantic model, we call the dict method to get the dictionary representation of the model that json serializer can deal with.</p> <p>If the object is not supported, we raise a TypeError.</p> PARAMETER DESCRIPTION <code>obj</code> <p>The object to serialize.</p> <p> TYPE: <code>object</code> </p> RETURNS DESCRIPTION <code>object</code> <p>The serializable object.</p> <p> TYPE: <code>object</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If the object is not a pydantic model, Path, or Media object.</p> Source code in <code>aana/utils/json.py</code> <pre><code>def json_serializer_default(obj: object) -&gt; object:\n    \"\"\"Default function for json serializer to handle custom objects.\n\n    If json serializer does not know how to serialize an object, it calls the default function.\n\n    For example, if we see that the object is a pydantic model,\n    we call the dict method to get the dictionary representation of the model\n    that json serializer can deal with.\n\n    If the object is not supported, we raise a TypeError.\n\n    Args:\n        obj (object): The object to serialize.\n\n    Returns:\n        object: The serializable object.\n\n    Raises:\n        TypeError: If the object is not a pydantic model, Path, or Media object.\n    \"\"\"\n    if isinstance(obj, Engine):\n        return None\n    if isinstance(obj, BaseModel):\n        return obj.model_dump()\n    if isinstance(obj, Path):\n        return str(obj)\n    if isinstance(obj, type):\n        return str(type)\n    if isinstance(obj, bytes):\n        return obj.decode()\n\n    from aana.core.models.media import Media\n\n    if isinstance(obj, Media):\n        return str(obj)\n\n    raise TypeError(type(obj))\n</code></pre>"},{"location":"reference/utils/#aana.utils.json.jsonify","title":"jsonify","text":"<pre><code>jsonify(data, option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_SORT_KEYS, as_bytes=False)\n</code></pre> <p>Serialize content using orjson.</p> PARAMETER DESCRIPTION <code>data</code> <p>The content to serialize.</p> <p> TYPE: <code>Any</code> </p> <code>option</code> <p>The option for orjson.dumps.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>OPT_SERIALIZE_NUMPY | OPT_SORT_KEYS</code> </p> <code>as_bytes</code> <p>Return output as bytes instead of string</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>str | bytes</code> <p>bytes | str: The serialized data as desired format.</p> Source code in <code>aana/utils/json.py</code> <pre><code>def jsonify(\n    data: Any,\n    option: int | None = orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_SORT_KEYS,\n    as_bytes: bool = False,\n) -&gt; str | bytes:\n    \"\"\"Serialize content using orjson.\n\n    Args:\n        data (Any): The content to serialize.\n        option (int | None): The option for orjson.dumps.\n        as_bytes (bool): Return output as bytes instead of string\n\n    Returns:\n        bytes | str: The serialized data as desired format.\n    \"\"\"\n    output = orjson.dumps(data, option=option, default=json_serializer_default)\n    return output if as_bytes else output.decode()\n</code></pre>"},{"location":"reference/utils/#aana.utils.download","title":"aana.utils.download","text":""},{"location":"reference/utils/#aana.utils.download.download_model","title":"download_model","text":"<pre><code>download_model(url, model_hash='', model_path=None, check_sum=True)\n</code></pre> <p>Download a model from a URL.</p> PARAMETER DESCRIPTION <code>url</code> <p>the URL of the file to download</p> <p> TYPE: <code>str</code> </p> <code>model_hash</code> <p>hash of the model file for checking sha256 hash if checksum is True</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>model_path</code> <p>optional model path where it needs to be downloaded</p> <p> TYPE: <code>Path</code> DEFAULT: <code>None</code> </p> <code>check_sum</code> <p>boolean to mention whether to check SHA-256 sum or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>the downloaded file path</p> <p> TYPE: <code>Path</code> </p> RAISES DESCRIPTION <code>DownloadException</code> <p>Request does not succeed.</p> Source code in <code>aana/utils/download.py</code> <pre><code>def download_model(\n    url: str, model_hash: str = \"\", model_path: Path | None = None, check_sum=True\n) -&gt; Path:\n    \"\"\"Download a model from a URL.\n\n    Args:\n        url (str): the URL of the file to download\n        model_hash (str): hash of the model file for checking sha256 hash if checksum is True\n        model_path (Path): optional model path where it needs to be downloaded\n        check_sum (bool): boolean to mention whether to check SHA-256 sum or not\n\n    Returns:\n        Path: the downloaded file path\n\n    Raises:\n        DownloadException: Request does not succeed.\n    \"\"\"\n    if model_path is None:\n        model_dir = settings.model_dir\n        if not model_dir.exists():\n            model_dir.mkdir(parents=True)\n        model_path = model_dir / \"model.bin\"\n\n    if model_path.exists() and not model_path.is_file():\n        raise RuntimeError(f\"Not a regular file: {model_path}\")  # noqa: TRY003\n\n    if not model_path.exists():\n        try:\n            with ExitStack() as stack:\n                source = stack.enter_context(urllib.request.urlopen(url))  # noqa: S310\n                output = stack.enter_context(Path.open(model_path, \"wb\"))\n\n                loop = tqdm(\n                    total=int(source.info().get(\"Content-Length\")),\n                    ncols=80,\n                    unit=\"iB\",\n                    unit_scale=True,\n                    unit_divisor=1024,\n                )\n\n                with loop:\n                    while True:\n                        buffer = source.read(8192)\n                        if not buffer:\n                            break\n\n                        output.write(buffer)\n                        loop.update(len(buffer))\n        except Exception as e:\n            raise DownloadException(url) from e\n\n    model_sha256_hash = get_sha256_hash_file(model_path)\n    if check_sum and model_sha256_hash != model_hash:\n        checksum_error = \"Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.\"\n        raise RuntimeError(f\"{checksum_error}\")\n\n    return model_path\n</code></pre>"},{"location":"reference/utils/#aana.utils.download.download_file","title":"download_file","text":"<pre><code>download_file(url)\n</code></pre> <p>Download a file from a URL.</p> PARAMETER DESCRIPTION <code>url</code> <p>the URL of the file to download</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bytes</code> <p>the file content</p> <p> TYPE: <code>bytes</code> </p> RAISES DESCRIPTION <code>DownloadException</code> <p>Request does not succeed.</p> Source code in <code>aana/utils/download.py</code> <pre><code>def download_file(url: str) -&gt; bytes:\n    \"\"\"Download a file from a URL.\n\n    Args:\n        url (str): the URL of the file to download\n\n    Returns:\n        bytes: the file content\n\n    Raises:\n        DownloadException: Request does not succeed.\n    \"\"\"\n    # TODO: add retries, check status code, etc.: add issue link\n    try:\n        response = requests.get(url)  # noqa: S113 TODO : add issue link\n    except Exception as e:\n        raise DownloadException(url) from e\n    return response.content\n</code></pre>"},{"location":"reference/utils/#aana.utils.gpu","title":"aana.utils.gpu","text":""},{"location":"reference/utils/#aana.utils.gpu.get_gpu_memory","title":"get_gpu_memory","text":"<pre><code>get_gpu_memory(gpu=0)\n</code></pre> <p>Get the total memory of a GPU in bytes.</p> PARAMETER DESCRIPTION <code>gpu</code> <p>the GPU index. Defaults to 0.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>int</code> <p>the total memory of the GPU in bytes</p> <p> TYPE: <code>int</code> </p> Source code in <code>aana/utils/gpu.py</code> <pre><code>def get_gpu_memory(gpu: int = 0) -&gt; int:\n    \"\"\"Get the total memory of a GPU in bytes.\n\n    Args:\n        gpu (int): the GPU index. Defaults to 0.\n\n    Returns:\n        int: the total memory of the GPU in bytes\n    \"\"\"\n    import torch\n\n    return torch.cuda.get_device_properties(gpu).total_memory\n</code></pre>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#media-models","title":"Media Models","text":"<p>Aana SDK provides models for such media types as audio, video, and images. These models make it easy to work with media files, download them, convert from other formats, and more.</p> <ul> <li><code>aana.core.models.Audio</code></li> <li><code>aana.core.models.Video</code></li> <li><code>aana.core.models.Image</code></li> <li><code>aana.core.models.Media</code></li> </ul> <p>Don't use models above in the Endpoint definition (as request or response body) because they are not serializable to JSON. Instead, use the following models as input models and call <code>convert_input_to_object()</code> method to convert them to the appropriate media type.</p> <ul> <li><code>aana.core.models.VideoInput</code></li> <li><code>aana.core.models.ImageInput</code></li> <li><code>aana.core.models.VideoInputList</code></li> <li><code>aana.core.models.ImageInputList</code></li> </ul> <p>For example, in the following code snippet, the <code>ImageInput</code> model is used in the endpoint definition, and then it is converted to the <code>Image</code> object.</p> <pre><code>class ImageClassificationEndpoint(Endpoint):\n    async def run(self, image: ImageInput) -&gt; ImageClassificationOutput:\n        image_obj: Image = image.convert_input_to_object()\n        ...\n</code></pre>"},{"location":"reference/models/#automatic-speech-recognition-asr-models","title":"Automatic Speech Recognition (ASR) Models","text":"<p>Models for working with automatic speech recognition (ASR) models. These models represent the output of ASR model like whisper and represent the transcription, segments, and words etc.</p>"},{"location":"reference/models/#caption-models","title":"Caption Models","text":"<p>Models for working with captions. These models represent the output of image captioning models like BLIP 2.</p>"},{"location":"reference/models/#chat-models","title":"Chat Models","text":"<p>Models for working with chat models. These models represent the input and output of chat models and models for OpenAI-compatible API.</p>"},{"location":"reference/models/#image-chat-models","title":"Image Chat Models","text":"<p>Models for working with visual chat models. These models represent the input of chat models containing text and image input for describing visual content of vision-language model (VLM).</p>"},{"location":"reference/models/#custom-config","title":"Custom Config","text":"<p>Custom Config model can be used to pass arbitrary configuration to the deployment.</p>"},{"location":"reference/models/#sampling-models","title":"Sampling Models","text":"<p>Contains Sampling Parameters model which can be used to pass sampling parameters to the LLM models.</p>"},{"location":"reference/models/#time-models","title":"Time Models","text":"<p>Contains time models like TimeInterval.</p>"},{"location":"reference/models/#types-models","title":"Types Models","text":"<p>Contains types models like Dtype.</p>"},{"location":"reference/models/#vad-models","title":"VAD Models","text":"<p>Contains Voice Activity Detection (VAD) models like VadParams, VadSegment, and VadSegments.</p>"},{"location":"reference/models/#video-models","title":"Video Models","text":"<p>Contains video models like VideoMetadata, VideoStatus, and VideoParams.</p>"},{"location":"reference/models/#whisper-models","title":"Whisper Models","text":"<p>Contains models for working with whisper models like WhisperParams.</p>"},{"location":"reference/models/asr/","title":"ASR Models","text":""},{"location":"reference/models/asr/#aana.core.models.asr","title":"aana.core.models.asr","text":""},{"location":"reference/models/asr/#aana.core.models.asr.AsrWord","title":"AsrWord","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic schema for Word from ASR model.</p> ATTRIBUTE DESCRIPTION <code>word</code> <p>The word text.</p> <p> TYPE: <code>str</code> </p> <code>speaker</code> <p>Speaker label for the word.</p> <p> TYPE: <code>str | None</code> </p> <code>time_interval</code> <p>Time interval of the word.</p> <p> TYPE: <code>TimeInterval</code> </p> <code>alignment_confidence</code> <p>Alignment confidence of the word, &gt;= 0.0 and &lt;= 1.0.</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/models/asr/#aana.core.models.asr.AsrWord.from_whisper","title":"from_whisper","text":"<pre><code>from_whisper(whisper_word)\n</code></pre> <p>Convert WhisperWord to AsrWord.</p> PARAMETER DESCRIPTION <code>whisper_word</code> <p>The WhisperWord from faster-whisper.</p> <p> TYPE: <code>Word</code> </p> RETURNS DESCRIPTION <code>AsrWord</code> <p>The converted AsrWord.</p> <p> TYPE: <code>AsrWord</code> </p> Source code in <code>aana/core/models/asr.py</code> <pre><code>@classmethod\ndef from_whisper(cls, whisper_word: \"WhisperWord\") -&gt; \"AsrWord\":\n    \"\"\"Convert WhisperWord to AsrWord.\n\n    Args:\n        whisper_word (WhisperWord): The WhisperWord from faster-whisper.\n\n    Returns:\n        AsrWord: The converted AsrWord.\n    \"\"\"\n    return cls(\n        speaker=None,\n        word=whisper_word.word,\n        time_interval=TimeInterval(start=whisper_word.start, end=whisper_word.end),\n        alignment_confidence=whisper_word.probability,\n    )\n</code></pre>"},{"location":"reference/models/asr/#aana.core.models.asr.AsrSegment","title":"AsrSegment","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic schema for Segment from ASR model.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The text of the segment (transcript/translation).</p> <p> TYPE: <code>str</code> </p> <code>time_interval</code> <p>Time interval of the segment.</p> <p> TYPE: <code>TimeInterval</code> </p> <code>confidence</code> <p>Confidence of the segment.</p> <p> TYPE: <code>float | None</code> </p> <code>no_speech_confidence</code> <p>Chance of being a silence segment.</p> <p> TYPE: <code>float | None</code> </p> <code>words</code> <p>List of words in the segment. Default is [].</p> <p> TYPE: <code>list[AsrWord]</code> </p> <code>speaker</code> <p>Speaker label. Default is None.</p> <p> TYPE: <code>str | None</code> </p>"},{"location":"reference/models/asr/#aana.core.models.asr.AsrSegment.from_whisper","title":"from_whisper","text":"<pre><code>from_whisper(whisper_segment)\n</code></pre> <p>Convert WhisperSegment to AsrSegment.</p> Source code in <code>aana/core/models/asr.py</code> <pre><code>@classmethod\ndef from_whisper(cls, whisper_segment: \"WhisperSegment\") -&gt; \"AsrSegment\":\n    \"\"\"Convert WhisperSegment to AsrSegment.\"\"\"\n    time_interval = TimeInterval(\n        start=whisper_segment.start, end=whisper_segment.end\n    )\n    try:\n        avg_logprob = whisper_segment.avg_logprob\n        confidence = np.exp(avg_logprob)\n    except AttributeError:\n        confidence = None\n\n    try:\n        words = [AsrWord.from_whisper(word) for word in whisper_segment.words]\n    except TypeError:  # \"None type object is not iterable\"\n        words = []\n    except AttributeError:  # \"'StreamSegment' object has no attribute 'words'\"\n        words = []\n    try:\n        no_speech_confidence = whisper_segment.no_speech_prob\n    except AttributeError:\n        no_speech_confidence = None\n\n    return cls(\n        text=whisper_segment.text,\n        time_interval=time_interval,\n        confidence=confidence,\n        no_speech_confidence=no_speech_confidence,\n        words=words,\n        speaker=None,\n    )\n</code></pre>"},{"location":"reference/models/asr/#aana.core.models.asr.AsrTranscriptionInfo","title":"AsrTranscriptionInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic schema for TranscriptionInfo.</p> ATTRIBUTE DESCRIPTION <code>language</code> <p>Language of the transcription.</p> <p> TYPE: <code>str</code> </p> <code>language_confidence</code> <p>Confidence of the language detection, &gt;= 0.0 and &lt;= 1.0. Default is 0.0.</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/models/asr/#aana.core.models.asr.AsrTranscriptionInfo.from_whisper","title":"from_whisper","text":"<pre><code>from_whisper(transcription_info)\n</code></pre> <p>Convert WhisperTranscriptionInfo to AsrTranscriptionInfo.</p> PARAMETER DESCRIPTION <code>transcription_info</code> <p>The WhisperTranscriptionInfo from faster-whisper.</p> <p> TYPE: <code>TranscriptionInfo</code> </p> RETURNS DESCRIPTION <code>AsrTranscriptionInfo</code> <p>The converted AsrTranscriptionInfo.</p> <p> TYPE: <code>AsrTranscriptionInfo</code> </p> Source code in <code>aana/core/models/asr.py</code> <pre><code>@classmethod\ndef from_whisper(\n    cls, transcription_info: \"WhisperTranscriptionInfo\"\n) -&gt; \"AsrTranscriptionInfo\":\n    \"\"\"Convert WhisperTranscriptionInfo to AsrTranscriptionInfo.\n\n    Args:\n        transcription_info (WhisperTranscriptionInfo): The WhisperTranscriptionInfo from faster-whisper.\n\n    Returns:\n        AsrTranscriptionInfo: The converted AsrTranscriptionInfo.\n    \"\"\"\n    return cls(\n        language=transcription_info.language,\n        language_confidence=transcription_info.language_probability,\n    )\n</code></pre>"},{"location":"reference/models/asr/#aana.core.models.asr.AsrTranscription","title":"AsrTranscription","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic schema for Transcription/Translation.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The text of the transcription/translation. Default is \"\".</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/models/captions/","title":"Caption Models","text":""},{"location":"reference/models/captions/#aana.core.models.captions","title":"aana.core.models.captions","text":""},{"location":"reference/models/captions/#aana.core.models.captions.Caption","title":"Caption","text":"<pre><code>Caption = str\n</code></pre> <p>A caption.</p>"},{"location":"reference/models/captions/#aana.core.models.captions.CaptionsList","title":"CaptionsList","text":"<pre><code>CaptionsList = list[Caption]\n</code></pre> <p>A list of captions.</p>"},{"location":"reference/models/chat/","title":"Chat Models","text":""},{"location":"reference/models/chat/#aana.core.models.chat","title":"aana.core.models.chat","text":""},{"location":"reference/models/chat/#aana.core.models.chat.Role","title":"Role","text":"<pre><code>Role = Literal['system', 'user', 'assistant']\n</code></pre> <p>The role of a participant in a conversation.</p> <ul> <li>\"system\": Used for instructions or context provided to the model.</li> <li>\"user\": Represents messages from the user.</li> <li>\"assistant\": Represents LLM responses.</li> </ul>"},{"location":"reference/models/chat/#aana.core.models.chat.Prompt","title":"Prompt","text":"<pre><code>Prompt = str\n</code></pre> <p>The prompt for the LLM.</p>"},{"location":"reference/models/chat/#aana.core.models.chat.Question","title":"Question","text":"<pre><code>Question = str\n</code></pre> <p>The question.</p>"},{"location":"reference/models/chat/#aana.core.models.chat.ChatMessage","title":"ChatMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>A chat message.</p> ATTRIBUTE DESCRIPTION <code>content</code> <p>the text of the message</p> <p> TYPE: <code>str</code> </p> <code>role</code> <p>the role of the message</p> <p> TYPE: <code>Role</code> </p>"},{"location":"reference/models/chat/#aana.core.models.chat.ChatDialog","title":"ChatDialog","text":"<p>               Bases: <code>BaseModel</code></p> <p>A chat dialog.</p> ATTRIBUTE DESCRIPTION <code>messages</code> <p>the messages in the dialog</p> <p> TYPE: <code>list[ChatMessage]</code> </p>"},{"location":"reference/models/chat/#aana.core.models.chat.ChatDialog.from_list","title":"from_list","text":"<pre><code>from_list(messages)\n</code></pre> <p>Create a ChatDialog from a list of dictionaries.</p> PARAMETER DESCRIPTION <code>messages</code> <p>the list of messages</p> <p> TYPE: <code>list[dict[str, str]]</code> </p> RETURNS DESCRIPTION <code>ChatDialog</code> <p>the chat dialog</p> <p> TYPE: <code>ChatDialog</code> </p> Source code in <code>aana/core/models/chat.py</code> <pre><code>@classmethod\ndef from_list(cls, messages: list[dict[str, str]]) -&gt; \"ChatDialog\":\n    \"\"\"Create a ChatDialog from a list of dictionaries.\n\n    Args:\n        messages (list[dict[str, str]]): the list of messages\n\n    Returns:\n        ChatDialog: the chat dialog\n    \"\"\"\n    return ChatDialog(messages=[ChatMessage(**message) for message in messages])\n</code></pre>"},{"location":"reference/models/chat/#aana.core.models.chat.ChatDialog.from_prompt","title":"from_prompt","text":"<pre><code>from_prompt(prompt)\n</code></pre> <p>Create a ChatDialog from a prompt.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>the prompt</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>ChatDialog</code> <p>the chat dialog</p> <p> TYPE: <code>ChatDialog</code> </p> Source code in <code>aana/core/models/chat.py</code> <pre><code>@classmethod\ndef from_prompt(cls, prompt: str) -&gt; \"ChatDialog\":\n    \"\"\"Create a ChatDialog from a prompt.\n\n    Args:\n        prompt (str): the prompt\n\n    Returns:\n        ChatDialog: the chat dialog\n    \"\"\"\n    return ChatDialog(messages=[ChatMessage(content=prompt, role=\"user\")])\n</code></pre>"},{"location":"reference/models/chat/#aana.core.models.chat.ChatCompletionRequest","title":"ChatCompletionRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>A chat completion request for OpenAI compatible API.</p> ATTRIBUTE DESCRIPTION <code>model</code> <p>the model name (name of the LLM deployment)</p> <p> TYPE: <code>str</code> </p> <code>messages</code> <p>a list of messages comprising the conversation so far</p> <p> TYPE: <code>list[ChatMessage]</code> </p> <code>temperature</code> <p>float that controls the randomness of the sampling</p> <p> TYPE: <code>float</code> </p> <code>top_p</code> <p>float that controls the cumulative probability of the top tokens to consider</p> <p> TYPE: <code>float</code> </p> <code>max_tokens</code> <p>the maximum number of tokens to generate</p> <p> TYPE: <code>int</code> </p> <code>repetition_penalty</code> <p>float that penalizes new tokens based on whether they appear in the prompt and the generated text so far</p> <p> TYPE: <code>float</code> </p> <code>stream</code> <p>if set, partial message deltas will be sent</p> <p> TYPE: <code>bool</code> </p>"},{"location":"reference/models/chat/#aana.core.models.chat.ChatCompletionChoice","title":"ChatCompletionChoice","text":"<p>               Bases: <code>BaseModel</code></p> <p>A chat completion choice for OpenAI compatible API.</p> ATTRIBUTE DESCRIPTION <code>index</code> <p>the index of the choice in the list of choices</p> <p> TYPE: <code>int</code> </p> <code>message</code> <p>a chat completion message generated by the model</p> <p> TYPE: <code>ChatMessage</code> </p>"},{"location":"reference/models/chat/#aana.core.models.chat.ChatCompletion","title":"ChatCompletion","text":"<p>               Bases: <code>BaseModel</code></p> <p>A chat completion for OpenAI compatible API.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>a unique identifier for the chat completion</p> <p> TYPE: <code>str</code> </p> <code>model</code> <p>the model used for the chat completion</p> <p> TYPE: <code>str</code> </p> <code>created</code> <p>the Unix timestamp (in seconds) of when the chat completion was created</p> <p> TYPE: <code>int</code> </p> <code>choices</code> <p>a list of chat completion choices</p> <p> TYPE: <code>list[ChatCompletionChoice]</code> </p> <code>object</code> <p>the object type, which is always <code>chat.completion</code></p> <p> TYPE: <code>Literal['chat.completion']</code> </p>"},{"location":"reference/models/custom_config/","title":"Custom Config","text":""},{"location":"reference/models/custom_config/#aana.core.models.custom_config","title":"aana.core.models.custom_config","text":""},{"location":"reference/models/custom_config/#aana.core.models.custom_config.CustomConfig","title":"CustomConfig","text":"<pre><code>CustomConfig = dict\n</code></pre> <p>A custom configuration field that can be used to pass arbitrary configuration to the deployment.</p> <p>For example, you can define a custom configuration field in a deployment configuration like this:</p> <pre><code>class HfPipelineConfig(BaseModel):\n    model_id: str\n    task: str | None = None\n    model_kwargs: CustomConfig = {}\n    pipeline_kwargs: CustomConfig = {}\n    generation_kwargs: CustomConfig = {}\n</code></pre> <p>Then you can use the custom configuration field to pass a configuration to the deployment:</p> <pre><code>HfPipelineConfig(\n    model_id=\"Salesforce/blip2-opt-2.7b\",\n    model_kwargs={\n        \"quantization_config\": BitsAndBytesConfig(\n            load_in_8bit=False, load_in_4bit=True\n        ),\n    },\n)\n</code></pre>"},{"location":"reference/models/image_chat/","title":"Image Chat Models","text":""},{"location":"reference/models/image_chat/#aana.core.models.image_chat","title":"aana.core.models.image_chat","text":""},{"location":"reference/models/image_chat/#aana.core.models.image_chat.TextContent","title":"TextContent","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text content for a chat message.</p> ATTRIBUTE DESCRIPTION <code>type</code> <p>the type of the content, always \"text\"</p> <p> TYPE: <code>Literal['text']</code> </p> <code>text</code> <p>the text of the message</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/models/image_chat/#aana.core.models.image_chat.ImageContent","title":"ImageContent","text":"<p>               Bases: <code>BaseModel</code></p> <p>Image content for a chat message.</p> ATTRIBUTE DESCRIPTION <code>type</code> <p>the type of the content, always \"image\"</p> <p> TYPE: <code>Literal['image']</code> </p> <code>image</code> <p>the image</p> <p> TYPE: <code>Image</code> </p>"},{"location":"reference/models/image_chat/#aana.core.models.image_chat.ImageChatMessage","title":"ImageChatMessage","text":"<p>               Bases: <code>BaseModel</code></p> <p>A chat message with image support.</p> ATTRIBUTE DESCRIPTION <code>content</code> <p>the content of the message</p> <p> TYPE: <code>list[Content]</code> </p> <code>role</code> <p>the role of the message</p> <p> TYPE: <code>Role</code> </p>"},{"location":"reference/models/image_chat/#aana.core.models.image_chat.ImageChatDialog","title":"ImageChatDialog","text":"<p>               Bases: <code>BaseModel</code></p> <p>A chat dialog with image support.</p> ATTRIBUTE DESCRIPTION <code>messages</code> <p>the list of messages</p> <p> TYPE: <code>list[ImageChatMessage]</code> </p>"},{"location":"reference/models/image_chat/#aana.core.models.image_chat.ImageChatDialog.from_list","title":"from_list","text":"<pre><code>from_list(messages)\n</code></pre> <p>Create an ImageChatDialog from a list of messages.</p> PARAMETER DESCRIPTION <code>messages</code> <p>the list of messages</p> <p> TYPE: <code>list[dict[str, str]]</code> </p> RETURNS DESCRIPTION <code>ImageChatDialog</code> <p>the chat dialog</p> <p> TYPE: <code>ImageChatDialog</code> </p> <p>Example: <pre><code>messages = [\n    {\n        \"content\": [\n            { \"type\": \"image\", \"image\": Image(...) },\n            { \"type\": \"text\", \"text\": \"...\" }\n        ],\n        \"role\": \"system\"\n    },\n    {\n        \"content\": [\n            { \"type\": \"image\", \"image\": Image(...) },\n            { \"type\": \"text\", \"text\": \"...\" }\n        ],\n        \"role\": \"user\"\n    }\n]\ndialog = ImageChatDialog.from_list(messages)\n</code></pre></p> Source code in <code>aana/core/models/image_chat.py</code> <pre><code>@classmethod\ndef from_list(cls, messages: list[dict[str, Any]]) -&gt; \"ImageChatDialog\":\n    \"\"\"Create an ImageChatDialog from a list of messages.\n\n    Args:\n        messages (list[dict[str, str]]): the list of messages\n\n    Returns:\n        ImageChatDialog: the chat dialog\n\n    Example:\n    ```\n    messages = [\n        {\n            \"content\": [\n                { \"type\": \"image\", \"image\": Image(...) },\n                { \"type\": \"text\", \"text\": \"...\" }\n            ],\n            \"role\": \"system\"\n        },\n        {\n            \"content\": [\n                { \"type\": \"image\", \"image\": Image(...) },\n                { \"type\": \"text\", \"text\": \"...\" }\n            ],\n            \"role\": \"user\"\n        }\n    ]\n    dialog = ImageChatDialog.from_list(messages)\n    ```\n    \"\"\"\n    return ImageChatDialog(\n        messages=[ImageChatMessage(**message) for message in messages]\n    )\n</code></pre>"},{"location":"reference/models/image_chat/#aana.core.models.image_chat.ImageChatDialog.from_prompt","title":"from_prompt","text":"<pre><code>from_prompt(prompt, images)\n</code></pre> <p>Create an ImageChatDialog from a prompt and a list of images.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>the prompt</p> <p> TYPE: <code>str</code> </p> <code>images</code> <p>the list of images</p> <p> TYPE: <code>list[Image]</code> </p> RETURNS DESCRIPTION <code>ImageChatDialog</code> <p>the chat dialog</p> <p> TYPE: <code>ImageChatDialog</code> </p> Source code in <code>aana/core/models/image_chat.py</code> <pre><code>@classmethod\ndef from_prompt(cls, prompt: str, images: list[Image]) -&gt; \"ImageChatDialog\":\n    \"\"\"Create an ImageChatDialog from a prompt and a list of images.\n\n    Args:\n        prompt (str): the prompt\n        images (list[Image]): the list of images\n\n    Returns:\n        ImageChatDialog: the chat dialog\n    \"\"\"\n    content: list[Content] = [ImageContent(image=image) for image in images]\n    content.append(TextContent(text=prompt))\n\n    return ImageChatDialog(\n        messages=[ImageChatMessage(content=content, role=\"user\")]\n    )\n</code></pre>"},{"location":"reference/models/image_chat/#aana.core.models.image_chat.ImageChatDialog.to_objects","title":"to_objects","text":"<pre><code>to_objects()\n</code></pre> <p>Convert ImageChatDialog to messages and images.</p> RETURNS DESCRIPTION <code>tuple[list[dict], list[Image]]</code> <p>tuple[list[dict], list[Image]]: the messages and the images</p> Source code in <code>aana/core/models/image_chat.py</code> <pre><code>def to_objects(self) -&gt; tuple[list[dict], list[Image]]:\n    \"\"\"Convert ImageChatDialog to messages and images.\n\n    Returns:\n        tuple[list[dict], list[Image]]: the messages and the images\n    \"\"\"\n    dialog_dict = self.model_dump(\n        exclude={\"messages\": {\"__all__\": {\"content\": {\"__all__\": {\"image\"}}}}}\n    )\n    messages = dialog_dict[\"messages\"]\n    # images = []\n    # for message in self.messages:\n    #     for content in message.content:\n    #         if content.type == \"image\":\n    #             images.append(content.image)\n    images = [content.image for message in self.messages for content in message.content if content.type == \"image\"]\n\n    return messages, images\n</code></pre>"},{"location":"reference/models/media/","title":"Media Models","text":"<p>The <code>aana.core.models</code> provides models for such media types as audio, video, and images.</p>"},{"location":"reference/models/media/#aana.core.models","title":"aana.core.models","text":""},{"location":"reference/models/sampling/","title":"Sampling Models","text":""},{"location":"reference/models/sampling/#aana.core.models.sampling","title":"aana.core.models.sampling","text":""},{"location":"reference/models/sampling/#aana.core.models.sampling.SamplingParams","title":"SamplingParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model for sampling parameters of LLM.</p> ATTRIBUTE DESCRIPTION <code>temperature</code> <p>Float that controls the randomness of the sampling. Lower values make the model more deterministic, while higher values make the model more random. Zero means greedy sampling.</p> <p> TYPE: <code>float</code> </p> <code>top_p</code> <p>Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1 to consider all tokens.</p> <p> TYPE: <code>float</code> </p> <code>top_k</code> <p>Integer that controls the number of top tokens to consider. Set to -1 to consider all tokens.</p> <p> TYPE: <code>int</code> </p> <code>max_tokens</code> <p>The maximum number of tokens to generate.</p> <p> TYPE: <code>int</code> </p> <code>repetition_penalty</code> <p>Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values &gt; 1 encourage the model to use new tokens, while values &lt; 1 encourage the model to repeat tokens. Default is 1.0 (no penalty).</p> <p> TYPE: <code>float</code> </p> <code>kwargs</code> <p>Extra keyword arguments to pass as sampling parameters.</p> <p> TYPE: <code>dict</code> </p>"},{"location":"reference/models/sampling/#aana.core.models.sampling.SamplingParams.check_top_k","title":"check_top_k","text":"<pre><code>check_top_k(v)\n</code></pre> <p>Validates a top_k argument.</p> <p>Makes sure it is either -1, or at least 1.</p> PARAMETER DESCRIPTION <code>v</code> <p>Value to validate.</p> <p> TYPE: <code>int</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>The value is not valid.</p> RETURNS DESCRIPTION <p>The top_k value.</p> Source code in <code>aana/core/models/sampling.py</code> <pre><code>@field_validator(\"top_k\")\ndef check_top_k(cls, v: int):\n    \"\"\"Validates a top_k argument.\n\n    Makes sure it is either -1, or at least 1.\n\n    Args:\n        v (int): Value to validate.\n\n    Raises:\n        ValueError: The value is not valid.\n\n    Returns:\n        The top_k value.\n    \"\"\"\n    if v is None:\n        return v\n    if v &lt; -1 or v == 0:\n        raise ValueError(f\"top_k must be -1 (disable), or at least 1, got {v}.\")  # noqa: TRY003\n    return v\n</code></pre>"},{"location":"reference/models/speaker/","title":"Speaker Models","text":""},{"location":"reference/models/speaker/#aana.core.models.speaker","title":"aana.core.models.speaker","text":""},{"location":"reference/models/speaker/#aana.core.models.speaker.SpeakerDiarizationSegments","title":"SpeakerDiarizationSegments","text":"<pre><code>SpeakerDiarizationSegments = list[SpeakerDiarizationSegment]\n</code></pre> <p>List of SpeakerDiarizationSegment objects.</p>"},{"location":"reference/models/speaker/#aana.core.models.speaker.PyannoteSpeakerDiarizationParams","title":"PyannoteSpeakerDiarizationParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model for the pyannote Speaker Diarization model parameters.</p> ATTRIBUTE DESCRIPTION <code>min_speakers</code> <p>The minimum number of speakers present in the audio.</p> <p> TYPE: <code>int | None</code> </p> <code>max_speakers</code> <p>The maximum number of speakers present in the audio.</p> <p> TYPE: <code>int | None</code> </p>"},{"location":"reference/models/speaker/#aana.core.models.speaker.SpeakerDiarizationSegment","title":"SpeakerDiarizationSegment","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic schema for Segment from Speaker Diarization model.</p> ATTRIBUTE DESCRIPTION <code>time_interval</code> <p>The start and end time of the segment</p> <p> TYPE: <code>TimeInterval</code> </p> <code>speaker</code> <p>speaker assignment of the model in the format \"SPEAKER_XX\"</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/models/speaker/#aana.core.models.speaker.SpeakerDiarizationSegment.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Generate dictionary with start, end and speaker keys from SpeakerDiarizationSegment.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary with start, end and speaker keys</p> <p> TYPE: <code>dict</code> </p> Source code in <code>aana/core/models/speaker.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Generate dictionary with start, end and speaker keys from SpeakerDiarizationSegment.\n\n    Returns:\n        dict: Dictionary with start, end and speaker keys\n    \"\"\"\n    return {\n        \"start\": self.time_interval.start,\n        \"end\": self.time_interval.end,\n        \"speaker\": self.speaker,\n    }\n</code></pre>"},{"location":"reference/models/time/","title":"Time Models","text":""},{"location":"reference/models/time/#aana.core.models.time","title":"aana.core.models.time","text":""},{"location":"reference/models/time/#aana.core.models.time.TimeInterval","title":"TimeInterval","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic schema for TimeInterval.</p> ATTRIBUTE DESCRIPTION <code>start</code> <p>Start time in seconds</p> <p> TYPE: <code>float</code> </p> <code>end</code> <p>End time in seconds</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/models/types/","title":"Types Models","text":""},{"location":"reference/models/types/#aana.core.models.types","title":"aana.core.models.types","text":""},{"location":"reference/models/types/#aana.core.models.types.Dtype","title":"Dtype","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Data types.</p> <p>Possible values are \"auto\", \"float32\", \"float16\", \"bfloat16\" and \"int8\".</p> ATTRIBUTE DESCRIPTION <code>AUTO</code> <p>auto</p> <p> TYPE: <code>str</code> </p> <code>FLOAT32</code> <p>float32</p> <p> TYPE: <code>str</code> </p> <code>FLOAT16</code> <p>float16</p> <p> TYPE: <code>str</code> </p> <code>BFLOAT16</code> <p>bfloat16</p> <p> TYPE: <code>str</code> </p> <code>INT8</code> <p>int8</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/models/types/#aana.core.models.types.Dtype.to_torch","title":"to_torch","text":"<pre><code>to_torch()\n</code></pre> <p>Convert the instance's dtype to a torch dtype.</p> RETURNS DESCRIPTION <code>dtype | str</code> <p>Union[torch.dtype, str]: the torch dtype or \"auto\"</p> RAISES DESCRIPTION <code>ValueError</code> <p>if the dtype is unknown</p> Source code in <code>aana/core/models/types.py</code> <pre><code>def to_torch(self) -&gt; torch.dtype | str:\n    \"\"\"Convert the instance's dtype to a torch dtype.\n\n    Returns:\n        Union[torch.dtype, str]: the torch dtype or \"auto\"\n\n    Raises:\n        ValueError: if the dtype is unknown\n    \"\"\"\n    match self.value:\n        case self.AUTO:\n            return \"auto\"\n        case self.FLOAT32:\n            return torch.float32\n        case self.FLOAT16:\n            return torch.float16\n        case self.BFLOAT16:\n            return torch.bfloat16\n        case self.INT8:\n            return torch.int8\n        case _:\n            raise ValueError(f\"Unknown dtype: {self}\")  # noqa: TRY003\n</code></pre>"},{"location":"reference/models/vad/","title":"VAD Models","text":""},{"location":"reference/models/vad/#aana.core.models.vad","title":"aana.core.models.vad","text":""},{"location":"reference/models/vad/#aana.core.models.vad.VadSegments","title":"VadSegments","text":"<pre><code>VadSegments = list[VadSegment]\n</code></pre> <p>List of VadSegment objects.</p>"},{"location":"reference/models/vad/#aana.core.models.vad.VadParams","title":"VadParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model for the Voice Activity Detection model parameters.</p> ATTRIBUTE DESCRIPTION <code>chunk_size</code> <p>The maximum length of each vad output chunk.</p> <p> TYPE: <code>float</code> </p> <code>merge_onset</code> <p>Onset to be used for the merging operation.</p> <p> TYPE: <code>float</code> </p> <code>merge_offset</code> <p>\"Optional offset to be used for the merging operation.</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/models/vad/#aana.core.models.vad.VadSegment","title":"VadSegment","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic schema for Segment from Voice Activity Detection model.</p> ATTRIBUTE DESCRIPTION <code>time_interval</code> <p>The start and end time of the segment</p> <p> TYPE: <code>TimeInterval</code> </p> <code>segments</code> <p>smaller voiced segments within a merged vad segment</p> <p> TYPE: <code>list[tuple[float, float]]</code> </p>"},{"location":"reference/models/vad/#aana.core.models.vad.VadSegment.to_whisper_dict","title":"to_whisper_dict","text":"<pre><code>to_whisper_dict()\n</code></pre> <p>Generate dictionary with start, end and segments keys from VADSegment for faster whisper.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary with start, end and segments keys</p> <p> TYPE: <code>dict</code> </p> Source code in <code>aana/core/models/vad.py</code> <pre><code>def to_whisper_dict(self) -&gt; dict:\n    \"\"\"Generate dictionary with start, end and segments keys from VADSegment for faster whisper.\n\n    Returns:\n        dict: Dictionary with start, end and segments keys\n    \"\"\"\n    return {\n        \"start\": self.time_interval.start,\n        \"end\": self.time_interval.end,\n        \"segments\": self.segments,\n    }\n</code></pre>"},{"location":"reference/models/video/","title":"Video Models","text":""},{"location":"reference/models/video/#aana.core.models.video","title":"aana.core.models.video","text":""},{"location":"reference/models/video/#aana.core.models.video.Video","title":"Video","text":"<pre><code>Video(path=None, url=None, content=None, media_id=lambda: str(uuid.uuid4())(), save_on_disk=True, is_saved=False, media_dir=settings.video_dir, title='', description='')\n</code></pre> <p>               Bases: <code>Media</code></p> <p>A class representing a video.</p> <p>At least one of <code>path</code>, <code>url</code>, or <code>content</code> must be provided. If <code>save_on_disk</code> is True, the video will be saved on disk automatically.</p> ATTRIBUTE DESCRIPTION <code>path</code> <p>the path to the video file</p> <p> TYPE: <code>Path</code> </p> <code>url</code> <p>the URL of the video</p> <p> TYPE: <code>str</code> </p> <code>content</code> <p>the content of the video in bytes</p> <p> TYPE: <code>bytes</code> </p> <code>media_id</code> <p>the ID of the video. If not provided, it will be generated automatically.</p> <p> TYPE: <code>MediaId</code> </p> <code>title</code> <p>the title of the video</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>the description of the video</p> <p> TYPE: <code>str</code> </p> <code>media_dir</code> <p>the directory to save the video in</p> <p> TYPE: <code>Path</code> </p>"},{"location":"reference/models/video/#aana.core.models.video.Video.save","title":"save","text":"<pre><code>save()\n</code></pre> <p>Save the media on disk.</p> <p>If the media is already available on disk, do nothing. If the media represented as a byte string, save it on disk If the media is represented as a URL, download it and save it on disk.</p> RAISES DESCRIPTION <code>ValueError</code> <p>if at least one of 'path', 'url', or 'content' is not provided</p> Source code in <code>aana/core/models/media.py</code> <pre><code>def save(self):\n    \"\"\"Save the media on disk.\n\n    If the media is already available on disk, do nothing.\n    If the media represented as a byte string, save it on disk\n    If the media is represented as a URL, download it and save it on disk.\n\n    Raises:\n        ValueError: if at least one of 'path', 'url', or 'content' is not provided\n    \"\"\"\n    if self.path:\n        return\n\n    if self.media_dir is None:\n        raise ValueError(  # noqa: TRY003\n            \"The 'media_dir' isn't defined for this media type.\"\n        )\n    self.media_dir.mkdir(parents=True, exist_ok=True)\n    file_path = self.media_dir / (self.media_id + \".mp4\")\n\n    if self.content:\n        self._save_from_content(file_path)\n    elif self.url:\n        self._save_from_url(file_path)\n    else:\n        raise ValueError(  # noqa: TRY003\n            \"At least one of 'path', 'url', or 'content' must be provided.\"\n        )\n    self.is_saved = True\n</code></pre>"},{"location":"reference/models/video/#aana.core.models.video.Video.get_content","title":"get_content","text":"<pre><code>get_content()\n</code></pre> <p>Get the content of the media as bytes.</p> RETURNS DESCRIPTION <code>bytes</code> <p>the content of the media</p> <p> TYPE: <code>bytes</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if at least one of 'path', 'url', or 'content' is not provided</p> Source code in <code>aana/core/models/media.py</code> <pre><code>def get_content(self) -&gt; bytes:\n    \"\"\"Get the content of the media as bytes.\n\n    Returns:\n        bytes: the content of the media\n\n    Raises:\n        ValueError: if at least one of 'path', 'url', or 'content' is not provided\n    \"\"\"\n    if self.content:\n        return self.content\n    elif self.path:\n        self._load_content_from_path()\n    elif self.url:\n        self._load_content_from_url()\n    else:\n        raise ValueError(  # noqa: TRY003\n            \"At least one of 'path', 'url', or 'content' must be provided.\"\n        )\n    assert self.content is not None  # noqa: S101\n    return self.content\n</code></pre>"},{"location":"reference/models/video/#aana.core.models.video.Video.cleanup","title":"cleanup","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup the media.</p> <p>If the media is saved on disk by the class, delete it.</p> Source code in <code>aana/core/models/media.py</code> <pre><code>def cleanup(self):\n    \"\"\"Cleanup the media.\n\n    If the media is saved on disk by the class, delete it.\n    \"\"\"\n    if self.is_saved and self.path:\n        self.path.unlink(missing_ok=True)\n</code></pre>"},{"location":"reference/models/video/#aana.core.models.video.Video.is_video","title":"is_video","text":"<pre><code>is_video()\n</code></pre> <p>Checks if it's a valid video.</p> Source code in <code>aana/core/models/video.py</code> <pre><code>def is_video(self) -&gt; bool:\n    \"\"\"Checks if it's a valid video.\"\"\"\n    if not self.path:\n        return False\n\n    try:\n        decord.VideoReader(str(self.path))\n    except DECORDError:\n        try:\n            decord.AudioReader(str(self.path))\n        except DECORDError:\n            return False\n    return True\n</code></pre>"},{"location":"reference/models/video/#aana.core.models.video.VideoMetadata","title":"VideoMetadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata of a video.</p> ATTRIBUTE DESCRIPTION <code>title</code> <p>the title of the video</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>the description of the video</p> <p> TYPE: <code>str</code> </p> <code>duration</code> <p>the duration of the video in seconds</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/models/video/#aana.core.models.video.VideoParams","title":"VideoParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>A pydantic model for video parameters.</p> ATTRIBUTE DESCRIPTION <code>extract_fps</code> <p>the number of frames to extract per second</p> <p> TYPE: <code>float</code> </p> <code>fast_mode_enabled</code> <p>whether to use fast mode (keyframes only)</p> <p> TYPE: <code>bool</code> </p>"},{"location":"reference/models/video/#aana.core.models.video.VideoInput","title":"VideoInput","text":"<p>               Bases: <code>BaseModel</code></p> <p>A video input.</p> <p>Exactly one of 'path', 'url', or 'content' must be provided.</p> <p>If 'content' is set to 'file', the video will be loaded from the files uploaded to the endpoint.</p> ATTRIBUTE DESCRIPTION <code>media_id</code> <p>the ID of the video. If not provided, it will be generated automatically.</p> <p> TYPE: <code>MediaId</code> </p> <code>path</code> <p>the file path of the video</p> <p> TYPE: <code>str</code> </p> <code>url</code> <p>the URL of the video (supports YouTube videos)</p> <p> TYPE: <code>AnyUrl</code> </p> <code>content</code> <p>the content of the video in bytes</p> <p> TYPE: <code>bytes</code> </p>"},{"location":"reference/models/video/#aana.core.models.video.VideoInput.check_only_one_field","title":"check_only_one_field","text":"<pre><code>check_only_one_field()\n</code></pre> <p>Check that exactly one of 'path', 'url', or 'content' is provided.</p> RAISES DESCRIPTION <code>ValueError</code> <p>if not exactly one of 'path', 'url', or 'content' is provided</p> RETURNS DESCRIPTION <code>Self</code> <p>the instance</p> <p> TYPE: <code>Self</code> </p> Source code in <code>aana/core/models/video.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_only_one_field(self) -&gt; Self:\n    \"\"\"Check that exactly one of 'path', 'url', or 'content' is provided.\n\n    Raises:\n        ValueError: if not exactly one of 'path', 'url', or 'content' is provided\n\n    Returns:\n        Self: the instance\n    \"\"\"\n    count = sum(value is not None for value in [self.path, self.url, self.content])\n    if count != 1:\n        raise ValueError(  # noqa: TRY003\n            \"Exactly one of 'path', 'url', or 'content' must be provided.\"\n        )\n    return self\n</code></pre>"},{"location":"reference/models/video/#aana.core.models.video.VideoInput.set_files","title":"set_files","text":"<pre><code>set_files(files)\n</code></pre> <p>Set the files for the video.</p> PARAMETER DESCRIPTION <code>files</code> <p>the files uploaded to the endpoint</p> <p> TYPE: <code>Dict[str, bytes]</code> </p> RAISES DESCRIPTION <code>UploadedFileNotFound</code> <p>if the file is not found</p> Source code in <code>aana/core/models/video.py</code> <pre><code>def set_files(self, files: dict[str, bytes]):\n    \"\"\"Set the files for the video.\n\n    Args:\n        files (Dict[str, bytes]): the files uploaded to the endpoint\n\n    Raises:\n        UploadedFileNotFound: if the file is not found\n    \"\"\"\n    if self.content:\n        file_name = self.content\n        if file_name not in files:\n            raise UploadedFileNotFound(filename=file_name)\n        self._file = files[file_name]\n</code></pre>"},{"location":"reference/models/video/#aana.core.models.video.VideoInput.convert_input_to_object","title":"convert_input_to_object","text":"<pre><code>convert_input_to_object()\n</code></pre> <p>Convert the video input to a video object.</p> RETURNS DESCRIPTION <code>Video</code> <p>the video object corresponding to the video input</p> <p> TYPE: <code>Video</code> </p> RAISES DESCRIPTION <code>UploadedFileNotFound</code> <p>if the file is not found</p> Source code in <code>aana/core/models/video.py</code> <pre><code>def convert_input_to_object(self) -&gt; Video:\n    \"\"\"Convert the video input to a video object.\n\n    Returns:\n        Video: the video object corresponding to the video input\n\n    Raises:\n        UploadedFileNotFound: if the file is not found\n    \"\"\"\n    if self.content and not self._file:\n        raise UploadedFileNotFound(filename=self.content)\n    content = self._file if self.content else None\n\n    return Video(\n        path=Path(self.path) if self.path is not None else None,\n        url=self.url,\n        content=content,\n        media_id=self.media_id,\n    )\n</code></pre>"},{"location":"reference/models/video/#aana.core.models.video.VideoInputList","title":"VideoInputList","text":"<p>               Bases: <code>BaseListModel</code></p> <p>A pydantic model for a list of video inputs.</p> <p>Only used for the requests, DO NOT use it for anything else.</p> <p>Convert it to a list of video objects with convert_input_to_object().</p>"},{"location":"reference/models/video/#aana.core.models.video.VideoInputList.check_non_empty","title":"check_non_empty","text":"<pre><code>check_non_empty()\n</code></pre> <p>Check that the list of videos isn't empty.</p> RAISES DESCRIPTION <code>ValueError</code> <p>if the list of videos is empty</p> RETURNS DESCRIPTION <code>Self</code> <p>the instance</p> <p> TYPE: <code>Self</code> </p> Source code in <code>aana/core/models/video.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_non_empty(self) -&gt; Self:\n    \"\"\"Check that the list of videos isn't empty.\n\n    Raises:\n        ValueError: if the list of videos is empty\n\n    Returns:\n        Self: the instance\n    \"\"\"\n    if len(self.root) == 0:\n        raise ValueError(\"The list of videos must not be empty.\")  # noqa: TRY003\n    return self\n</code></pre>"},{"location":"reference/models/video/#aana.core.models.video.VideoInputList.set_files","title":"set_files","text":"<pre><code>set_files(files)\n</code></pre> <p>Set the files for the videos.</p> PARAMETER DESCRIPTION <code>files</code> <p>the files uploaded to the endpoint</p> <p> TYPE: <code>dict[str, bytes]</code> </p> RAISES DESCRIPTION <code>UploadedFileNotFound</code> <p>if the file is not found</p> Source code in <code>aana/core/models/video.py</code> <pre><code>def set_files(self, files: dict[str, bytes]):\n    \"\"\"Set the files for the videos.\n\n    Args:\n        files (dict[str, bytes]): the files uploaded to the endpoint\n\n    Raises:\n        UploadedFileNotFound: if the file is not found\n    \"\"\"\n    for video in self.root:\n        video.set_files(files)\n</code></pre>"},{"location":"reference/models/video/#aana.core.models.video.VideoInputList.convert_input_to_object","title":"convert_input_to_object","text":"<pre><code>convert_input_to_object()\n</code></pre> <p>Convert the VideoInputList to a list of video inputs.</p> RETURNS DESCRIPTION <code>list[VideoInput]</code> <p>List[VideoInput]: the list of video inputs</p> Source code in <code>aana/core/models/video.py</code> <pre><code>def convert_input_to_object(self) -&gt; list[VideoInput]:\n    \"\"\"Convert the VideoInputList to a list of video inputs.\n\n    Returns:\n        List[VideoInput]: the list of video inputs\n    \"\"\"\n    return self.root\n</code></pre>"},{"location":"reference/models/whisper/","title":"Whisper Models","text":""},{"location":"reference/models/whisper/#aana.core.models.whisper","title":"aana.core.models.whisper","text":""},{"location":"reference/models/whisper/#aana.core.models.whisper.WhisperParams","title":"WhisperParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model for the Whisper audio-to-text model parameters.</p> ATTRIBUTE DESCRIPTION <code>language</code> <p>Optional language code such as \"en\" or \"fr\".             If None, language will be automatically detected.</p> <p> TYPE: <code>str</code> </p> <code>beam_size</code> <p>Size of the beam for decoding.</p> <p> TYPE: <code>int</code> </p> <code>best_of</code> <p>Number of best candidate sentences to consider.</p> <p> TYPE: <code>int</code> </p> <code>temperature</code> <p>Controls the sampling randomness.  It can be a tuple of temperatures, which will be successively used upon failures according to either compression_ratio_threshold or log_prob_threshold.</p> <p> TYPE: <code>Union[float, List[float], Tuple[float, ...]]</code> </p> <code>word_timestamps</code> <p>Whether to extract word-level timestamps.</p> <p> TYPE: <code>bool</code> </p> <code>vad_filter</code> <p>Whether to enable voice activity detection to filter non-speech.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"reference/models/whisper/#aana.core.models.whisper.WhisperParams.check_temperature","title":"check_temperature","text":"<pre><code>check_temperature(v)\n</code></pre> <p>Validates a temperature value.</p> PARAMETER DESCRIPTION <code>v</code> <p>Value to validate.</p> <p> TYPE: <code>float</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Temperature is out of range.</p> RETURNS DESCRIPTION <p>Temperature value.</p> Source code in <code>aana/core/models/whisper.py</code> <pre><code>@field_validator(\"temperature\")\n@classmethod\ndef check_temperature(cls, v: float):\n    \"\"\"Validates a temperature value.\n\n    Args:\n        v (float): Value to validate.\n\n    Raises:\n        ValueError: Temperature is out of range.\n\n    Returns:\n        Temperature value.\n    \"\"\"\n    if isinstance(v, float) and not 0 &lt;= v &lt;= 1:\n        raise ValueError(  # noqa: TRY003\n            \"Temperature must be between 0 and 1 when a single float is provided.\"\n        )\n    if isinstance(v, list | tuple) and not all(0 &lt;= t &lt;= 1 for t in v):\n        raise ValueError(  # noqa: TRY003\n            \"Each temperature in the sequence must be between 0 and 1.\"\n        )\n    return v\n</code></pre>"},{"location":"reference/models/whisper/#aana.core.models.whisper.BatchedWhisperParams","title":"BatchedWhisperParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model for the Batched version of Whisper audio-to-text model parameters.</p> ATTRIBUTE DESCRIPTION <code>language</code> <p>Optional language code such as \"en\" or \"fr\".             If None, language will be automatically detected.</p> <p> TYPE: <code>str</code> </p> <code>beam_size</code> <p>Size of the beam for decoding.</p> <p> TYPE: <code>int</code> </p> <code>best_of</code> <p>Number of best candidate sentences to consider.</p> <p> TYPE: <code>int</code> </p> <code>temperature</code> <p>Controls the sampling randomness.  It can be a tuple of temperatures, which will be successively used upon failures according to either compression_ratio_threshold or log_prob_threshold.</p> <p> TYPE: <code>Union[float, List[float], Tuple[float, ...]]</code> </p> <code>#TODO</code> <p>add other parameters</p> <p> TYPE: <code>Union[float, List[float], Tuple[float, ...]]</code> </p>"},{"location":"reference/models/whisper/#aana.core.models.whisper.BatchedWhisperParams.check_temperature","title":"check_temperature","text":"<pre><code>check_temperature(v)\n</code></pre> <p>Validates a temperature value.</p> PARAMETER DESCRIPTION <code>v</code> <p>Value to validate.</p> <p> TYPE: <code>float</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>Temperature is out of range.</p> RETURNS DESCRIPTION <p>Temperature value.</p> Source code in <code>aana/core/models/whisper.py</code> <pre><code>@field_validator(\"temperature\")\n@classmethod\ndef check_temperature(cls, v: float):\n    \"\"\"Validates a temperature value.\n\n    Args:\n        v (float): Value to validate.\n\n    Raises:\n        ValueError: Temperature is out of range.\n\n    Returns:\n        Temperature value.\n    \"\"\"\n    if isinstance(v, float) and not 0 &lt;= v &lt;= 1:\n        raise ValueError(  # noqa: TRY003\n            \"Temperature must be between 0 and 1 when a single float is provided.\"\n        )\n    if isinstance(v, list | tuple) and not all(0 &lt;= t &lt;= 1 for t in v):\n        raise ValueError(  # noqa: TRY003\n            \"Each temperature in the sequence must be between 0 and 1.\"\n        )\n    return v\n</code></pre>"},{"location":"reference/storage/","title":"Storage","text":"<p>Aana SDK provides an integration with an SQL database to store and retrieve data. </p> <p>Currently, Aana SDK supports SQLite (default) and PostgreSQL databases. See Database Configuration for more information.</p> <p>The database integration is based on the SQLAlchemy library and consists of two main components: </p> <ul> <li>Models - Database models (entities) that represent tables in the database.</li> <li>Repositories - Classes that provide an interface to interact with the database models.</li> </ul> <p>To use the database integration, you can either:</p> <ul> <li>Use the provided models and repositories.</li> <li>Create your own models and repositories by extending the provided ones (for example, extending the VideoEntity model to add custom fields).</li> <li>Create your own models and repositories from scratch/base classes (for example, creating a new model for a new entity).</li> </ul>"},{"location":"reference/storage/#how-to-use-provided-models-and-repositories","title":"How to Use Provided Models and Repositories","text":"<p>If you want to use the provided models and repositories, you can use the following steps:</p>"},{"location":"reference/storage/#get-session-object","title":"Get session object","text":"<p>You can use <code>get_session</code> method from the <code>aana.storage.session</code> module:</p> <pre><code>from aana.storage.session import get_session\n\nsession = get_session()\n</code></pre> <p>If you are using Endpoint, please use <code>get_session</code> function with the context manager:</p> <pre><code>from aana.api import Endpoint\nfrom aana.storage.session import get_session\n\nclass TranscribeVideoEndpoint(Endpoint):\n\n    async def run(self, video: VideoInput) -&gt; WhisperOutput:\n        with get_session() as session:\n            repo = SomeRepository(session)\n            repo.some_method(...)\n            # or \n            SomeRepository(session).some_method(...)\n</code></pre>"},{"location":"reference/storage/#create-a-repository-object-and-use-it-to-interact-with-the-database","title":"Create a repository object and use it to interact with the database.","text":"<p>You can use the provided repositories from the <code>aana.storage.repository</code> module. See Repositories for the list of available repositories.</p> <p>For example, to work with the <code>VideoEntity</code> model, you can create a <code>VideoRepository</code> object:</p> <pre><code>from aana.storage.repository import VideoRepository\n\nvideo_repository = VideoRepository(session)\n</code></pre> <p>And then use the repository object to interact with the database. For example, to save a video object to the database (storing media ID, URL, path, title, description, etc.):  </p> <pre><code>from aana.core.models import Video\n\nvideo = Video(title=\"My Video\", url=\"https://example.com/video.mp4\")\nvideo_repository.save(video)\n</code></pre> <p>Or, if you are using Endpoint, you can create a repository object in the <code>initialize</code> method:</p> <pre><code>from aana.api import Endpoint\nfrom aana.storage.repository import VideoRepository\nfrom aana.storage.session import get_session\n\nclass TranscribeVideoEndpoint(Endpoint):\n\n    async def run(self, video: VideoInput) -&gt; WhisperOutput:\n        video_obj: Video = await run_remote(download_video)(video_input=video)\n        with get_session() as session:\n            VideoRepository(session).save(video_obj)\n        # ...\n</code></pre>"},{"location":"reference/storage/models/","title":"Models","text":""},{"location":"reference/storage/models/#aana.storage.models","title":"aana.storage.models","text":""},{"location":"reference/storage/models/#aana.storage.models.BaseEntity","title":"BaseEntity","text":"<p>               Bases: <code>DeclarativeBase</code>, <code>InheritanceReuseMixin</code></p> <p>Base for all ORM classes.</p>"},{"location":"reference/storage/models/#aana.storage.models.BaseEntity.from_parent","title":"from_parent","text":"<pre><code>from_parent(parent_instance, **kwargs)\n</code></pre> <p>Create a new instance of the child class, reusing attributes from the parent instance.</p> PARAMETER DESCRIPTION <code>parent_instance</code> <p>An instance of the parent class</p> <p> TYPE: <code>Any</code> </p> <code>kwargs</code> <p>Additional keyword arguments to set on the new instance</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the child class</p> <p> TYPE: <code>T</code> </p> Source code in <code>aana/storage/models/base.py</code> <pre><code>@classmethod\ndef from_parent(cls: type[T], parent_instance: Any, **kwargs: Any) -&gt; T:\n    \"\"\"Create a new instance of the child class, reusing attributes from the parent instance.\n\n    Args:\n        parent_instance (Any): An instance of the parent class\n        kwargs (Any): Additional keyword arguments to set on the new instance\n\n    Returns:\n        T: A new instance of the child class\n    \"\"\"\n    # Get the mapped attributes of the parent class\n    mapper = object_mapper(parent_instance)\n    attributes = {\n        prop.key: getattr(parent_instance, prop.key)\n        for prop in mapper.iterate_properties\n        if hasattr(parent_instance, prop.key)\n        and prop.key\n        != mapper.polymorphic_on.name  # don't copy the polymorphic_on attribute from the parent\n    }\n\n    # Update attributes with any additional kwargs\n    attributes.update(kwargs)\n\n    # Create and return a new instance of the child class\n    return cls(**attributes)\n</code></pre>"},{"location":"reference/storage/models/#aana.storage.models.CaptionEntity","title":"CaptionEntity","text":"<p>               Bases: <code>BaseEntity</code>, <code>TimeStampEntity</code></p> <p>ORM model for video captions.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for the caption.</p> <p> TYPE: <code>int</code> </p> <code>model</code> <p>Name of the model used to generate the caption.</p> <p> TYPE: <code>str</code> </p> <code>frame_id</code> <p>The 0-based frame id of video for caption.</p> <p> TYPE: <code>int</code> </p> <code>caption</code> <p>Frame caption.</p> <p> TYPE: <code>str</code> </p> <code>timestamp</code> <p>Frame timestamp in seconds.</p> <p> TYPE: <code>float</code> </p> <code>caption_type</code> <p>The type of caption (populated automatically by ORM based on <code>polymorphic_identity</code> of subclass).</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/storage/models/#aana.storage.models.CaptionEntity.from_parent","title":"from_parent","text":"<pre><code>from_parent(parent_instance, **kwargs)\n</code></pre> <p>Create a new instance of the child class, reusing attributes from the parent instance.</p> PARAMETER DESCRIPTION <code>parent_instance</code> <p>An instance of the parent class</p> <p> TYPE: <code>Any</code> </p> <code>kwargs</code> <p>Additional keyword arguments to set on the new instance</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the child class</p> <p> TYPE: <code>T</code> </p> Source code in <code>aana/storage/models/base.py</code> <pre><code>@classmethod\ndef from_parent(cls: type[T], parent_instance: Any, **kwargs: Any) -&gt; T:\n    \"\"\"Create a new instance of the child class, reusing attributes from the parent instance.\n\n    Args:\n        parent_instance (Any): An instance of the parent class\n        kwargs (Any): Additional keyword arguments to set on the new instance\n\n    Returns:\n        T: A new instance of the child class\n    \"\"\"\n    # Get the mapped attributes of the parent class\n    mapper = object_mapper(parent_instance)\n    attributes = {\n        prop.key: getattr(parent_instance, prop.key)\n        for prop in mapper.iterate_properties\n        if hasattr(parent_instance, prop.key)\n        and prop.key\n        != mapper.polymorphic_on.name  # don't copy the polymorphic_on attribute from the parent\n    }\n\n    # Update attributes with any additional kwargs\n    attributes.update(kwargs)\n\n    # Create and return a new instance of the child class\n    return cls(**attributes)\n</code></pre>"},{"location":"reference/storage/models/#aana.storage.models.CaptionEntity.from_caption_output","title":"from_caption_output","text":"<pre><code>from_caption_output(model_name, caption, frame_id, timestamp)\n</code></pre> <p>Converts a Caption pydantic model to a CaptionEntity.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the model used to generate the caption.</p> <p> TYPE: <code>str</code> </p> <code>caption</code> <p>Caption pydantic model.</p> <p> TYPE: <code>Caption</code> </p> <code>frame_id</code> <p>The 0-based frame id of video for caption.</p> <p> TYPE: <code>int</code> </p> <code>timestamp</code> <p>Frame timestamp in seconds.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>CaptionEntity</code> <p>ORM model for video captions.</p> <p> TYPE: <code>CaptionEntity</code> </p> Source code in <code>aana/storage/models/caption.py</code> <pre><code>@classmethod\ndef from_caption_output(\n    cls,\n    model_name: str,\n    caption: Caption,\n    frame_id: int,\n    timestamp: float,\n) -&gt; CaptionEntity:\n    \"\"\"Converts a Caption pydantic model to a CaptionEntity.\n\n    Args:\n        model_name (str): Name of the model used to generate the caption.\n        caption (Caption): Caption pydantic model.\n        frame_id (int): The 0-based frame id of video for caption.\n        timestamp (float): Frame timestamp in seconds.\n\n    Returns:\n        CaptionEntity: ORM model for video captions.\n    \"\"\"\n    return CaptionEntity(\n        model=model_name,\n        frame_id=frame_id,\n        caption=str(caption),\n        timestamp=timestamp,\n    )\n</code></pre>"},{"location":"reference/storage/models/#aana.storage.models.MediaEntity","title":"MediaEntity","text":"<p>               Bases: <code>BaseEntity</code>, <code>TimeStampEntity</code></p> <p>Base ORM class for media (e.g. videos, images, etc.).</p> <p>This class is meant to be subclassed by other media types.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for the media.</p> <p> TYPE: <code>MediaId</code> </p> <code>media_type</code> <p>The type of media (populated automatically by ORM based on <code>polymorphic_identity</code> of   subclass).</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/storage/models/#aana.storage.models.MediaEntity.from_parent","title":"from_parent","text":"<pre><code>from_parent(parent_instance, **kwargs)\n</code></pre> <p>Create a new instance of the child class, reusing attributes from the parent instance.</p> PARAMETER DESCRIPTION <code>parent_instance</code> <p>An instance of the parent class</p> <p> TYPE: <code>Any</code> </p> <code>kwargs</code> <p>Additional keyword arguments to set on the new instance</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the child class</p> <p> TYPE: <code>T</code> </p> Source code in <code>aana/storage/models/base.py</code> <pre><code>@classmethod\ndef from_parent(cls: type[T], parent_instance: Any, **kwargs: Any) -&gt; T:\n    \"\"\"Create a new instance of the child class, reusing attributes from the parent instance.\n\n    Args:\n        parent_instance (Any): An instance of the parent class\n        kwargs (Any): Additional keyword arguments to set on the new instance\n\n    Returns:\n        T: A new instance of the child class\n    \"\"\"\n    # Get the mapped attributes of the parent class\n    mapper = object_mapper(parent_instance)\n    attributes = {\n        prop.key: getattr(parent_instance, prop.key)\n        for prop in mapper.iterate_properties\n        if hasattr(parent_instance, prop.key)\n        and prop.key\n        != mapper.polymorphic_on.name  # don't copy the polymorphic_on attribute from the parent\n    }\n\n    # Update attributes with any additional kwargs\n    attributes.update(kwargs)\n\n    # Create and return a new instance of the child class\n    return cls(**attributes)\n</code></pre>"},{"location":"reference/storage/models/#aana.storage.models.TranscriptEntity","title":"TranscriptEntity","text":"<p>               Bases: <code>BaseEntity</code>, <code>TimeStampEntity</code></p> <p>ORM class for media transcripts generated by a model.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for the transcript.</p> <p> TYPE: <code>int</code> </p> <code>model</code> <p>Name of the model used to generate the transcript.</p> <p> TYPE: <code>str</code> </p> <code>transcript</code> <p>Full text transcript of the media.</p> <p> TYPE: <code>str</code> </p> <code>segments</code> <p>Segments of the transcript.</p> <p> TYPE: <code>dict</code> </p> <code>language</code> <p>Language of the transcript as predicted by the model.</p> <p> TYPE: <code>str</code> </p> <code>language_confidence</code> <p>Confidence score of language prediction.</p> <p> TYPE: <code>float</code> </p> <code>transcript_type</code> <p>The type of transcript (populated automatically by ORM based on <code>polymorphic_identity</code> of subclass).</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/storage/models/#aana.storage.models.TranscriptEntity.from_parent","title":"from_parent","text":"<pre><code>from_parent(parent_instance, **kwargs)\n</code></pre> <p>Create a new instance of the child class, reusing attributes from the parent instance.</p> PARAMETER DESCRIPTION <code>parent_instance</code> <p>An instance of the parent class</p> <p> TYPE: <code>Any</code> </p> <code>kwargs</code> <p>Additional keyword arguments to set on the new instance</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the child class</p> <p> TYPE: <code>T</code> </p> Source code in <code>aana/storage/models/base.py</code> <pre><code>@classmethod\ndef from_parent(cls: type[T], parent_instance: Any, **kwargs: Any) -&gt; T:\n    \"\"\"Create a new instance of the child class, reusing attributes from the parent instance.\n\n    Args:\n        parent_instance (Any): An instance of the parent class\n        kwargs (Any): Additional keyword arguments to set on the new instance\n\n    Returns:\n        T: A new instance of the child class\n    \"\"\"\n    # Get the mapped attributes of the parent class\n    mapper = object_mapper(parent_instance)\n    attributes = {\n        prop.key: getattr(parent_instance, prop.key)\n        for prop in mapper.iterate_properties\n        if hasattr(parent_instance, prop.key)\n        and prop.key\n        != mapper.polymorphic_on.name  # don't copy the polymorphic_on attribute from the parent\n    }\n\n    # Update attributes with any additional kwargs\n    attributes.update(kwargs)\n\n    # Create and return a new instance of the child class\n    return cls(**attributes)\n</code></pre>"},{"location":"reference/storage/models/#aana.storage.models.TranscriptEntity.from_asr_output","title":"from_asr_output","text":"<pre><code>from_asr_output(model_name, info, transcription, segments)\n</code></pre> <p>Converts an AsrTranscriptionInfo and AsrTranscription to a single Transcript entity.</p> PARAMETER DESCRIPTION <code>model_name</code> <p>Name of the model used to generate the transcript.</p> <p> TYPE: <code>str</code> </p> <code>info</code> <p>Information about the transcription.</p> <p> TYPE: <code>AsrTranscriptionInfo</code> </p> <code>transcription</code> <p>The full transcription.</p> <p> TYPE: <code>AsrTranscription</code> </p> <code>segments</code> <p>Segments of the transcription.</p> <p> TYPE: <code>AsrSegments</code> </p> RETURNS DESCRIPTION <code>TranscriptEntity</code> <p>A new instance of the TranscriptEntity class.</p> <p> TYPE: <code>TranscriptEntity</code> </p> Source code in <code>aana/storage/models/transcript.py</code> <pre><code>@classmethod\ndef from_asr_output(\n    cls,\n    model_name: str,\n    info: AsrTranscriptionInfo,\n    transcription: AsrTranscription,\n    segments: AsrSegments,\n) -&gt; TranscriptEntity:\n    \"\"\"Converts an AsrTranscriptionInfo and AsrTranscription to a single Transcript entity.\n\n    Args:\n        model_name (str): Name of the model used to generate the transcript.\n        info (AsrTranscriptionInfo): Information about the transcription.\n        transcription (AsrTranscription): The full transcription.\n        segments (AsrSegments): Segments of the transcription.\n\n    Returns:\n        TranscriptEntity: A new instance of the TranscriptEntity class.\n    \"\"\"\n    return TranscriptEntity(\n        model=model_name,\n        language=info.language,\n        language_confidence=info.language_confidence,\n        transcript=transcription.text,\n        segments=[s.model_dump() for s in segments],\n    )\n</code></pre>"},{"location":"reference/storage/models/#aana.storage.models.VideoEntity","title":"VideoEntity","text":"<p>               Bases: <code>MediaEntity</code></p> <p>Base ORM class for videos.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for the video.</p> <p> TYPE: <code>MediaId</code> </p> <code>path</code> <p>Path to the video file.</p> <p> TYPE: <code>str</code> </p> <code>url</code> <p>URL to the video file.</p> <p> TYPE: <code>str</code> </p> <code>title</code> <p>Title of the video.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Description of the video.</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/storage/models/#aana.storage.models.VideoEntity.from_parent","title":"from_parent","text":"<pre><code>from_parent(parent_instance, **kwargs)\n</code></pre> <p>Create a new instance of the child class, reusing attributes from the parent instance.</p> PARAMETER DESCRIPTION <code>parent_instance</code> <p>An instance of the parent class</p> <p> TYPE: <code>Any</code> </p> <code>kwargs</code> <p>Additional keyword arguments to set on the new instance</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>T</code> <p>A new instance of the child class</p> <p> TYPE: <code>T</code> </p> Source code in <code>aana/storage/models/base.py</code> <pre><code>@classmethod\ndef from_parent(cls: type[T], parent_instance: Any, **kwargs: Any) -&gt; T:\n    \"\"\"Create a new instance of the child class, reusing attributes from the parent instance.\n\n    Args:\n        parent_instance (Any): An instance of the parent class\n        kwargs (Any): Additional keyword arguments to set on the new instance\n\n    Returns:\n        T: A new instance of the child class\n    \"\"\"\n    # Get the mapped attributes of the parent class\n    mapper = object_mapper(parent_instance)\n    attributes = {\n        prop.key: getattr(parent_instance, prop.key)\n        for prop in mapper.iterate_properties\n        if hasattr(parent_instance, prop.key)\n        and prop.key\n        != mapper.polymorphic_on.name  # don't copy the polymorphic_on attribute from the parent\n    }\n\n    # Update attributes with any additional kwargs\n    attributes.update(kwargs)\n\n    # Create and return a new instance of the child class\n    return cls(**attributes)\n</code></pre>"},{"location":"reference/storage/repositories/","title":"Repositories","text":""},{"location":"reference/storage/repositories/#aana.storage.repository","title":"aana.storage.repository","text":""}]}