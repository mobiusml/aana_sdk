{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install  ipywidgets\n",
    "# %pip install ipywidgets vllm transformers\n",
    "# %pip install  hf_transfer huggingface_hub\n",
    "# %pip install torch\n",
    "# %pip install git+https://github.com/mobiusml/gemlite.git -qU\n",
    "# %pip install git+https://github.com/mobiusml/hqq.git -q\n",
    "# !apt-get update && apt-get install -y nvidia-cuda-toolkit\n",
    "# Uninstall existing PyTorch\n",
    "# !pip uninstall -y torch torchvision torchaudio\n",
    "# Install PyTorch with CUDA support\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install setuptools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLLM_USE_V1=0\n",
    "import os\n",
    "\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hqq\n",
    "\n",
    "hqq.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gemlite\n",
    "\n",
    "gemlite.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/aana_sdk/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 13:26:15 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 13:26:16,524\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 13:26:24 [config.py:689] This model supports multiple tasks: {'generate', 'reward', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 04-22 13:26:24 [config.py:768] hqq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 04-22 13:26:24 [llm_engine.py:243] Initializing a V0 LLM engine (v0.8.4) with config: model='mobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf', speculative_config=None, tokenizer='mobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=hqq, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=mobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-22 13:26:25 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 04-22 13:26:26 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-22 13:26:26 [model_runner.py:1110] Starting to load model mobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf...\n",
      "INFO 04-22 13:26:26 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:06<00:06,  6.44s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  2.77s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 13:26:33 [loader.py:458] Loading weights took 6.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 13:26:34 [model_runner.py:1146] Model loading took 5.4719 GiB and 8.028157 seconds\n",
      "INFO 04-22 13:27:04 [worker.py:267] Memory profiling takes 29.50 seconds\n",
      "INFO 04-22 13:27:04 [worker.py:267] the current vLLM instance can use total_gpu_memory (44.45GiB) x gpu_memory_utilization (0.90) = 40.00GiB\n",
      "INFO 04-22 13:27:04 [worker.py:267] model weights take 5.47GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 33.06GiB.\n",
      "INFO 04-22 13:27:04 [executor_base.py:112] # cuda blocks: 38686, # CPU blocks: 4681\n",
      "INFO 04-22 13:27:04 [executor_base.py:117] Maximum concurrency for 4096 tokens per request: 151.12x\n",
      "INFO 04-22 13:27:06 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [07:50<00:00, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 13:34:57 [model_runner.py:1598] Graph capturing finished in 471 secs, took 0.23 GiB\n",
      "INFO 04-22 13:34:57 [llm_engine.py:449] init engine (profile, create kv cache, warmup model) took 502.88 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it, est. speed input: 3.16 toks/s, output: 93.02 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of Germany is Berlin. Berlin has been the capital of Germany since 1990, when the country was reunified. The city has a rich history and is known for its art, culture, and architecture. Some notable landmarks in Berlin include the Brandenburg Gate, the Reichstag building, and the Berlin Wall. The city is also home to many museums, parks, and cultural events, making it a popular destination for tourists from around the world. In addition to serving as the political and cultural center of Germany, Berlin is also an important economic and transportation hub in the country. The city's location at the intersection of major transportation routes, including the Oder and Spree rivers, as well as its proximity to the Baltic Sea, has made it a key center for trade and commerce in Germany and beyond. Overall, Berlin is a vibrant and dynamic city with a rich history and cultural heritage, and it continues to be an important center of politics, culture, and economics in Germany and the wider world.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "from hqq.utils.vllm import set_vllm_hqq_backend, VLLM_HQQ_BACKEND\n",
    "\n",
    "set_vllm_hqq_backend(backend=VLLM_HQQ_BACKEND.GEMLITE)\n",
    "\n",
    "model_id = \"mobiuslabsgmbh/Qwen2.5-7B-Instruct_4bitgs64_hqq_hf\"\n",
    "\n",
    "llm = LLM(model=model_id, max_model_len=4096, enable_chunked_prefill=False)\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=1024)\n",
    "outputs = llm.generate([\"What is the capital of Germany?\"], sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it, est. speed input: 4.33 toks/s, output: 89.72 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of Germany is Berlin. \n",
      "\n",
      "Some additional information about Berlin:\n",
      "\n",
      "1. It is located in the eastern part of Germany.\n",
      "2. Berlin has a rich history, having been divided into East and West during the Cold War.\n",
      "3. It is known for its modern architecture, museums, art scene, and vibrant nightlife.\n",
      "4. The city hosts major international events and is home to many embassies.\n",
      "5. Its population is around 3.7 million people, making it the largest city in Germany. \n",
      "\n",
      "If you're looking for another German city, it's worth noting that Frankfurt is also a major city and is often referred to as the financial capital of Germany. However, Berlin remains the official capital.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate([\"What is the capital of Germany?\"], sampling_params)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
