# Automatic Speech Recognition (ASR) Models

[WhisperDeployment](./../../reference/deployments.md#aana.deployments.WhisperDeployment) allows you to transcribe or translate audio with Whisper models. The deployment is based on the [faster-whisper](https://github.com/SYSTRAN/faster-whisper) library.

[WhisperConfig](./../../reference/deployments.md#aana.deployments.WhisperConfig) is used to configure the Whisper deployment.

::: aana.deployments.WhisperConfig
    options:
        show_bases: false
        heading_level: 4
        show_docstring_description: false
        docstring_section_style: list

### Example Configurations

As an example, let's see how to configure the Whisper deployment for the [Whisper Medium model](https://huggingface.co/Systran/faster-whisper-medium).


!!! example "Whisper Medium"
    
    ```python
    from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType

    WhisperDeployment.options(
        num_replicas=1,
        max_ongoing_requests=1000,
        ray_actor_options={"num_gpus": 0.25},
        user_config=WhisperConfig(
            model_size=WhisperModelSize.MEDIUM,
            compute_type=WhisperComputeType.FLOAT16,
        ).model_dump(mode="json"),
    )
    ```

Model size is the one of the Whisper model sizes available in the `faster-whisper` library. `compute_type` is the data type to be used for the model.

Here are some other possible configurations for the Whisper deployment:

??? example "Whisper Tiny on CPU"
    
    ```python
    from aana.deployments.whisper_deployment import WhisperDeployment, WhisperConfig, WhisperModelSize, WhisperComputeType

    # for CPU do not specify num_gpus and use FLOAT32 compute type
    WhisperDeployment.options(
        num_replicas=1,
        user_config=WhisperConfig(
            model_size=WhisperModelSize.TINY,
            compute_type=WhisperComputeType.FLOAT32,
        ).model_dump(mode="json"),
    )
    ```

### Diarized ASR

Diarized transcription can be generated by using [WhisperDeployment](./../../reference/deployments.md#aana.deployments.WhisperDeployment) and [PyannoteSpeakerDiarizationDeployment](./../../reference/deployments.md#aana.deployments.PyannoteSpeakerDiarizationDeployment) and combining the timelines using post processing with [PostProcessingForDiarizedAsr](./../../reference/processors.md#aana.processors.speaker.PostProcessingForDiarizedAsr).

Example configuration for the PyannoteSpeakerDiarization model is available at [Speaker Diarization](./speaker_recognition.md/#speaker-diarization-sd-models) model hub.

You can simply define the model deployments and the endpoint to transcribe the video with diarization. Below code snippet shows the how to combine the outputs from ASR and diarization deployments:


```python
from aana.processors.speaker import PostProcessingForDiarizedAsr
from aana.core.models.base import pydantic_to_dict


# diarized transcript requires word_timestamps from ASR
whisper_params.word_timestamps = True

# asr_handle is an AanaDeploymentHandle for WhisperDeployment
transcription = await self.asr_handle.transcribe(
    audio=audio, params=whisper_params
)

# diar_handle is an AanaDeploymentHandle for PyannoteSpeakerDiarizationDeployment
diarized_output = await self.diar_handle.diarize(
    audio=audio, params=diar_params
)

updated_segments = PostProcessingForDiarizedAsr.process(
    diarized_segments=diarized_output["segments"],
    transcription_segments=transcription["segments"],
)

# updated_segments will have speaker information as well:

# [AsrSegment(text=' Hello. Hello.', 
#            time_interval=TimeInterval(start=6.38, end=7.84), 
#            confidence=0.8329984157521475, 
#            no_speech_confidence=0.012033582665026188, 
#            words=[AsrWord(word=' Hello.', speaker='SPEAKER_01',time_interval=TimeInterval(start=6.38, end=7.0), alignment_confidence=0.6853185296058655), 
#                   AsrWord(word=' Hello.', speaker='SPEAKER_01', time_interval=TimeInterval(start=7.5, end=7.84), alignment_confidence=0.7124693989753723)], 
#           speaker='SPEAKER_01'), 
#
# AsrSegment(text=" Oh, hello. I didn't know you were there.", 
#            time_interval=TimeInterval(start=8.3, end=9.68), 
#            confidence=0.8329984157521475, 
#            no_speech_confidence=0.012033582665026188, 
#            words=[AsrWord(word=' Oh,', speaker='SPEAKER_02', time_interval=TimeInterval(start=8.3, end=8.48), alignment_confidence=0.8500092029571533), 
#                   AsrWord(word=' hello.', speaker='SPEAKER_02', time_interval=TimeInterval(start=8.5, end=8.76), alignment_confidence=0.9408962726593018), ...], 
#            speaker='SPEAKER_02'), 
# ...
# ]

```
An example notebook on diarized transcription is available at [notebooks/diarized_transcription_example.ipynb](https://github.com/mobiusml/aana_sdk/tree/main/notebooks/diarized_transcription_example.ipynb).


