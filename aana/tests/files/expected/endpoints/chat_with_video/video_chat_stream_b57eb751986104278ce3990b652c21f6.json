[
    {
        "data": {
            "model_name": "TheBloke/Llama-2-7b-Chat-AWQ"
        },
        "error": "InferenceException",
        "message": "InferenceException(model_name=TheBloke/Llama-2-7b-Chat-AWQ)",
        "stacktrace": "\u001b[36mray::py:VLLMDeployment.handle_request_streaming()\u001b[39m (pid=2231385, ip=172.17.0.5, actor_id=8f7c98042d31c2a59839124e01000000, repr=<ray.serve._private.replica.ServeReplica:aana_tests_integration_test_chat_with_video.py:VLLMDeployment object at 0x7f5b2051b7f0>)\n    async for result in generator:\n  File \"/root/.cache/pypoetry/virtualenvs/aana-vIr3-B0u-py3.10/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 896, in call_user_method_generator\n    async for result in result_generator:\n  File \"/workspaces/aana_sdk/aana/utils/test.py\", line 144, in wrapper_generator\n    async for item in func(*args, **kwargs):\n  File \"/workspaces/aana_sdk/aana/deployments/vllm_deployment.py\", line 248, in chat_stream\n    async for chunk in self.generate_stream(prompt, sampling_params):\n  File \"/workspaces/aana_sdk/aana/utils/test.py\", line 144, in wrapper_generator\n    async for item in func(*args, **kwargs):\n  File \"/workspaces/aana_sdk/aana/deployments/vllm_deployment.py\", line 175, in generate_stream\n    raise InferenceException(model_name=self.model) from e\naana.exceptions.general.InferenceException: InferenceException(model_name=TheBloke/Llama-2-7b-Chat-AWQ)"
    }
]