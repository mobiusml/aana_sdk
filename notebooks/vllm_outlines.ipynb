{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/aana-vIr3-B0u-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-30 13:02:00,494\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-08-30 13:02:05,514\tWARNING api.py:346 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:05,517\tWARNING api.py:397 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:07,051\tWARNING api.py:346 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:07,052\tWARNING api.py:397 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:07,590\tWARNING api.py:346 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:07,592\tWARNING api.py:397 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:07,602\tWARNING api.py:346 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:07,603\tWARNING api.py:397 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:07,630\tWARNING api.py:346 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:07,631\tWARNING api.py:397 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:08,965\tWARNING api.py:346 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:08,967\tWARNING api.py:397 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:09,205\tWARNING api.py:346 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "2024-08-30 13:02:09,209\tWARNING api.py:397 -- The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ProxyActor pid=2775598)\u001b[0m INFO 2024-08-30 13:02:21,667 proxy 172.17.0.2 proxy.py:1179 - Proxy starting on node 79f87f06cd7652c413cb36bc9866673c5f931ae49f51e1c8dfd3b1c4 (HTTP port: 8000).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:03:09 async_llm_engine.py:141] Finished request 95f87ee1442740658364f77f10eea756.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:03:15,312 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36984868864; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:03:25,323 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36984078336; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:03:35,335 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36984033280; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:03:45,347 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36983996416; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:03:55,359 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36983918592; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:04:06 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:04:06 async_llm_engine.py:141] Finished request d320674c42704b9a94a77aa9289aa97b.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:04:15,384 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36983750656; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:04:25,396 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36982759424; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:04:35,408 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36982689792; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:04:45,420 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36982652928; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "Compiling FSM index for all state transitions:   0%|          | 0/9 [00:00<?, ?it/s]\n",
      "Compiling FSM index for all state transitions: 100%|██████████| 9/9 [00:00<00:00, 51.19it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:05:15,454 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36973023232; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:05:45,490 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36972068864; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:05:55,502 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36972044288; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:06:05,514 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36971962368; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:06:15,525 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36971925504; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:06:25,535 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36970876928; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:06:35,548 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36970799104; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:07:15,596 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36970577920; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:07:25,607 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36969463808; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "from aana.sdk import AanaSDK\n",
    "\n",
    "aana = AanaSDK().connect(show_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aana.core.models.sampling import SamplingParams\n",
    "from aana.core.models.types import Dtype\n",
    "from aana.deployments.vllm_deployment import VLLMConfig, VLLMDeployment\n",
    "\n",
    "deployment = VLLMDeployment.options(\n",
    "    num_replicas=1,\n",
    "    max_ongoing_requests=1000,\n",
    "    ray_actor_options={\"num_gpus\": 0.25},\n",
    "    user_config=VLLMConfig(\n",
    "        model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        dtype=Dtype.FLOAT16,\n",
    "        gpu_memory_reserved=10000,\n",
    "        enforce_eager=True,\n",
    "        default_sampling_params=SamplingParams(\n",
    "            temperature=0.0, top_p=1.0, top_k=-1, max_tokens=1024\n",
    "        ),\n",
    "        engine_args={\n",
    "            \"trust_remote_code\": True,\n",
    "        },\n",
    "    ).model_dump(mode=\"json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=2775506)\u001b[0m INFO 2024-08-30 13:02:21,938 controller 2775506 deployment_state.py:1598 - Deploying new version of Deployment(name='VLLMDeployment', app='vllm_deployment') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=2775506)\u001b[0m INFO 2024-08-30 13:02:22,044 controller 2775506 deployment_state.py:1844 - Adding 1 replica to Deployment(name='VLLMDeployment', app='vllm_deployment').\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:02:25,253 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36988960768; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:27,853 vllm_deployment_VLLMDeployment n2j79mfq api.py:346 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:27,853 vllm_deployment_VLLMDeployment n2j79mfq api.py:397 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:29,356 vllm_deployment_VLLMDeployment n2j79mfq api.py:346 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:29,357 vllm_deployment_VLLMDeployment n2j79mfq api.py:397 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:30,085 vllm_deployment_VLLMDeployment n2j79mfq api.py:346 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:30,085 vllm_deployment_VLLMDeployment n2j79mfq api.py:397 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:30,092 vllm_deployment_VLLMDeployment n2j79mfq api.py:346 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:30,093 vllm_deployment_VLLMDeployment n2j79mfq api.py:397 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:30,115 vllm_deployment_VLLMDeployment n2j79mfq api.py:346 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:30,116 vllm_deployment_VLLMDeployment n2j79mfq api.py:397 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:31,650 vllm_deployment_VLLMDeployment n2j79mfq api.py:346 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:31,650 vllm_deployment_VLLMDeployment n2j79mfq api.py:397 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:31,876 vllm_deployment_VLLMDeployment n2j79mfq api.py:346 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 2024-08-30 13:02:31,876 vllm_deployment_VLLMDeployment n2j79mfq api.py:397 - The default value for `max_ongoing_requests` has changed from 100 to 5 in Ray 2.32.0.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:02:35,265 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36988788736; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m WARNING 08-30 13:02:36 config.py:1454] Casting torch.bfloat16 to torch.float16.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:02:36 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='microsoft/Phi-3-mini-4k-instruct', speculative_config=None, tokenizer='microsoft/Phi-3-mini-4k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=microsoft/Phi-3-mini-4k-instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:02:37 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:02:37 selector.py:54] Using XFormers backend.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:02:38 model_runner.py:720] Starting to load model microsoft/Phi-3-mini-4k-instruct...\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:02:38 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:02:38 selector.py:54] Using XFormers backend.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:02:38 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.52s/it]\n",
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:02:45,277 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36988755968; capacity: 983347249152. Object creation will fail if spilling is required.\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.27s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:10<00:00,  5.00s/it]\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:02:49 model_runner.py:732] Loading model weights took 7.1183 GB\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:02:50 gpu_executor.py:102] # GPU blocks: 379, # CPU blocks: 682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=2775506)\u001b[0m WARNING 2024-08-30 13:02:52,144 controller 2775506 deployment_state.py:2166 - Deployment 'VLLMDeployment' in application 'vllm_deployment' has 1 replicas that have taken more than 30s to initialize. This may be caused by a slow __init__ or reconfigure method.\n"
     ]
    }
   ],
   "source": [
    "aana.register_deployment(\n",
    "    \"vllm_deployment\",\n",
    "    deployment,\n",
    "    deploy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aana.core.models.chat import ChatDialog, ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aana.deployments.aana_deployment_handle import AanaDeploymentHandle\n",
    "\n",
    "handle = await AanaDeploymentHandle.create(\"vllm_deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class CityDescription(BaseModel):\n",
    "    city: str\n",
    "    country: str\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = ChatDialog(\n",
    "    messages=[\n",
    "        ChatMessage(role=\"user\", content=\"Tell me about Paris.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:03:49 async_llm_engine.py:174] Added request 43e483dc06e64d2ea6058e20b0ae937d.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:03:49 metrics.py:406] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:03:54 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.7%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:03:54 async_llm_engine.py:141] Finished request 43e483dc06e64d2ea6058e20b0ae937d.\n"
     ]
    }
   ],
   "source": [
    "response = await handle.chat(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris, the capital city of France, is renowned for its rich history, art, fashion, and culture. It is often referred to as \"The City of Light\" due to its influential role during the Age of Enlightenment and its early adoption of street lighting.\n",
      "\n",
      "Paris is home to numerous iconic landmarks, including the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, which houses the famous Mona Lisa painting. The city is also known for its beautiful parks, such as the Luxembourg Gardens and the Tuileries Garden.\n",
      "\n",
      "Paris is a hub for the arts, with numerous theaters, galleries, and museums. The city hosts several prestigious events, such as the Paris Fashion Week and the Cannes Film Festival.\n",
      "\n",
      "The city's cuisine is also world-renowned, with a variety of traditional French dishes, such as croissants, baguettes, and escargot. Paris is also famous for its wine and cheese, with numerous wine bars and cheese shops scattered throughout the city.\n",
      "\n",
      "Paris is a popular tourist destination, attracting millions of visitors each year. The city's rich history, culture, and architecture make it a must-visit destination for travelers from around the world.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"message\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "schema = json.dumps(CityDescription.model_json_schema(), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"properties\": {\n",
      "    \"city\": {\n",
      "      \"title\": \"City\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"country\": {\n",
      "      \"title\": \"Country\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"description\": {\n",
      "      \"title\": \"Description\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"city\",\n",
      "    \"country\",\n",
      "    \"description\"\n",
      "  ],\n",
      "  \"title\": \"CityDescription\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:04:01 async_llm_engine.py:174] Added request d320674c42704b9a94a77aa9289aa97b.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:04:01 metrics.py:406] Avg prompt throughput: 1.1 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:04:05,372 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36983799808; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "response = await handle.chat(dialog, SamplingParams(json_schema=schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city='Paris' country='France' description=\"Paris, often referred to as 'The City of Light', is the capital city of France and one of the most iconic cities in the world. Known for its rich history, art, fashion, and culture, Paris is home to numerous world-renowned landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, which houses the famous Mona Lisa. The city is also famous for its café culture, gourmet cuisine, and romantic ambiance. Paris is a major global center for art, fashion, gastronomy, and culture, and its 19th-century cityscape is crisscrossed by wide boulevards and the River Seine. The city's 19th-century cityscape is crisscrossed by wide boulevards and the River Seine. The city's 19th-century cityscape is crisscrossed by wide boulevards and the River Seine. The city's 19th-century cityscape is crisscrossed by wide boulevards and the River Seine.\"\n"
     ]
    }
   ],
   "source": [
    "print(CityDescription.model_validate_json(response[\"message\"].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:06:45,559 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36970754048; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "dialog = ChatDialog(\n",
    "    messages=[\n",
    "        ChatMessage(\n",
    "            role=\"user\",\n",
    "            content=\"What is Pi? Give me the first 15 digits. Only the first 15 digits.\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:06:46 async_llm_engine.py:174] Added request 6858ff8d95b54951acce4b4a7ae0d850.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:06:46 metrics.py:406] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 10.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:06:47 async_llm_engine.py:141] Finished request 6858ff8d95b54951acce4b4a7ae0d850.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:06:50 async_llm_engine.py:174] Added request f269f9165ae74f969ab1a1953bc059de.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:06:51 metrics.py:406] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "response = await handle.chat(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pi (π) is a mathematical constant representing the ratio of a circle's circumference to its diameter. It is an irrational number, meaning it cannot be exactly expressed as a simple fraction and its decimal representation goes on infinitely without repeating. The first 15 digits of Pi are: 3.141592653589793.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"message\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:06:55,572 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36970688512; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:06:56 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 51.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.5%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:07:01 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 10.0%, CPU KV cache usage: 0.0%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2024-08-30 13:07:05,584 E 2771744 2771781] (raylet) file_system_monitor.cc:111: /tmp/ray/session_2024-08-30_13-02-13_610467_2771290 is over 95% full, available space: 36970610688; capacity: 983347249152. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:07:06 metrics.py:406] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.2%, CPU KV cache usage: 0.0%.\n",
      "\u001b[36m(ServeReplica:vllm_deployment:VLLMDeployment pid=2776751)\u001b[0m INFO 08-30 13:07:09 async_llm_engine.py:141] Finished request f269f9165ae74f969ab1a1953bc059de.\n"
     ]
    }
   ],
   "source": [
    "response = await handle.chat(\n",
    "    dialog,\n",
    "    SamplingParams(regex_string=\"(-)?(0|[1-9][0-9]*)(\\\\.[0-9]+)?([eE][+-][0-9]+)?\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14159265358979323846264338327950288419716939937510582097494459230781640628620899862803482534211706798214808651328230664709384460955058223172535940812848111745028410270193852110555964462294895493038196442881097566593344612847564823378678316527120190914564856692346034861045432664821339360726024914127372458700660631558817488152092096282925409171536436789259036001133053054882046652138414695194151160943305727036575959195309218611738193261179310511854807446237996274956735188575272489122793818301194912983367336244065664308602139494639522473719070217986094370277053921717629317675238467481846766940513200056812714526356082778577134275778960917363717872146844090122495343014654958537105079227968925892354201995611212902196086403441815981362977477130996051870721134999999837297804995105973173281609631859502445945534690830264252230825334468503526193118817101000313783875288658753320838142061717766914730359825349042875546873115956286388235378759375195778185778053217122680661300192787661119590921642019893809525720106548586327\n"
     ]
    }
   ],
   "source": [
    "print(response[\"message\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aana-vIr3-B0u-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
