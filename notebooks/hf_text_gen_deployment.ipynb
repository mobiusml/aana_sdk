{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Text Generation Integration with Aana SDK\n",
    "The notebook shows how to run LLMs with Hugging Face Transformers and Aana SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Aana SDK and connect to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/aana_sdk/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-22 11:33:41,911\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-04-22 11:33:45,123\tWARNING utils.py:594 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2025-04-22 11:33:45,128\tWARNING utils.py:606 -- Ray currently does not support initializing Ray with fractional cpus. Your num_cpus will be truncated from 20.4 to 20.\n",
      "2025-04-22 11:33:46,237\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO 2025-04-22 11:33:48,978 serve 120992 -- Started Serve in namespace \"serve\".\n"
     ]
    }
   ],
   "source": [
    "from aana.sdk import AanaSDK\n",
    "\n",
    "aana_app = AanaSDK().connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy Gemma-3 model from Hugging Face Transformers as Aana Deployment. We deploy the model with quantization to speed up the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-22 11:33:54,400 serve 120992 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n",
      "WARNING 2025-04-22 11:33:54,403 serve 120992 -- The new client HTTP config differs from the existing one in the following fields: ['location']. The new HTTP config is ignored.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from aana.deployments.hf_text_generation_deployment import (\n",
    "    HfTextGenerationConfig,\n",
    "    HfTextGenerationDeployment,\n",
    ")\n",
    "\n",
    "hf_text_generation_deployment = HfTextGenerationDeployment.options(\n",
    "    num_replicas=1,  # The number of replicas of the model to deploy\n",
    "    ray_actor_options={\n",
    "        \"num_gpus\": 1\n",
    "    },  # Allocate 1 GPU, should be > 0 if the model requires GPU\n",
    "    user_config=HfTextGenerationConfig(\n",
    "        model_id=\"google/gemma-3-1b-it\",  # The model ID from the Hugging Face model hub\n",
    "        model_kwargs={\n",
    "            \"trust_remote_code\": True,  # Required for this particular model\n",
    "            \"quantization_config\": BitsAndBytesConfig(  # Quantization configuration for the model, we are using 8-bit quantization\n",
    "                load_in_8bit=True,\n",
    "            ),\n",
    "        },\n",
    "    ).model_dump(mode=\"json\"),\n",
    ")\n",
    "\n",
    "aana_app.register_deployment(\n",
    "    name=\"hf_llm\",  # Name of the deployment, which will be using to access the deployment\n",
    "    instance=hf_text_generation_deployment,  # Instance of the deployment that we just created above\n",
    "    deploy=True,  # Tell Aana to deploy the component immediately instead of waiting aana_app.deploy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create AanaDeploymentHandle to connect to the deployment. We use the same name `hf_llm` that we used while deploying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-04-22 11:34:04,721 serve 120992 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7a4ff6a4de40>.\n"
     ]
    }
   ],
   "source": [
    "from aana.deployments.aana_deployment_handle import AanaDeploymentHandle\n",
    "\n",
    "handle = await AanaDeploymentHandle.create(\"hf_llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HfTextGenerationDeployment` can be used to generate text from the model given fully formed prompt with chat template already applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<bos><start_of_turn>user\\nCan you provide ways to eat combinations of bananas and dragonfruits?<end_of_turn><start_of_turn>model\\nSure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.<end_of_turn><start_of_turn>user\\nWhat about solving an 2x + 3 = 7 equation?<end_of_turn><start_of_turn>model\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's tackle that 2x + 3 = 7 equation! This is a classic example of a step-by-step solution. Here's how we'll break it down:\n",
      "\n",
      "**1. Isolate the Variable:**\n",
      "\n",
      "* The goal is to get 'x' by itself on one side of the equation.\n",
      "* Subtract 3 from both sides: 2x + 3 - 3 = 7 - 3\n",
      "* Simplify: 2x = 4\n",
      "\n",
      "**2. Solve for x:**\n",
      "\n",
      "* Divide both sides by 2: 2x / 2 = 4 / 2\n",
      "* Simplify: x = 2\n",
      "\n",
      "**Therefore, the solution to the equation 2x + 3 = 7 is x = 2.**\n",
      "\n",
      "**Let's check our answer:**\n",
      "\n",
      "* 2(2) + 3 = 4 + 3 = 7  (This matches the original equation!)\n",
      "\n",
      "**Key Concepts Used:**\n",
      "\n",
      "* **Inverse Operations:**  We use subtraction (like subtracting 3) to undo the addition.\n",
      "* **Equality:**  We're trying to get the equation to *equal* a specific value (7).\n",
      "\n",
      "**Let"
     ]
    }
   ],
   "source": [
    "async for item in handle.generate_stream(prompt=prompt):\n",
    "    print(item[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Okay, let's tackle that 2x + 3 = 7 equation! This is a classic example of a step-by-step solution. Here's how we'll break it down:\\n\\n**1. Isolate the Variable:**\\n\\n* The goal is to get 'x' by itself on one side of the equation.\\n* Subtract 3 from both sides: 2x + 3 - 3 = 7 - 3\\n* Simplify: 2x = 4\\n\\n**2. Solve for x:**\\n\\n* Divide both sides by 2: 2x / 2 = 4 / 2\\n* Simplify: x = 2\\n\\n**Therefore, the solution to the equation 2x + 3 = 7 is x = 2.**\\n\\n**Let's check our answer:**\\n\\n* 2(2) + 3 = 4 + 3 = 7  (This matches the original equation!)\\n\\n**Key Concepts Used:**\\n\\n* **Inverse Operations:**  We use subtraction (like subtracting 3) to undo the addition.\\n* **Equality:**  We're trying to get the equation to *equal* a specific value (7).\\n\\n**Let\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await handle.generate(prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also give `HfTextGenerationDeployment` a list of messages and it will apply chat template automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aana SDK provides ChatDialog class to form dialog object from the list of messages. This dialog object then can be passed to `HfTextGenerationDeployment` to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aana.core.models.chat import ChatDialog\n",
    "\n",
    "dialog = ChatDialog.from_list(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's solve the equation 2x + 3 = 7. Here's how to do it step-by-step:\n",
      "\n",
      "1. **Subtract 3 from both sides:**\n",
      "   2x + 3 - 3 = 7 - 3\n",
      "   2x = 4\n",
      "\n",
      "2. **Divide both sides by 2:**\n",
      "   2x / 2 = 4 / 2\n",
      "   x = 2\n",
      "\n",
      "**Therefore, the solution is x = 2**\n",
      "\n",
      "Let me know if you'd like to try another equation!做到قدمه\n",
      "\n",
      "\n",
      "\n",
      "Okay, let's tackle that equation:\n",
      "\n",
      "**2x + 3 = 7**\n",
      "\n",
      "Here's how to solve it:\n",
      "\n",
      "1. **Subtract 3 from both sides:**\n",
      "   2x + 3 - 3 = 7 - 3\n",
      "   2x = 4\n",
      "\n",
      "2. **Divide both sides by 2:**\n",
      "   2x / 2 = 4 / 2\n",
      "   x = 2\n",
      "\n",
      "**Therefore, the solution is x = 2**\n",
      "\n",
      "Let me know if you'd like to try another equation!\n"
     ]
    }
   ],
   "source": [
    "async for item in handle.chat_stream(dialog=dialog):\n",
    "    print(item[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': ChatMessage(content=\"Okay, let's solve the equation 2x + 3 = 7. Here's how to do it step-by-step:\\n\\n1. **Subtract 3 from both sides:**\\n   2x + 3 - 3 = 7 - 3\\n   2x = 4\\n\\n2. **Divide both sides by 2:**\\n   2x / 2 = 4 / 2\\n   x = 2\\n\\n**Therefore, the solution is x = 2**\\n\\nLet me know if you'd like to try another equation!做到قدمه\\n\\n\\n\\nOkay, let's tackle that equation:\\n\\n**2x + 3 = 7**\\n\\nHere's how to solve it:\\n\\n1. **Subtract 3 from both sides:**\\n   2x + 3 - 3 = 7 - 3\\n   2x = 4\\n\\n2. **Divide both sides by 2:**\\n   2x / 2 = 4 / 2\\n   x = 2\\n\\n**Therefore, the solution is x = 2**\\n\\nLet me know if you'd like to try another equation!\\n\", role='assistant')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await handle.chat(dialog=dialog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully deployed an LLM using Aana SDK. You can add Aana Endpoints to your application to interact with the deployed model.\n",
    "\n",
    "Aana SDK also provides OpenAI-compatible API to interact with the deployed model. It allows you to access the Aana applications with any OpenAI-compatible client. See [OpenAI-compatible API docs](/docs/pages/openai_api.md) for more details.\n",
    "\n",
    "You can also deploy LLMs using [vLLM integration](/docs/pages/integrations.md#vllm) with Aana SDK. It is a more efficient way to deploy LLMs if you have a GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
