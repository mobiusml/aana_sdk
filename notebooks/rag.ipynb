{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation using Haystack and Aana SDK\n",
    "\n",
    "This notebook demonstrates how to use Haystack and Aana SDK to build an application to answer user's quries about videos with Retrieval Augmented Generation (RAG).\n",
    "\n",
    "The application works as follows:\n",
    "- Whisper model is used to transcribe the video.\n",
    "- The transcribed text is split into chunks and an embedding is generated for each chunk.\n",
    "- Chunks and their embeddings are indexed in a datastore.\n",
    "- When a user asks a question, the question is used to retrieve relevant chunks from the datastore.\n",
    "- The retrieved chunks are used to generate a prompt.\n",
    "- The prompt is passed to an LLM to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Aana SDK and connect to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/aana-vIr3-B0u-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-26 10:02:05,158\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/root/.cache/pypoetry/virtualenvs/aana-vIr3-B0u-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_dir\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "2024-06-26 10:02:10,595\tWARNING services.py:2009 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-06-26 10:02:10,748\tINFO worker.py:1740 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from aana.sdk import AanaSDK\n",
    "\n",
    "aana_app = AanaSDK().connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to deploy a Whisper model and an LLM model. We will use predefined `WhisperDeployment` and `HfTextGenerationDeployment` classes to deploy these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/aana-vIr3-B0u-py3.10/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "The new client HTTP config differs from the existing one in the following fields: ['location']. The new HTTP config is ignored.\n",
      "2024-06-26 10:02:25,266\tINFO handle.py:126 -- Created DeploymentHandle '6283397g' for Deployment(name='WhisperDeployment', app='asr_deployment').\n",
      "2024-06-26 10:02:25,268\tINFO handle.py:126 -- Created DeploymentHandle '52rx2zc8' for Deployment(name='WhisperDeployment', app='asr_deployment').\n",
      "2024-06-26 10:02:40,422\tINFO handle.py:126 -- Created DeploymentHandle '00dcwn9z' for Deployment(name='WhisperDeployment', app='asr_deployment').\n",
      "2024-06-26 10:02:40,425\tINFO api.py:584 -- Deployed app 'asr_deployment' successfully.\n"
     ]
    }
   ],
   "source": [
    "from aana.deployments.whisper_deployment import (\n",
    "    WhisperComputeType,\n",
    "    WhisperConfig,\n",
    "    WhisperDeployment,\n",
    "    WhisperModelSize,\n",
    ")\n",
    "\n",
    "asr_deployment = WhisperDeployment.options(\n",
    "    num_replicas=1,\n",
    "    ray_actor_options={\"num_gpus\": 0.25},\n",
    "    user_config=WhisperConfig(\n",
    "        model_size=WhisperModelSize.MEDIUM,\n",
    "        compute_type=WhisperComputeType.FLOAT16,\n",
    "    ).model_dump(mode=\"json\"),\n",
    ")\n",
    "\n",
    "aana_app.register_deployment(\n",
    "    name=\"asr_deployment\",\n",
    "    instance=asr_deployment,\n",
    "    deploy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new client HTTP config differs from the existing one in the following fields: ['location']. The new HTTP config is ignored.\n",
      "2024-06-26 10:02:40,509\tINFO handle.py:126 -- Created DeploymentHandle 'tp3nze3w' for Deployment(name='HfTextGenerationDeployment', app='llm_deployment').\n",
      "2024-06-26 10:02:40,511\tINFO handle.py:126 -- Created DeploymentHandle 'hsfoe5jo' for Deployment(name='HfTextGenerationDeployment', app='llm_deployment').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:03:09,807\tINFO handle.py:126 -- Created DeploymentHandle '2f015cnh' for Deployment(name='HfTextGenerationDeployment', app='llm_deployment').\n",
      "2024-06-26 10:03:09,808\tINFO api.py:584 -- Deployed app 'llm_deployment' successfully.\n"
     ]
    }
   ],
   "source": [
    "from aana.deployments.hf_text_generation_deployment import (\n",
    "    HfTextGenerationConfig,\n",
    "    HfTextGenerationDeployment,\n",
    ")\n",
    "\n",
    "hf_text_generation_deployment = HfTextGenerationDeployment.options(\n",
    "    num_replicas=1,\n",
    "    ray_actor_options={\"num_gpus\": 0.5},\n",
    "    user_config=HfTextGenerationConfig(\n",
    "        model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        model_kwargs={\n",
    "            \"trust_remote_code\": True,\n",
    "        },\n",
    "    ).model_dump(mode=\"json\"),\n",
    ")\n",
    "aana_app.register_deployment(\n",
    "    name=\"llm_deployment\",\n",
    "    instance=hf_text_generation_deployment,\n",
    "    deploy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aana SDK provides a deployment class for Haystack components, `HaystackComponentDeployment`. `HaystackComponentDeployment` is a class that allows to deploy Haystack components as a separate deployment. This is quite useful for deploying components that represent deep learning models. This has a few advantages:\n",
    "- It allows to deploy the model only once and reuse it from multiple Haystack Pipelines. This leads to more efficient resource usage like GPU memory.\n",
    "- It allows you to scale Haystack Pipelines to a cluster of machines with minimal effort. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy text embedder and document embdedder that we will be using to build Haystack pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new client HTTP config differs from the existing one in the following fields: ['location']. The new HTTP config is ignored.\n",
      "2024-06-26 10:03:10,040\tINFO handle.py:126 -- Created DeploymentHandle '0e3kkek3' for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment').\n",
      "2024-06-26 10:03:10,041\tINFO handle.py:126 -- Created DeploymentHandle 'v853vgjg' for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment').\n",
      "2024-06-26 10:03:21,107\tINFO handle.py:126 -- Created DeploymentHandle '92o499mo' for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment').\n",
      "2024-06-26 10:03:21,108\tINFO api.py:584 -- Deployed app 'text_embedder_deployment' successfully.\n"
     ]
    }
   ],
   "source": [
    "from aana.deployments.haystack_component_deployment import (\n",
    "    HaystackComponentDeployment,\n",
    "    HaystackComponentDeploymentConfig,\n",
    ")\n",
    "\n",
    "text_embedder_deployment = HaystackComponentDeployment.options(\n",
    "    num_replicas=1,\n",
    "    ray_actor_options={\"num_gpus\": 0.1},\n",
    "    user_config=HaystackComponentDeploymentConfig(\n",
    "        component=\"haystack.components.embedders.SentenceTransformersTextEmbedder\",\n",
    "        params={\"model\": \"sentence-transformers/all-mpnet-base-v2\"},\n",
    "    ).model_dump(),\n",
    ")\n",
    "aana_app.register_deployment(\n",
    "    name=\"text_embedder_deployment\",\n",
    "    instance=text_embedder_deployment,\n",
    "    deploy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:03:21,120\tWARNING deployment.py:417 -- DeprecationWarning: `max_concurrent_queries` in `@serve.deployment` has been deprecated and replaced by `max_ongoing_requests`.\n",
      "The new client HTTP config differs from the existing one in the following fields: ['location']. The new HTTP config is ignored.\n",
      "2024-06-26 10:03:21,131\tINFO handle.py:126 -- Created DeploymentHandle '04jrkkcw' for Deployment(name='HaystackComponentDeployment', app='document_embedder_deployment').\n",
      "2024-06-26 10:03:21,131\tINFO handle.py:126 -- Created DeploymentHandle '2m5s1s16' for Deployment(name='HaystackComponentDeployment', app='document_embedder_deployment').\n",
      "2024-06-26 10:03:33,231\tINFO handle.py:126 -- Created DeploymentHandle 'tqe6usks' for Deployment(name='HaystackComponentDeployment', app='document_embedder_deployment').\n",
      "2024-06-26 10:03:33,232\tINFO api.py:584 -- Deployed app 'document_embedder_deployment' successfully.\n"
     ]
    }
   ],
   "source": [
    "document_embedder_deployment = HaystackComponentDeployment.options(\n",
    "    num_replicas=1,\n",
    "    max_concurrent_queries=1000,\n",
    "    ray_actor_options={\"num_gpus\": 0.1},\n",
    "    user_config=HaystackComponentDeploymentConfig(\n",
    "        component=\"haystack.components.embedders.SentenceTransformersDocumentEmbedder\",\n",
    "        params={\"model\": \"sentence-transformers/all-mpnet-base-v2\"},\n",
    "    ).model_dump(),\n",
    ")\n",
    "aana_app.register_deployment(\n",
    "    name=\"document_embedder_deployment\",\n",
    "    instance=document_embedder_deployment,\n",
    "    deploy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have deployed all the necessary models, we can build two Haystack pipelines:\n",
    "- Indexing pipeline: This pipeline will be used to split the transcribed text into chunks and index them in a datastore. The transcription step will be done outside the pipeline but if you really want it to be a part of the pipeline, you can create custom Haystack components for it.\n",
    "- Query pipeline: This pipeline will be used to retrieve relevant chunks from the datastore and generate an answer using the LLM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to transcribe the video. We will download the video and extract the audio from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aana.core.models.video import VideoInput\n",
    "\n",
    "video_input = VideoInput(url=\"https://www.youtube.com/watch?v=UQuIVsNzqDk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=UQuIVsNzqDk\n",
      "[youtube] UQuIVsNzqDk: Downloading webpage\n",
      "[youtube] UQuIVsNzqDk: Downloading ios player API JSON\n",
      "[youtube] UQuIVsNzqDk: Downloading android player API JSON\n"
     ]
    }
   ],
   "source": [
    "from aana.integrations.external.yt_dlp import download_video\n",
    "from aana.processors.video import extract_audio\n",
    "\n",
    "video = download_video(video_input=video_input)\n",
    "audio = extract_audio(video=video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already deployed the Whisper model for ASR. Now we need to create a handle that we can use to interact with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:03:42,922\tINFO handle.py:126 -- Created DeploymentHandle '5at1j4b1' for Deployment(name='WhisperDeployment', app='asr_deployment').\n",
      "2024-06-26 10:03:42,937\tINFO pow_2_scheduler.py:260 -- Got updated replicas for Deployment(name='WhisperDeployment', app='asr_deployment'): {'9ueivx35'}.\n"
     ]
    }
   ],
   "source": [
    "from aana.deployments.aana_deployment_handle import AanaDeploymentHandle\n",
    "\n",
    "asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the audio to the Whisper model to transcribe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:03:42,966\tINFO handle.py:126 -- Created DeploymentHandle 'ubtapa51' for Deployment(name='WhisperDeployment', app='asr_deployment').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Do you ever feel like visual effects in old movies were better? What if I told you that wasn't just nostalgia speaking? Back in the 1960s, Disney invented a technology that was in many ways superior to the green screen. But that tech has long since been forgotten. And what if I told you that we found a way to recreate it? Being able to layer one moving image over another is the fundamental building block of visual effects. Every single crazy effect shot from every movie you love relies on this basic core technique. And the primary way we do that is with green screen. Or blue screen. But there are lots of problems with green screen. Even in this modern era, you can't film blurry or transparent things. You can't wear clothes that are the same color as the screen. And the spill of the color oftentimes ruins footage. If I wanted to make a movie about a clown wearing all the colors of the rainbow getting married on Mars, I can't. And that bothers me. If I could get my hands on an invention that didn't have any of these issues, it would be like a filmmaking superpower. It would be like magic. Do you really think so? What's special about that hat? It's transparent. It's transparent. It's transparent. I thought the stuff I was seeing on screen was impossible. They're keying a veil. They're keying salt, smoke. They're even wearing blue and green clothes. This movie from 1964 broke every rule of chroma keying. And they did it all without computers. So how do they do it? To explain the science behind the magic, we are joined by Dr. Paul Dubevich. The sodium vapor process. Instead of a blue screen or a green screen, they used what was sometimes called a yellow screen. There's a very specific spectrum of yellow made from a low pressure sodium vapor light that puts out one wavelength of light at 589 nanometers. Because it's just one wavelength and because of how dichroic filters work, you can actually block just that one wavelength or let just that one wavelength go through. So the magic of the sodium vapor process is they used a beam splitter prism so that the light that comes through the lens gets split onto two strips of film at the same time. The sodium vapor wavelength of 589 nanometers reflects out this way and all the rest of the spectrum goes through and can expose a color image of the actor. And that's exactly what you need to get your map. What's kind of magical about it is that you can block that one wavelength of yellow without messing up all of the other colors. In that scene, there's no matte lines. He's motion blur. He's dancing around. The compositing is impeccable. Yeah, it's perfect. You couldn't have gotten such great blur over the alpha channel with a green screen or a blue screen. It seems too good to be true. Why are we not still using this technology today? Yeah, really? What gives? Because they were never able to replicate prisms. I've heard that Disney was only able to produce one prism. Is that true? They had to join two pieces of glass and then have layers of material with different indices of refraction. It's a very custom job and it would probably cost tens of thousands of dollars at a minimum. Apparently, there were three of these ever made. And also, we don't know where any of these prisms are today. And that means that I'm never going to get to answer this burning question I have. Is sodium vapor better than green screen? The science, the physics, they all tell me it is. But because the prisms haven't lost the time, I'm never going to get to realize my dream of a clown getting married on Mars. And then one day, I got a message from Paul De Bevec. He had done what I thought was impossible. He had recreated Disney's magic prism and he needed somebody to test it out. This will be the first test of the sodium vapor process in over 30 years. Well, I guess this is like our hello world of sodium vapor matting. We've got our color image and our sodium vapor image there. There it is. Look at that. That's the idea. It's working. The first thing you need are some sodium vapor lights. So thankfully, they still make the bulbs. Eventually, after we've had these on for 10 minutes, they'll start glowing that color. I think it's working. And then we've got a couple of LED lights here that are going to illuminate the actor. Sodium vapor mats. Woohoo. Hey. Pretty sweet. How is this working? How did Paul De Bevec manage to resurrect Disney's lost prism? Well, with a deep understanding of the science of light and a little bit of creativity. I'll do a beam splitter, but I'll just do a regular beam splitter like this beam splitter. And then we'll filter the light after it comes out of the prism. Paul recreated the physics of the sodium vapor process, but he did it with all off-the-shelf components. Instead of a custom beam splitter, he used two filters. And instead of two strips of film, he just used two cameras. If you ever think your rigs are janky, they're all janky. For the first time ever, my dreams are within my grasp. Want to know what's within your grasp? A beautifully designed website, thanks to our sponsor, Squarespace. Whether you're an artist, an entrepreneur, or dreaming of starting something new, Squarespace is your canvas. Building a unique online presence has never been easier, thanks to Squarespace Blueprint, their new guided design system that lets you choose layout and styling options tailored to your business. And thanks to their optimized SEO tools, you'll show up in more searches and get discovered fast. Good luck to whoever's going to key this footage. Offer your customers a seamless checkout that accepts credit cards, PayPal, Apple Pay, and ineligible countries after paying clear pay. There's no way this is going to get chroma keyed. With Squarespace's fluid engine, creativity knows no bounds. Start with a template and then make it entirely yours. Drag, drop, and design your heart out on any device. So what's stopping you? Head to squarespace.com for a free trial, and when you are ready to launch, visit squarespace.com slash corridor crew to save 10% off your first purchase of a website or a domain. With squarespace.com, your dream is just a click away. All right, now let's find out what I'm going to do with all these hard to key clothes. Paul, I would like to tell you about this character. Every time I approach a production company with it, they're like, no, can't do it, it's impossible. Okay. And it's a story about a clown that's trying to get married. You look so dumb. Me? What? Look, I've heard big things about this magic crystal, okay? But this is ridiculous. That's impossible to key. Look at that. I hate to break it to you, but this video is going to be a disappointment. JC, I feel for you to come on out. It's my wedding day. Oh my god. That would be difficult to key, wouldn't it? It would. I forgot to mention that the clown's getting married on Mars, hence the need to replace the background. Oh, of course, yeah. I think this is going to be fantastic. I suppose we should clean up the lighting. Yeah. Kill all of the full spectrum lighting. We need that one covered. Yeah. I need to cover one of the skylights so that we don't have all this daylight spill coming in. And my dress is like Marilyn Monroe-ing right now. Keep the side down. It's the best a clown can do. So as you can see right now, we're spending a lot of time getting these flags up because what we don't want to have happen is we don't want the sodium vapor light to be hitting JC at all because then it's going to be basically turning her transparent. So this is a spectral light meter. It tells you the spectrum of the light. For example, this light here, that shows it's LED lights. It's made out of red, green, and blue LEDs to make the color white. I can also take it and put it in front of the sodium vapor lights. There is our sodium vapor spike at 589 nanometers. So let's see if there's any sodium vapor light hitting our subject. Mostly good, but if I look closely here, I can see a tiny little blip. See that tiny little blip of sodium vapor? Yeah, science. We're very close, but we're getting a little bit of side spill on her. And we're just cleaning that up here. If we're the first ones to do this in like 30 years, I want to do it right, you know? It went down. Nice. That should be better. Looks like we almost have a perfect mat. I guess the only thing left to do is to shoot it. Maybe we should shoot. Cool, go ahead and roll the cameras. Here we are on Mars. The clown is looking for the groom. Uh-oh, there's aliens behind you. Maybe turn around, wave at the aliens. I want to get one shot of myself as well. Wearing my green hoodie, which I refuse to take off. And hopefully the sodium vapor process lets us do something. Nico, I want to see you do some head banging. I want you to throw that head around. I think we got it. That would be the hardest green screen shot. That would be so hard to do on a green screen. And cut. Cut. Dude, this is going to be as successful as Mary Poppins. Oh my God, wow. Paul's research has been super influential on me. To get to work with him on an experiment is super, super cool. I hope this works. All right, let's take a look at what we got. Here's what our color footage looks like. And if I look at the sodium vapor shot, wow, it looks pretty good. Let's see what happens if I take that and turn it into a black and white transparency mask. Wow, this should work really well. But before I try pulling a transparency key here, I need to also try this with green screen to do a true comparison to know if this is better. I'm using all the tricks or perfect green screen that I've acquired over the years. Lights with a hint of green on them. Lights on Jordan with a hint of magenta to cancel out the green spill from the green screen. I'm going to do my best to light this green screen and shoot it as good as I can. All right, hi, you're on Mars. Wave to us, the audience. So we all know that sodium vapor should give scientifically better results. But green screen tools have had years and years and years to mature. I can start pulling in the thickness of that matte. But while the dress starts to come back, the veil now has these ugly lines. I can try to, you know, close the holes in the dress using a different tool here. But it patches up the holes in the veil. There's just no way to handle the range of greens here. And for green screen tools to know what we want to keep and what we don't want to keep. Oh, no. Yuck. Destroyed. Ew. Actually not that bad. Be honest, did you do your best with the key? I did. The issue is that I can't get the transparency of the veil and like the slightly off green dress. In fact, the only way you could do a shot like this is to go in and cut out all these different sections by hand and have a bunch of patchwork different solutions in the image. If the sodium vapor process is superior, it won't have any of these issues. All right, so it's time to try the sodium vapor process. Do the composite and see if it works. Here we have JC and here we have our background, right? And the way we're going to do this actually mirrors the way they did it on film back in the day. So the thing is, if you just take two pieces of film and you layer them over each other, you end up with a double exposure. You need something called a holdout matte that leaves a hole for you to simply add your other image on top. So we're going to take our background and we're going to subtract the matte. We're going to take our foreground and we're going to subtract everything that isn't the matte. Now we take our background and our foreground and we simply add them together. Honestly, I'm really excited to see this because it felt like we were doing one of the coolest, most high tech things we've ever done. And I would love for this to pay off. Here are the results and I'll let you guys be the judge. Whoa. Wow. That is wild. Dude, it's Mary Poppins. This is amazing. It's incredible. No, you dog. Water. He's drinking water on Mars. This is my favorite one. The water is just such a flags. Check out the motion blur. Oh, the hands. Oh, wow. The motion blur drops the color exposure correctly on the background right there. It turns it red. Oh, correct math. Oh my God. It's so good. Dude, those little tiny hairs on top. Oh my God. I didn't have to do any work. You didn't have to tweak your like white clip black. No, I hate that. De-spill bias. No, there's no spill. What about your edge feather? No edge feather. No de-spill. No messing with gamma over your mat. No thresholds. No restoring fringe. No cleaning the blacks. No cleaning the whites. Nothing. Just turn it on. Just turn it on. Wow. Good job. That is amazing. I'm Mary Poppins, y'all. So I just got the opinion of everybody at Corridor, but I need to get the opinion that actually really matters. Oh, wow. You're zooming in on it. You are not afraid of your mat lines. That was clearly exactly what needed to be filmed to show off this technique. Exactly. What about a volume? Couldn't you use a volume? In today's age, when you could just go and shoot on a volume with an LED screen, is sodium vapor worth pursuing? In practice, a lot of those in-camera backgrounds will get replaced. So it seems like we're still in a world where we need to be able to cut out backgrounds and put people on new things. Yeah, flexibility in post-production is incredibly valuable. One thing that you're noticing is like composing tools are getting better because they're starting to have a bit of machine learning inside of them. Machine learning needs training data. The question is, where do you get all that training data? There is no perfect, easy compositing technique, and I thought this would be a good way to do it. So sodium vapor is another one of those essential steps in this progress towards having perfect transparency for compositing and visual effects then. Absolutely. It is the gold standard, the yellow standard. Well, Paul, thank you once again for joining us here on the Corridor channel. If you like these deep dives into classic visual effects technology along with industry experts talking about it, you'll definitely love our Abyss video. So go check that out. And yeah, thank you. I really appreciate it. Great to see you. Glad you could join us. This was fun. This is a great time. Totally. Totally.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription_result = await asr_handle.transcribe(audio=audio)\n",
    "transcription = transcription_result[\"transcription\"].text\n",
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a Haystack pipeline to index the transcribed text. \n",
    "\n",
    "We will use Qdrant as the datastore to store the chunks and their embeddings. You need to set up Qdrant before running this notebook. You can find the instructions to install Qdrant [here](https://qdrant.tech/documentation/guides/installation/). Alternatively, you can use [Qdrant Cloud](https://cloud.qdrant.io) or use the following one-liner to run Qdrant locally (not recommended for production use, only for testing purposes):\n",
    "\n",
    "```bash\n",
    "curl -L https://github.com/qdrant/qdrant/releases/download/v1.9.7/qdrant-x86_64-unknown-linux-gnu.tar.gz | tar xz && ./qdrant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step of creating a Haystack pipeline is to define the components that will be used in the pipeline. We will use the following components:\n",
    "- `DocumentCleaner`: This component is used to clean the text before splitting it into chunks.\n",
    "- `DocumentSplitter`: This component is used to split the text into chunks.\n",
    "- `DocumentEmbedder`: This component is used to generate embeddings for the chunks. We will use the text embedder that we deployed earlier. For that we need to use `RemoteHaystackComponent` and pass the name of the deployment that we created earlier. Make sure to call `warm_up()` on the component before building the pipeline to initialize the component.\n",
    "- `DocumentWriter`: This component is used to write the chunks and their embeddings to the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:04:37,883\tINFO handle.py:126 -- Created DeploymentHandle '9w7e9514' for Deployment(name='HaystackComponentDeployment', app='document_embedder_deployment').\n",
      "2024-06-26 10:04:37,932\tINFO handle.py:126 -- Created DeploymentHandle 'y8pedui7' for Deployment(name='HaystackComponentDeployment', app='document_embedder_deployment').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:04:37,909\tINFO pow_2_scheduler.py:260 -- Got updated replicas for Deployment(name='HaystackComponentDeployment', app='document_embedder_deployment'): {'01s99fez'}.\n",
      "2024-06-26 10:04:39,593\tINFO pow_2_scheduler.py:260 -- Got updated replicas for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment'): {'v5912u0o'}.\n",
      "2024-06-26 10:04:40,083\tINFO pow_2_scheduler.py:260 -- Got updated replicas for Deployment(name='HfTextGenerationDeployment', app='llm_deployment'): {'tnji8e4l'}.\n"
     ]
    }
   ],
   "source": [
    "from haystack import Document, Pipeline\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.types import DuplicatePolicy\n",
    "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
    "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
    "\n",
    "from aana.deployments.haystack_component_deployment import RemoteHaystackComponent\n",
    "\n",
    "cleaner = DocumentCleaner()\n",
    "splitter = DocumentSplitter(split_by=\"sentence\", split_length=1)\n",
    "document_store = QdrantDocumentStore(\n",
    "    url=\"http://localhost:6333\", index=\"video_transcriptions\"\n",
    ")\n",
    "document_embedder = RemoteHaystackComponent(\"document_embedder_deployment\")\n",
    "writer = DocumentWriter(document_store=document_store, policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "document_embedder.warm_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a pipeline using these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fb31863b880>\n",
       "🚅 Components\n",
       "  - cleaner: DocumentCleaner\n",
       "  - splitter: DocumentSplitter\n",
       "  - document_embedder: RemoteHaystackComponent\n",
       "  - writer: DocumentWriter\n",
       "🛤️ Connections\n",
       "  - cleaner.documents -> splitter.documents (List[Document])\n",
       "  - splitter.documents -> document_embedder.documents (List[Document])\n",
       "  - document_embedder.documents -> writer.documents (List[Document])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexing_pipeline = Pipeline()\n",
    "\n",
    "indexing_pipeline.add_component(\"cleaner\", cleaner)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"document_embedder\", document_embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "indexing_pipeline.connect(\"cleaner.documents\", \"splitter.documents\")\n",
    "indexing_pipeline.connect(\"splitter.documents\", \"document_embedder.documents\")\n",
    "indexing_pipeline.connect(\"document_embedder.documents\", \"writer.documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the pipeline to index the transcribed text we got from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:04:38,117\tINFO handle.py:126 -- Created DeploymentHandle '10r9q4i8' for Deployment(name='HaystackComponentDeployment', app='document_embedder_deployment').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [00:00, 395.35it/s]                         \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 253}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcription_doc = Document(content=transcription)\n",
    "result = indexing_pipeline.run({\"cleaner\": {\"documents\": [transcription_doc]}})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the indexing pipeline should tell you that there are some documents added to the datastore. That means the indexing was successful.\n",
    "\n",
    "Now we can create a query pipeline to answer user's questions.\n",
    "\n",
    "We will use the following components in the query pipeline:\n",
    "- `TextEmbedder`: This component is used to generate embeddings for the question. We will use the text embedder that we deployed earlier. For that we need to use `RemoteHaystackComponent` and pass the name of the deployment that we created earlier. Make sure to call `warm_up()` on the component before building the pipeline to initialize the component.\n",
    "- `QdrantEmbeddingRetriever`: This component is used to retrieve relevant chunks from the datastore given embeddings.\n",
    "- `PromptBuilder`: This component is used to generate a prompt from the retrieved documents based on the provided template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:04:39,583\tINFO handle.py:126 -- Created DeploymentHandle 'p84nhckr' for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment').\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:04:39,653\tINFO handle.py:126 -- Created DeploymentHandle '8myuo6ey' for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fb30d243460>\n",
       "🚅 Components\n",
       "  - text_embedder: RemoteHaystackComponent\n",
       "  - retriever: QdrantEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Given these documents, answer the question.\n",
    "Documents:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "text_embedder = RemoteHaystackComponent(\"text_embedder_deployment\")\n",
    "retriever = QdrantEmbeddingRetriever(document_store=document_store)\n",
    "prompt_builder = PromptBuilder(template=prompt_template)\n",
    "\n",
    "text_embedder.warm_up()\n",
    "\n",
    "query_pipeline = Pipeline()\n",
    "query_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "query_pipeline.add_component(\"retriever\", retriever)\n",
    "query_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "\n",
    "query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "query_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we didn't use the LLM model directly in the pipeline. Instead, we used `PromptBuilder` to generate a prompt and then we will use the LLM model to generate an answer based on the prompt. This is not the only way to do it and we will show you how to use the LLM model directly in the pipeline later.\n",
    "\n",
    "Now let's run the query pipeline to get the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:04:39,683\tINFO handle.py:126 -- Created DeploymentHandle 'epha0907' for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given these documents, answer the question.\n",
      "Documents:\n",
      "\n",
      "     The sodium vapor process.\n",
      "\n",
      "     So sodium vapor is another one of those essential steps in this progress towards having perfect transparency for compositing and visual effects then.\n",
      "\n",
      "     So we all know that sodium vapor should give scientifically better results.\n",
      "\n",
      "     All right, so it's time to try the sodium vapor process.\n",
      "\n",
      "     If the sodium vapor process is superior, it won't have any of these issues.\n",
      "\n",
      "     Sodium vapor mats.\n",
      "\n",
      "     This will be the first test of the sodium vapor process in over 30 years.\n",
      "\n",
      "     See that tiny little blip of sodium vapor? Yeah, science.\n",
      "\n",
      "     And hopefully the sodium vapor process lets us do something.\n",
      "\n",
      "     So the magic of the sodium vapor process is they used a beam splitter prism so that the light that comes through the lens gets split onto two strips of film at the same time.\n",
      "\n",
      "Question: What is a sodium vapour process?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "question = \"What is a sodium vapour process?\"\n",
    "\n",
    "result = query_pipeline.run(\n",
    "    {\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}}\n",
    ")\n",
    "prompt = result[\"prompt_builder\"][\"prompt\"]\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a prompt now. To send the prompt to the LLM model, we need to create a handle that we can use to interact with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:04:40,068\tINFO handle.py:126 -- Created DeploymentHandle '3omfzzor' for Deployment(name='HfTextGenerationDeployment', app='llm_deployment').\n"
     ]
    }
   ],
   "source": [
    "llm_handle = await AanaDeploymentHandle.create(\"llm_deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM deployment expects `ChatDialog` as an input. We can use the `ChatDialog.from_prompt` method to create a `ChatDialog` object from the prompt and then send it to the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:04:40,124\tINFO handle.py:126 -- Created DeploymentHandle '3qg8qsuh' for Deployment(name='HfTextGenerationDeployment', app='llm_deployment').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': ChatMessage(content='The sodium vapor process is a technique used in compositing and visual effects that involves using sodium vapor to achieve scientifically better results in creating transparent images. It utilizes a beam splitter prism to split light that comes through a lens onto two strips of film simultaneously, allowing for precise and accurate compositing. This process has been considered superior to other methods and is being revisited after a long period of not being used, with the hope that it can contribute to advancements in the field. The sodium vapor process is a photographic technique that was historically used in the film industry to create seamless composites and visual effects. It involves the use of sodium vapor lamps to illuminate a scene, which then emits a distinct yellow light. This light is captured on film, which can be used to isolate and manipulate elements within a scene with high precision. The process is known for its ability to produce clear and accurate results, which is why it was considered a step towards achieving perfect transparency in visual effects. The technique uses a beam splitter prism to project the sodium light onto two separate film strips simultaneously, allowing for detailed compositing work. The sod', role='assistant')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aana.core.models.chat import ChatDialog\n",
    "\n",
    "await llm_handle.chat(dialog=ChatDialog.from_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "We got the answer from the LLM model. But we got in as a single response. We can use `chat_stream` method to stream the tokens from the LLM to get the answer in a more interactive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:04:53,589\tINFO handle.py:126 -- Created DeploymentHandle 'qenercv9' for Deployment(name='HfTextGenerationDeployment', app='llm_deployment').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sodium vapor process is a technique used in compositing and visual effects that involves using sodium vapor to achieve scientifically better results in creating transparent images. It utilizes a beam splitter prism to split light that comes through a lens onto two strips of film simultaneously, allowing for precise and accurate compositing. This process has been considered superior to other methods and is being revisited after a long period of not being used, with the hope that it can contribute to advancements in the field. The sodium vapor process is a photographic technique that was historically used in the film industry to create seamless composites and visual effects. It involves the use of sodium vapor lamps to illuminate a scene, which then emits a distinct yellow light. This light is captured on film, which can be used to isolate and manipulate elements within a scene with high precision. The process is known for its ability to produce clear and accurate results, which is why it was considered a step towards achieving perfect transparency in visual effects. The technique uses a beam splitter prism to project the sodium light onto two separate film strips simultaneously, allowing for detailed compositing work. The sod"
     ]
    }
   ],
   "source": [
    "async for chunk in llm_handle.chat_stream(dialog=ChatDialog.from_prompt(prompt)):\n",
    "    print(chunk[\"text\"], end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! Now we have two pipelines: one for indexing the transcribed text and one for answering user's questions. We used the Whisper model for ASR, the text embedder for generating embeddings, and the LLM model for generating answers. We also used Qdrant as the datastore to store the chunks and their embeddings. We used `PromptBuilder` to generate a prompt and then used the LLM model to generate an answer based on the prompt.\n",
    "\n",
    "Now you can package these pipelines into Aana Endpoints to create an Aana Application. See [tutorial](/docs/pages/tutorial.md) for more details on how to create an Aana Application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I promised before, I will show you how to use the LLM model directly in the pipeline to generate an answer. For that we can use `AanaDeploymentComponent` that allows to wrap any Aana deployments into a Haystack component. We will use `AanaDeploymentComponent` to wrap the LLM deployment and use it in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:05:07,224\tINFO handle.py:126 -- Created DeploymentHandle 'vt8fzelm' for Deployment(name='HfTextGenerationDeployment', app='llm_deployment').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': ChatMessage(content='The sodium vapor process is a technique used in compositing and visual effects that involves using sodium vapor to achieve scientifically better results in creating transparent images. It utilizes a beam splitter prism to split light that comes through a lens onto two strips of film simultaneously, allowing for precise and accurate compositing. This process has been considered superior to other methods and is being revisited after a long period of not being used, with the hope that it can contribute to advancements in the field. The sodium vapor process is a photographic technique that was historically used in the film industry to create seamless composites and visual effects. It involves the use of sodium vapor lamps to illuminate a scene, which then emits a distinct yellow light. This light is captured on film, which can be used to isolate and manipulate elements within a scene with high precision. The process is known for its ability to produce clear and accurate results, which is why it was considered a step towards achieving perfect transparency in visual effects. The technique uses a beam splitter prism to project the sodium light onto two separate film strips simultaneously, allowing for detailed compositing work. The sod', role='assistant')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aana.integrations.haystack.deployment_component import AanaDeploymentComponent\n",
    "\n",
    "llm_component = AanaDeploymentComponent(llm_handle, \"chat\")\n",
    "llm_component.run(dialog=ChatDialog.from_prompt(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us a component for LLM. But the issue is that our LLM deployment expects `ChatDialog` as an input. What we can do is to create a custom component that will take the prompt and generate a `ChatDialog` object from it. See [Creating Custom Components](https://docs.haystack.deepset.ai/docs/custom-components) for more details on how to create custom components in Haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component\n",
    "\n",
    "\n",
    "@component\n",
    "class ChatDialogGenerator:\n",
    "    \"\"\"A component generating a chat dialog from a given prompt.\"\"\"\n",
    "\n",
    "    @component.output_types(dialog=ChatDialog, note=str)\n",
    "    def run(self, prompt: str):\n",
    "        \"\"\"Generate a chat dialog from a given prompt.\"\"\"\n",
    "        dialog = ChatDialog.from_prompt(prompt)\n",
    "        return {\"dialog\": dialog, \"note\": \"chat dialog is generated from the prompt\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can update the query pipeline to use the LLM model directly to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:05:20,456\tINFO handle.py:126 -- Created DeploymentHandle 'xfvlwe9u' for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment').\n",
      "2024-06-26 10:05:20,469\tINFO handle.py:126 -- Created DeploymentHandle 'n1o4brzn' for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fb30d242500>\n",
       "🚅 Components\n",
       "  - text_embedder: RemoteHaystackComponent\n",
       "  - retriever: QdrantEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - chat_dialog_generator: ChatDialogGenerator\n",
       "  - llm: AanaDeploymentComponent\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> chat_dialog_generator.prompt (str)\n",
       "  - chat_dialog_generator.dialog -> llm.dialog (ChatDialog)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embedder = RemoteHaystackComponent(\"text_embedder_deployment\")\n",
    "retriever = QdrantEmbeddingRetriever(document_store=document_store)\n",
    "prompt_builder = PromptBuilder(template=prompt_template)\n",
    "chat_dialog_generator = ChatDialogGenerator()\n",
    "llm_component = AanaDeploymentComponent(llm_handle, \"chat\")\n",
    "\n",
    "text_embedder.warm_up()\n",
    "\n",
    "query_pipeline = Pipeline()\n",
    "query_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "query_pipeline.add_component(\"retriever\", retriever)\n",
    "query_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "query_pipeline.add_component(\"chat_dialog_generator\", chat_dialog_generator)\n",
    "query_pipeline.add_component(\"llm\", llm_component)\n",
    "\n",
    "query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "query_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "query_pipeline.connect(\"prompt_builder.prompt\", \"chat_dialog_generator.prompt\")\n",
    "query_pipeline.connect(\"chat_dialog_generator.dialog\", \"llm.dialog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the query pipeline to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 10:05:20,498\tINFO handle.py:126 -- Created DeploymentHandle 'waala4g5' for Deployment(name='HaystackComponentDeployment', app='text_embedder_deployment').\n",
      "2024-06-26 10:05:20,547\tINFO handle.py:126 -- Created DeploymentHandle 'yrakqgl0' for Deployment(name='HfTextGenerationDeployment', app='llm_deployment').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatMessage(content='The sodium vapor process is a technique used in compositing and visual effects that involves using sodium vapor to achieve scientifically better results in creating transparent images. It utilizes a beam splitter prism to split light that comes through a lens onto two strips of film simultaneously, allowing for precise and accurate compositing. This process has been considered superior to other methods and is being revisited after a long period of not being used, with the hope that it can contribute to advancements in the field. The sodium vapor process is a photographic technique that was historically used in the film industry to create seamless composites and visual effects. It involves the use of sodium vapor lamps to illuminate a scene, which then emits a distinct yellow light. This light is captured on film, which can be used to isolate and manipulate elements within a scene with high precision. The process is known for its ability to produce clear and accurate results, which is why it was considered a step towards achieving perfect transparency in visual effects. The technique uses a beam splitter prism to project the sodium light onto two separate film strips simultaneously, allowing for detailed compositing work. The sod', role='assistant')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aana.core.models.sampling import SamplingParams\n",
    "\n",
    "question = \"What is a sodium vapour process?\"\n",
    "\n",
    "result = query_pipeline.run(\n",
    "    {\n",
    "        \"text_embedder\": {\"text\": question},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "        \"llm\": {\"sampling_params\": SamplingParams()},\n",
    "    }\n",
    ")\n",
    "result[\"llm\"][\"message\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works but Haystack Pipeline doesn't support streaming. That's why we recommend using `PromptBuilder` to generate a prompt and then use the LLM in streaming mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aana-vIr3-B0u-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
