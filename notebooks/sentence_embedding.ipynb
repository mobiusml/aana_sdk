{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# 1. load model\n",
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "\n",
    "# For retrieval you need to pass this prompt.\n",
    "query = \"Represent this sentence for searching relevant passages: A man is eating a piece of bread\"\n",
    "\n",
    "docs = [\n",
    "    query,\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating pasta.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "]\n",
    "\n",
    "# # 2. Encode\n",
    "# embeddings = model.encode(docs)\n",
    "\n",
    "# similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "# print('similarities:', similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-11 11:29:43,525\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: `route_prefix` in `@serve.deployment` has been deprecated. To specify a route prefix for an application, pass it into `serve.run` instead.\n",
      "2024-04-11 11:29:45,082\tINFO worker.py:1540 -- Connecting to existing Ray cluster at address: 172.17.0.3:49821...\n",
      "2024-04-11 11:29:45,094\tINFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:49:00 async_llm_engine.py:433] Received request 386306b7bee449a4a816b101297d82f3: prompt: None, prefix_pos: None,sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True), prompt_token_ids: [1, 29871, 13, 1, 518, 25580, 29962, 11221, 278, 1494, 2472, 29892, 1234, 278, 1139, 2114, 1474, 515, 278, 2793, 366, 505, 29889, 13, 13, 3644, 727, 338, 694, 2472, 297, 278, 4333, 29892, 1827, 393, 322, 2832, 873, 26506, 304, 1234, 278, 1139, 29889, 13, 13, 2677, 29901, 13, 13, 268, 1205, 727, 526, 14568, 310, 4828, 411, 7933, 4315, 29889, 13, 7753, 297, 445, 5400, 3152, 29892, 366, 508, 29915, 29873, 2706, 1999, 332, 719, 470, 17772, 2712, 29889, 13, 887, 508, 29915, 29873, 19531, 22095, 393, 526, 278, 1021, 2927, 408, 278, 4315, 29889, 13, 1126, 278, 805, 453, 310, 278, 2927, 18941, 296, 1355, 5796, 1144, 3661, 482, 29889, 13, 29874, 767, 16246, 373, 263, 380, 1507, 297, 4565, 310, 263, 7933, 4315, 13, 29874, 767, 338, 13587, 263, 12917, 310, 4094, 13, 29874, 767, 411, 263, 367, 538, 322, 263, 4796, 528, 2728, 13, 29874, 767, 13587, 263, 28761, 322, 22378, 13, 13, 268, 306, 1348, 591, 2355, 372, 29889, 13, 2193, 723, 367, 278, 2898, 342, 7933, 4315, 10322, 29889, 13, 2193, 723, 367, 577, 2898, 304, 437, 373, 263, 7933, 4315, 29889, 13, 29874, 1999, 332, 719, 1967, 310, 263, 2022, 411, 1009, 2343, 1623, 13, 29874, 2022, 411, 1472, 11315, 13407, 297, 4565, 310, 263, 13328, 10090, 13, 29874, 767, 411, 1472, 11315, 322, 263, 367, 538, 338, 13407, 297, 4565, 310, 263, 13328, 10090, 13, 29874, 767, 1560, 5475, 1550, 13587, 670, 1361, 304, 670, 3700, 13, 13, 268, 306, 29915, 29885, 773, 599, 278, 534, 7358, 470, 4922, 7933, 4315, 393, 306, 29915, 345, 16692, 975, 278, 2440, 29889, 13, 365, 5861, 411, 263, 13182, 310, 7933, 373, 963, 29889, 13, 365, 5861, 373, 18284, 411, 263, 13182, 310, 2320, 6381, 304, 12611, 714, 278, 7933, 805, 453, 515, 278, 7933, 4315, 29889, 13, 29874, 767, 338, 13407, 297, 4565, 310, 263, 10656, 322, 3367, 15334, 13, 12692, 4315, 297, 263, 8693, 411, 26068, 322, 21083, 13, 29874, 2022, 338, 773, 263, 1591, 29873, 304, 2761, 263, 4742, 13, 29874, 2022, 338, 13587, 263, 3578, 701, 304, 263, 10656, 13, 13, 268, 7569, 2323, 12220, 1537, 2779, 10322, 515, 1432, 14064, 366, 5360, 337, 3687, 373, 445, 6996, 7136, 11043, 29889, 13, 1126, 278, 7601, 982, 591, 437, 393, 338, 411, 7933, 4315, 29889, 13, 1394, 7254, 4315, 29889, 13, 1552, 22049, 7123, 4259, 29871, 29906, 448, 1020, 3955, 13, 1552, 298, 711, 2966, 448, 278, 298, 711, 2966, 448, 9088, 29871, 29896, 13, 485, 21709, 1095, 11802, 448, 9088, 29871, 29896, 13, 29874, 767, 16246, 373, 263, 11774, 411, 670, 21152, 21692, 13, 1552, 7933, 767, 338, 16246, 373, 263, 380, 1507, 13, 29874, 767, 411, 1472, 11315, 322, 367, 538, 16246, 297, 4565, 310, 263, 7254, 3239, 13, 13, 268, 1317, 20892, 1974, 325, 26191, 2253, 1135, 7933, 4315, 29973, 13, 450, 10466, 29892, 278, 17558, 29892, 896, 599, 2649, 592, 372, 338, 29889, 13, 1205, 1363, 278, 544, 12903, 7359, 29915, 29873, 5714, 278, 931, 29892, 306, 29915, 29885, 2360, 2675, 304, 679, 304, 16289, 590, 12561, 310, 263, 274, 677, 29876, 2805, 8300, 373, 16852, 29889, 13, 29874, 767, 411, 1472, 11315, 21653, 670, 5076, 411, 670, 6567, 13, 29874, 767, 411, 3151, 368, 11315, 322, 367, 538, 13407, 297, 4565, 310, 263, 7933, 4315, 13, 29874, 767, 338, 13407, 297, 4565, 310, 263, 7933, 4315, 13, 29874, 767, 411, 1472, 11315, 322, 367, 538, 338, 13407, 297, 4565, 310, 263, 7933, 4315, 13, 13, 13, 16492, 29901, 3750, 7933, 4315, 338, 451, 263, 4922, 1650, 29973, 13, 29961, 29914, 25580, 29962], lora_request: None.\n",
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:49:00 metrics.py:161] Avg prompt throughput: 0.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%\n",
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:49:05 metrics.py:161] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:49:10 metrics.py:161] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.5%, CPU KV cache usage: 0.0%\n",
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:49:12 async_llm_engine.py:110] Finished request 386306b7bee449a4a816b101297d82f3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 2024-04-11 12:49:12,781 vllm_llama2_7b_chat_deployment_VLLMDeployment rfEPGC replica.py:772 - GENERATE OK 12731.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:51:52 async_llm_engine.py:433] Received request 4245904ff5f943bdafbafdd40e3c1046: prompt: None, prefix_pos: None,sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True), prompt_token_ids: [1, 29871, 13, 1, 518, 25580, 29962, 11221, 278, 1494, 2472, 29892, 1234, 278, 1139, 2114, 1474, 515, 278, 2793, 366, 505, 29889, 13, 13, 3644, 727, 338, 694, 2472, 297, 278, 4333, 29892, 1827, 393, 322, 2832, 873, 26506, 304, 1234, 278, 1139, 29889, 13, 13, 2677, 29901, 13, 13, 268, 1205, 727, 526, 14568, 310, 4828, 411, 7933, 4315, 29889, 13, 7753, 297, 445, 5400, 3152, 29892, 366, 508, 29915, 29873, 2706, 1999, 332, 719, 470, 17772, 2712, 29889, 13, 887, 508, 29915, 29873, 19531, 22095, 393, 526, 278, 1021, 2927, 408, 278, 4315, 29889, 13, 1126, 278, 805, 453, 310, 278, 2927, 18941, 296, 1355, 5796, 1144, 3661, 482, 29889, 13, 29874, 767, 16246, 373, 263, 380, 1507, 297, 4565, 310, 263, 7933, 4315, 13, 29874, 767, 338, 13587, 263, 12917, 310, 4094, 13, 29874, 767, 411, 263, 367, 538, 322, 263, 4796, 528, 2728, 13, 29874, 767, 13587, 263, 28761, 322, 22378, 13, 13, 268, 306, 1348, 591, 2355, 372, 29889, 13, 2193, 723, 367, 278, 2898, 342, 7933, 4315, 10322, 29889, 13, 2193, 723, 367, 577, 2898, 304, 437, 373, 263, 7933, 4315, 29889, 13, 29874, 1999, 332, 719, 1967, 310, 263, 2022, 411, 1009, 2343, 1623, 13, 29874, 2022, 411, 1472, 11315, 13407, 297, 4565, 310, 263, 13328, 10090, 13, 29874, 767, 411, 1472, 11315, 322, 263, 367, 538, 338, 13407, 297, 4565, 310, 263, 13328, 10090, 13, 29874, 767, 1560, 5475, 1550, 13587, 670, 1361, 304, 670, 3700, 13, 13, 268, 306, 29915, 29885, 773, 599, 278, 534, 7358, 470, 4922, 7933, 4315, 393, 306, 29915, 345, 16692, 975, 278, 2440, 29889, 13, 365, 5861, 411, 263, 13182, 310, 7933, 373, 963, 29889, 13, 365, 5861, 373, 18284, 411, 263, 13182, 310, 2320, 6381, 304, 12611, 714, 278, 7933, 805, 453, 515, 278, 7933, 4315, 29889, 13, 29874, 767, 338, 13407, 297, 4565, 310, 263, 10656, 322, 3367, 15334, 13, 12692, 4315, 297, 263, 8693, 411, 26068, 322, 21083, 13, 29874, 2022, 338, 773, 263, 1591, 29873, 304, 2761, 263, 4742, 13, 29874, 2022, 338, 13587, 263, 3578, 701, 304, 263, 10656, 13, 13, 268, 7569, 2323, 12220, 1537, 2779, 10322, 515, 1432, 14064, 366, 5360, 337, 3687, 373, 445, 6996, 7136, 11043, 29889, 13, 1126, 278, 7601, 982, 591, 437, 393, 338, 411, 7933, 4315, 29889, 13, 1394, 7254, 4315, 29889, 13, 1552, 22049, 7123, 4259, 29871, 29906, 448, 1020, 3955, 13, 1552, 298, 711, 2966, 448, 278, 298, 711, 2966, 448, 9088, 29871, 29896, 13, 485, 21709, 1095, 11802, 448, 9088, 29871, 29896, 13, 29874, 767, 16246, 373, 263, 11774, 411, 670, 21152, 21692, 13, 1552, 7933, 767, 338, 16246, 373, 263, 380, 1507, 13, 29874, 767, 411, 1472, 11315, 322, 367, 538, 16246, 297, 4565, 310, 263, 7254, 3239, 13, 13, 268, 1317, 20892, 1974, 325, 26191, 2253, 1135, 7933, 4315, 29973, 13, 450, 10466, 29892, 278, 17558, 29892, 896, 599, 2649, 592, 372, 338, 29889, 13, 1205, 1363, 278, 544, 12903, 7359, 29915, 29873, 5714, 278, 931, 29892, 306, 29915, 29885, 2360, 2675, 304, 679, 304, 16289, 590, 12561, 310, 263, 274, 677, 29876, 2805, 8300, 373, 16852, 29889, 13, 29874, 767, 411, 1472, 11315, 21653, 670, 5076, 411, 670, 6567, 13, 29874, 767, 411, 3151, 368, 11315, 322, 367, 538, 13407, 297, 4565, 310, 263, 7933, 4315, 13, 29874, 767, 338, 13407, 297, 4565, 310, 263, 7933, 4315, 13, 29874, 767, 411, 1472, 11315, 322, 367, 538, 338, 13407, 297, 4565, 310, 263, 7933, 4315, 13, 13, 13, 16492, 29901, 3750, 7933, 4315, 338, 451, 263, 4922, 1650, 29973, 13, 29961, 29914, 25580, 29962], lora_request: None.\n",
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:51:52 metrics.py:161] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.6%, CPU KV cache usage: 0.0%\n",
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:51:57 metrics.py:161] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 0.0%\n",
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:52:02 metrics.py:161] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 8.6%, CPU KV cache usage: 0.0%\n",
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 04-11 12:52:04 async_llm_engine.py:110] Finished request 4245904ff5f943bdafbafdd40e3c1046.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:vllm_llama2_7b_chat_deployment:VLLMDeployment pid=55423)\u001b[0m INFO 2024-04-11 12:52:04,807 vllm_llama2_7b_chat_deployment_VLLMDeployment rfEPGC replica.py:772 - GENERATE OK 12458.5ms\n"
     ]
    }
   ],
   "source": [
    "from aana.api.sdk import AanaSDK\n",
    "from aana.api.sdk import get_deployment\n",
    "\n",
    "\n",
    "aana_sdk = AanaSDK(port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_dir\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=53030)\u001b[0m INFO 2024-04-11 11:29:51,130 controller 53030 deployment_state.py:1547 - Deploying new version of deployment SentenceTransformerDeployment in application 'mxbai_embed_large_v1_deployment'. Setting initial target number of replicas to 1.\n",
      "\u001b[36m(ServeController pid=53030)\u001b[0m INFO 2024-04-11 11:29:51,234 controller 53030 deployment_state.py:1831 - Adding 1 replica to deployment SentenceTransformerDeployment in application 'mxbai_embed_large_v1_deployment'.\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m /root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_dir\" has conflict with protected namespace \"model_\".\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m \n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m ERROR 2024-04-11 11:49:53,596 mxbai_embed_large_v1_deployment_SentenceTransformerDeployment nPzsiH replica.py:756 - Request failed due to RayTaskError:\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 753, in wrap_user_method_call\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     yield\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 914, in call_user_method\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     raise e from None\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment.handle_request()\u001b[39m (pid=104002, ip=172.17.0.3)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/aana_sdk/aana/deployments/sentence_transformer_deployment.py\", line 105, in _generate\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     embeddings = self.model.encode(sentences)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 347, in encode\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     length_sorted_idx = np.argsort([-self._text_length(sen) for sen in sentences])\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 347, in <listcomp>\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     length_sorted_idx = np.argsort([-self._text_length(sen) for sen in sentences])\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 857, in _text_length\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     return len(next(iter(text.values())))\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m TypeError: object of type 'float' has no len()\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m \n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m \n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m \u001b[36mray::ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment.handle_request()\u001b[39m (pid=104002, ip=172.17.0.3)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/ray/serve/_private/utils.py\", line 165, in wrap_to_ray_error\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     raise exception\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 895, in call_user_method\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     result = await method_to_call(*request_args, **request_kwargs)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/aana_sdk/aana/deployments/sentence_transformer_deployment.py\", line 88, in embed_batch\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     return await self.batch_processor.process(kwargs)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/aana_sdk/aana/utils/batch_processor.py\", line 103, in process\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     outputs = await asyncio.gather(*futures)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     result = self.fn(*self.args, **self.kwargs)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/aana_sdk/aana/deployments/sentence_transformer_deployment.py\", line 62, in <lambda>\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     process_batch=lambda request: self._generate(**request),\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/aana_sdk/aana/deployments/sentence_transformer_deployment.py\", line 108, in _generate\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     raise InferenceException(self.model_id) from e\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m aana.exceptions.general.InferenceException: InferenceException(model_name=mixedbread-ai/mxbai-embed-large-v1)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m INFO 2024-04-11 11:49:53,602 mxbai_embed_large_v1_deployment_SentenceTransformerDeployment nPzsiH replica.py:772 - EMBED_BATCH ERROR 46.0ms\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m INFO 2024-04-11 11:52:48,722 mxbai_embed_large_v1_deployment_SentenceTransformerDeployment nPzsiH replica.py:772 - EMBED_BATCH OK 1114.8ms\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m INFO 2024-04-11 11:53:17,750 mxbai_embed_large_v1_deployment_SentenceTransformerDeployment nPzsiH replica.py:772 - EMBED_BATCH OK 679.8ms\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m INFO 2024-04-11 11:53:31,972 mxbai_embed_large_v1_deployment_SentenceTransformerDeployment nPzsiH replica.py:772 - EMBED_BATCH OK 695.1ms\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m ERROR 2024-04-11 12:37:36,700 mxbai_embed_large_v1_deployment_SentenceTransformerDeployment nPzsiH replica.py:756 - Request failed due to RayTaskError:\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 753, in wrap_user_method_call\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     yield\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 914, in call_user_method\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     raise e from None\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m ray.exceptions.RayTaskError: \u001b[36mray::ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment.handle_request()\u001b[39m (pid=104002, ip=172.17.0.3)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/ray/serve/_private/utils.py\", line 165, in wrap_to_ray_error\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     raise exception\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 875, in call_user_method\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     runner_method = self.get_runner_method(request_metadata)\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m   File \"/root/.cache/pypoetry/virtualenvs/aana-XDlPP_xZ-py3.10/lib/python3.10/site-packages/ray/serve/_private/replica.py\", line 663, in get_runner_method\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m     raise RayServeException(\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m ray.serve.exceptions.RayServeException: Tried to call a method 'embed' that does not exist. Available methods: ['_generate', 'apply_config', 'embed_batch', 'model', 'reconfigure'].\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m INFO 2024-04-11 12:37:36,700 mxbai_embed_large_v1_deployment_SentenceTransformerDeployment nPzsiH replica.py:772 - EMBED ERROR 1.2ms\n",
      "\u001b[36m(ServeReplica:mxbai_embed_large_v1_deployment:SentenceTransformerDeployment pid=104002)\u001b[0m INFO 2024-04-11 12:37:48,273 mxbai_embed_large_v1_deployment_SentenceTransformerDeployment nPzsiH replica.py:772 - EMBED_BATCH OK 40.2ms\n"
     ]
    }
   ],
   "source": [
    "from aana.deployments.sentence_transformer_deployment import (\n",
    "    SentenceTransformerConfig,\n",
    "    SentenceTransformerDeployment,\n",
    ")\n",
    "\n",
    "sentence_transformer_deployment = SentenceTransformerDeployment.options(\n",
    "    num_replicas=1,\n",
    "    max_concurrent_queries=1000,\n",
    "    ray_actor_options={\"num_gpus\": 0.25},\n",
    "    user_config=SentenceTransformerConfig(\n",
    "        model=\"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "        batch_size=4,\n",
    "        num_processing_threads=2,\n",
    "    ).model_dump(),\n",
    ")\n",
    "\n",
    "aana_sdk.register_deployment(\n",
    "    \"mxbai_embed_large_v1_deployment\", sentence_transformer_deployment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = get_deployment(\"mxbai_embed_large_v1_deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"I am a sentence for which I would like to get its embedding.\",\n",
    "    \"And I am another sentence.\",\n",
    "    \"I am a sentence for which I would like to get its embedding.\",\n",
    "    \"And I am another sentence.\",\n",
    "    \"I am a sentence for which I would like to get its embedding.\",\n",
    "    \"And I am another sentence.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await handle.embed_batch.remote(sentences=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"embedding\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aana-XDlPP_xZ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
