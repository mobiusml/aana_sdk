{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an App for diarized transcription Using Aana SDK\n",
    "\n",
    "This notebook provides an example of getting diarized transcription from video. Please note that the pyannote diarization model is a gated model. Follow [speaker diarization deployment docs](./../docs/pages/model_hub/speaker_recognition.md) to get access to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, set the environment and install aana SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%pip install aana -qqqU \n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "# %env HF_TOKEN=<YOUR HF_TOKEN GOES HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Whisper and PyannoteSpeakerDiarization deployments, define the TranscribeVideoWithDiarEndpoint for diarized transcription. Register deployments and the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aana.api.api_generation import Endpoint\n",
    "from aana.core.models.base import pydantic_to_dict\n",
    "from aana.core.models.speaker import PyannoteSpeakerDiarizationParams\n",
    "from aana.core.models.video import VideoInput\n",
    "from aana.core.models.whisper import WhisperParams\n",
    "from aana.deployments.aana_deployment_handle import AanaDeploymentHandle\n",
    "from aana.deployments.pyannote_speaker_diarization_deployment import (\n",
    "    PyannoteSpeakerDiarizationConfig,\n",
    "    PyannoteSpeakerDiarizationDeployment,\n",
    ")\n",
    "from aana.deployments.whisper_deployment import (\n",
    "    WhisperComputeType,\n",
    "    WhisperConfig,\n",
    "    WhisperDeployment,\n",
    "    WhisperModelSize,\n",
    "    WhisperOutput,\n",
    ")\n",
    "from aana.integrations.external.yt_dlp import download_video\n",
    "from aana.processors.remote import run_remote\n",
    "from aana.processors.speaker import asr_postprocessing_for_diarization\n",
    "from aana.processors.video import extract_audio\n",
    "from aana.sdk import AanaSDK\n",
    "\n",
    "# Define the model deployments.\n",
    "asr_deployment = WhisperDeployment.options(\n",
    "    num_replicas=1,\n",
    "    ray_actor_options={\n",
    "        \"num_gpus\": 0.25\n",
    "    },  # Remove this line if you want to run Whisper on a CPU.# Also change type to float32.\n",
    "    user_config=WhisperConfig(\n",
    "        model_size=WhisperModelSize.SMALL,\n",
    "        compute_type=WhisperComputeType.FLOAT16,\n",
    "    ).model_dump(mode=\"json\"),\n",
    ")\n",
    "diarization_deployment = PyannoteSpeakerDiarizationDeployment.options(\n",
    "    num_replicas=1,\n",
    "    ray_actor_options={\n",
    "        \"num_gpus\": 0.1\n",
    "    },  # Remove this line if you want to run the model on a CPU.\n",
    "    user_config=PyannoteSpeakerDiarizationConfig(\n",
    "        model_id=\"pyannote/speaker-diarization-3.1\"\n",
    "    ).model_dump(mode=\"json\"),\n",
    ")\n",
    "deployments = [\n",
    "    {\"name\": \"asr_deployment\", \"instance\": asr_deployment},\n",
    "    {\"name\": \"diarization_deployment\", \"instance\": diarization_deployment},\n",
    "]\n",
    "\n",
    "\n",
    "# Define the endpoint to transcribe the video with diarization.\n",
    "class TranscribeVideoWithDiarEndpoint(Endpoint):\n",
    "    \"\"\"Transcribe video with diarization endpoint.\"\"\"\n",
    "\n",
    "    async def initialize(self):\n",
    "        \"\"\"Initialize the endpoint.\"\"\"\n",
    "        self.asr_handle = await AanaDeploymentHandle.create(\"asr_deployment\")\n",
    "        self.diar_handle = await AanaDeploymentHandle.create(\"diarization_deployment\")\n",
    "        await super().initialize()\n",
    "\n",
    "    async def run(\n",
    "        self,\n",
    "        video: VideoInput,\n",
    "        whisper_params: WhisperParams,\n",
    "        diar_params: PyannoteSpeakerDiarizationParams,\n",
    "    ) -> WhisperOutput:\n",
    "        \"\"\"Transcribe video with diarization.\"\"\"\n",
    "        video_obj = await run_remote(download_video)(video_input=video)\n",
    "        audio = extract_audio(video=video_obj)\n",
    "\n",
    "        # diarized transcript requires word_timestamps from ASR\n",
    "        whisper_params.word_timestamps = True\n",
    "        transcription = await self.asr_handle.transcribe(\n",
    "            audio=audio, params=whisper_params\n",
    "        )\n",
    "        diarized_output = await self.diar_handle.diarize(\n",
    "            audio=audio, params=diar_params\n",
    "        )\n",
    "        transcription = asr_postprocessing_for_diarization(\n",
    "            diarized_output, transcription\n",
    "        )\n",
    "        output = pydantic_to_dict(transcription)\n",
    "\n",
    "        output_keys = [\"time_interval\", \"speaker\", \"text\"]\n",
    "        filtered_data = [\n",
    "            {k: v for k, v in entry.items() if k in output_keys}\n",
    "            for entry in output[\"segments\"]\n",
    "        ]\n",
    "\n",
    "        return filtered_data\n",
    "\n",
    "\n",
    "endpoints = [\n",
    "    {\n",
    "        \"name\": \"transcribe_video\",\n",
    "        \"path\": \"/video/transcribe\",\n",
    "        \"summary\": \"Transcribe a video\",\n",
    "        \"endpoint_cls\": TranscribeVideoWithDiarEndpoint,\n",
    "    },\n",
    "]\n",
    "\n",
    "aana_app = AanaSDK(name=\"transcribe_video_app\")\n",
    "\n",
    "for deployment in deployments:\n",
    "    aana_app.register_deployment(**deployment)\n",
    "\n",
    "for endpoint in endpoints:\n",
    "    aana_app.register_endpoint(**endpoint)\n",
    "\n",
    "aana_app.connect(\n",
    "    host=\"127.0.0.1\", port=8000, show_logs=False\n",
    ")  # Connects to the Ray cluster or starts a new one.\n",
    "aana_app.migrate()  # Runs the migrations to create the database tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the App!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Deployed successfully.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mDeployed successfully.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Documentation is available at <a href=\"http://127.0.0.1:8000/docs\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:8000/docs</span></a> and <a href=\"http://127.0.0.1:8000/redoc\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:8000/redoc</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Documentation is available at \u001b]8;id=135178;http://127.0.0.1:8000/docs\u001b\\\u001b[4;94mhttp://127.0.0.1:8000/docs\u001b[0m\u001b]8;;\u001b\\ and \u001b]8;id=716794;http://127.0.0.1:8000/redoc\u001b\\\u001b[4;94mhttp://127.0.0.1:8000/redoc\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aana_app.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aana_app.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the app running, lets provide an example audio with multiple speakers for transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'FileNotFoundError', 'message': \"[Errno 2] No such file or directory: 'aana/tests/files/expected/whisper/whisper_medium_sd_sample.wav_speaker.json'\", 'data': {}, 'stacktrace': 'Traceback (most recent call last):\\n  File \"/workspaces/aana_sdk/aana/api/api_generation.py\", line 337, in route_func_body\\n    output = await self.run(**data_dict)\\n  File \"/tmp/ipykernel_1758291/2995001203.py\", line 78, in run\\n  File \"/workspaces/aana_sdk/aana/processors/speaker.py\", line 411, in asr_postprocessing_for_diarization\\n    speaker_labelled_transcription = assign_word_speakers(\\n  File \"/workspaces/aana_sdk/aana/processors/speaker.py\", line 87, in assign_word_speakers\\n    with open(expected_output_path, \"w\") as json_file:\\nFileNotFoundError: [Errno 2] No such file or directory: \\'aana/tests/files/expected/whisper/whisper_medium_sd_sample.wav_speaker.json\\'\\n'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "video = {\n",
    "    \"path\": \"../aana/tests/files/audios/sd_sample.wav\",  # Video URL, Aana SDK supports URLs (including YouTube), file paths or even raw video data\n",
    "    \"media_id\": \"sd_sample\",  # Media ID, so we can ask questions about the video later by using this ID\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"whisper_params\": {\n",
    "        \"word_timestamps\": True,  # Enable word_timestamps\n",
    "    },\n",
    "    \"video\": video,\n",
    "}\n",
    "\n",
    "url = \"http://127.0.0.1:8000/video/transcribe\"\n",
    "\n",
    "# No streaming support possible for diarized transcription as it needs complete ASR output beforehand.\n",
    "response = requests.post(url, data={\"body\": json.dumps(data)})\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each transcribed segment comes with a corresponding speaker label as well!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aana-vIr3-B0u-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
